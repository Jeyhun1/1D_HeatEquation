{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XqFaBeDXyHq",
        "outputId": "78321e18-009d-450b-aa5b-cde2b0e3a11e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Was not able to import Horovod. Thus Horovod support is not enabled\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor, ones, stack, load\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.pyplot import figure\n",
        "import pandas as pd\n",
        "from torch.nn import Module\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from scipy import stats\n",
        "from tesladatadiff4 import TeslaDatasetAll, TeslaDatasetSlice\n",
        "\n",
        "sys.path.append(\"/content/drive/MyDrive/NeuralSolvers-heat-eqn\") \n",
        "import PINNFramework as pf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEqKEcG8YMpg",
        "outputId": "3b85586a-b486-4081-f956-e3e1d0e65b4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Use cuda if it is available, else use the cpu\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ildyl-FvX2kQ"
      },
      "outputs": [],
      "source": [
        "normalize = 1000\n",
        "batch_size = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWB2kB0jYYb-",
        "outputId": "a7acf18e-11ca-4b17-c649-54fcf4766d1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(445266, 10)\n"
          ]
        }
      ],
      "source": [
        "# Create instance of the dataset\n",
        "#ds = TeslaDatasetAll(device = device, normalize = normalize,rel_time = True)\n",
        "ds = TeslaDatasetSlice(device = device, normalize = normalize, rel_time = False, ID = -1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qto5MS9dlclL"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
        "#next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2RpWmEp6Zk2"
      },
      "outputs": [],
      "source": [
        "model = pf.models.MLP(input_size=5,\n",
        "                      output_size=1, \n",
        "                      hidden_size=100, \n",
        "                      num_hidden=4, \n",
        "                      lb=ds.lb, \n",
        "                      ub=ds.ub,\n",
        "                      activation = torch.relu\n",
        "                      )\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXZeiNIRoK2K"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "criterion = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p01JZkSBvB4",
        "outputId": "c56e8462-5b37-4bdd-9a67-2c5604e8ec76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 0\n",
            "Loss after mini-batch    50: 0.98053344\n",
            "Loss after mini-batch   100: 0.88697836\n",
            "Loss after mini-batch   150: 1.32621973\n",
            "Loss after mini-batch   200: 0.69637037\n",
            "Epoch  0 Total Loss  0.9360988411471385\n",
            "Starting epoch 1\n",
            "Loss after mini-batch    50: 0.74217468\n",
            "Loss after mini-batch   100: 0.84072015\n",
            "Loss after mini-batch   150: 1.12598679\n",
            "Loss after mini-batch   200: 0.72346769\n",
            "Epoch  1 Total Loss  0.8281935177905776\n",
            "Starting epoch 2\n",
            "Loss after mini-batch    50: 0.61906959\n",
            "Loss after mini-batch   100: 0.80596302\n",
            "Loss after mini-batch   150: 1.05133999\n",
            "Loss after mini-batch   200: 0.67395666\n",
            "Epoch  2 Total Loss  0.7585812560432171\n",
            "Starting epoch 3\n",
            "Loss after mini-batch    50: 0.57062238\n",
            "Loss after mini-batch   100: 0.74504729\n",
            "Loss after mini-batch   150: 0.98588836\n",
            "Loss after mini-batch   200: 0.60500894\n",
            "Epoch  3 Total Loss  0.7028150910067312\n",
            "Starting epoch 4\n",
            "Loss after mini-batch    50: 0.55821042\n",
            "Loss after mini-batch   100: 0.73802843\n",
            "Loss after mini-batch   150: 0.86556567\n",
            "Loss after mini-batch   200: 0.60936279\n",
            "Epoch  4 Total Loss  0.6736529415234103\n",
            "Starting epoch 5\n",
            "Loss after mini-batch    50: 0.79101264\n",
            "Loss after mini-batch   100: 0.68740534\n",
            "Loss after mini-batch   150: 0.88749957\n",
            "Loss after mini-batch   200: 0.70172873\n",
            "Epoch  5 Total Loss  0.7393404156260124\n",
            "Starting epoch 6\n",
            "Loss after mini-batch    50: 0.64878546\n",
            "Loss after mini-batch   100: 0.67555635\n",
            "Loss after mini-batch   150: 0.68748237\n",
            "Loss after mini-batch   200: 0.67051210\n",
            "Epoch  6 Total Loss  0.6504274684072839\n",
            "Starting epoch 7\n",
            "Loss after mini-batch    50: 0.61190341\n",
            "Loss after mini-batch   100: 0.66555790\n",
            "Loss after mini-batch   150: 0.66285613\n",
            "Loss after mini-batch   200: 0.61480989\n",
            "Epoch  7 Total Loss  0.6214818823014583\n",
            "Starting epoch 8\n",
            "Loss after mini-batch    50: 0.57193017\n",
            "Loss after mini-batch   100: 0.64920886\n",
            "Loss after mini-batch   150: 0.63876591\n",
            "Loss after mini-batch   200: 0.59132613\n",
            "Epoch  8 Total Loss  0.5971536103618937\n",
            "Starting epoch 9\n",
            "Loss after mini-batch    50: 0.55505190\n",
            "Loss after mini-batch   100: 0.64290667\n",
            "Loss after mini-batch   150: 0.62099223\n",
            "Loss after mini-batch   200: 0.59645226\n",
            "Epoch  9 Total Loss  0.5891266375899725\n",
            "Starting epoch 10\n",
            "Loss after mini-batch    50: 0.54395095\n",
            "Loss after mini-batch   100: 0.63626141\n",
            "Loss after mini-batch   150: 0.60492356\n",
            "Loss after mini-batch   200: 0.59333558\n",
            "Epoch  10 Total Loss  0.5805749591961161\n",
            "Starting epoch 11\n",
            "Loss after mini-batch    50: 0.53610374\n",
            "Loss after mini-batch   100: 0.62962199\n",
            "Loss after mini-batch   150: 0.59144412\n",
            "Loss after mini-batch   200: 0.60465188\n",
            "Epoch  11 Total Loss  0.576835381268703\n",
            "Starting epoch 12\n",
            "Loss after mini-batch    50: 0.54313087\n",
            "Loss after mini-batch   100: 0.62186496\n",
            "Loss after mini-batch   150: 0.58352198\n",
            "Loss after mini-batch   200: 0.59476508\n",
            "Epoch  12 Total Loss  0.5727562033571303\n",
            "Starting epoch 13\n",
            "Loss after mini-batch    50: 0.52509913\n",
            "Loss after mini-batch   100: 0.60984742\n",
            "Loss after mini-batch   150: 0.57862768\n",
            "Loss after mini-batch   200: 0.61596055\n",
            "Epoch  13 Total Loss  0.5701817635728746\n",
            "Starting epoch 14\n",
            "Loss after mini-batch    50: 0.55230519\n",
            "Loss after mini-batch   100: 0.59461010\n",
            "Loss after mini-batch   150: 0.58545342\n",
            "Loss after mini-batch   200: 0.61886801\n",
            "Epoch  14 Total Loss  0.5754670513144463\n",
            "Starting epoch 15\n",
            "Loss after mini-batch    50: 0.51127216\n",
            "Loss after mini-batch   100: 0.60695391\n",
            "Loss after mini-batch   150: 0.60943829\n",
            "Loss after mini-batch   200: 0.61731088\n",
            "Epoch  15 Total Loss  0.5753949890425856\n",
            "Starting epoch 16\n",
            "Loss after mini-batch    50: 0.52667545\n",
            "Loss after mini-batch   100: 0.57399260\n",
            "Loss after mini-batch   150: 0.57357267\n",
            "Loss after mini-batch   200: 0.57545792\n",
            "Epoch  16 Total Loss  0.5522901467816613\n",
            "Starting epoch 17\n",
            "Loss after mini-batch    50: 0.48246285\n",
            "Loss after mini-batch   100: 0.58800226\n",
            "Loss after mini-batch   150: 0.55940703\n",
            "Loss after mini-batch   200: 0.60324500\n",
            "Epoch  17 Total Loss  0.5491054803133011\n",
            "Starting epoch 18\n",
            "Loss after mini-batch    50: 0.53035337\n",
            "Loss after mini-batch   100: 0.57128783\n",
            "Loss after mini-batch   150: 0.58006644\n",
            "Loss after mini-batch   200: 0.59963244\n",
            "Epoch  18 Total Loss  0.5569642614872289\n",
            "Starting epoch 19\n",
            "Loss after mini-batch    50: 0.48300936\n",
            "Loss after mini-batch   100: 0.58301795\n",
            "Loss after mini-batch   150: 0.61912588\n",
            "Loss after mini-batch   200: 0.61590632\n",
            "Epoch  19 Total Loss  0.5641662544066753\n",
            "Starting epoch 20\n",
            "Loss after mini-batch    50: 0.51105128\n",
            "Loss after mini-batch   100: 0.59342085\n",
            "Loss after mini-batch   150: 0.71468141\n",
            "Loss after mini-batch   200: 0.62951128\n",
            "Epoch  20 Total Loss  0.6026558986407867\n",
            "Starting epoch 21\n",
            "Loss after mini-batch    50: 0.48531147\n",
            "Loss after mini-batch   100: 0.56357545\n",
            "Loss after mini-batch   150: 0.57708587\n",
            "Loss after mini-batch   200: 0.56567834\n",
            "Epoch  21 Total Loss  0.5367564342001778\n",
            "Starting epoch 22\n",
            "Loss after mini-batch    50: 0.45150938\n",
            "Loss after mini-batch   100: 0.55529334\n",
            "Loss after mini-batch   150: 0.54831597\n",
            "Loss after mini-batch   200: 0.59986563\n",
            "Epoch  22 Total Loss  0.5323098413902548\n",
            "Starting epoch 23\n",
            "Loss after mini-batch    50: 0.50811404\n",
            "Loss after mini-batch   100: 0.55825175\n",
            "Loss after mini-batch   150: 0.57408299\n",
            "Loss after mini-batch   200: 0.57759594\n",
            "Epoch  23 Total Loss  0.5430919313418824\n",
            "Starting epoch 24\n",
            "Loss after mini-batch    50: 0.45722770\n",
            "Loss after mini-batch   100: 0.54922541\n",
            "Loss after mini-batch   150: 0.56007545\n",
            "Loss after mini-batch   200: 0.60252891\n",
            "Epoch  24 Total Loss  0.5356993918187506\n",
            "Starting epoch 25\n",
            "Loss after mini-batch    50: 0.49000557\n",
            "Loss after mini-batch   100: 0.58041524\n",
            "Loss after mini-batch   150: 0.71350504\n",
            "Loss after mini-batch   200: 0.61726914\n",
            "Epoch  25 Total Loss  0.5888954907091796\n",
            "Starting epoch 26\n",
            "Loss after mini-batch    50: 0.56010578\n",
            "Loss after mini-batch   100: 0.58434853\n",
            "Loss after mini-batch   150: 0.57434656\n",
            "Loss after mini-batch   200: 0.55071580\n",
            "Epoch  26 Total Loss  0.554066162983622\n",
            "Starting epoch 27\n",
            "Loss after mini-batch    50: 0.48091597\n",
            "Loss after mini-batch   100: 0.58314688\n",
            "Loss after mini-batch   150: 0.58883209\n",
            "Loss after mini-batch   200: 0.58678848\n",
            "Epoch  27 Total Loss  0.5504204590052981\n",
            "Starting epoch 28\n",
            "Loss after mini-batch    50: 0.55259596\n",
            "Loss after mini-batch   100: 0.58282913\n",
            "Loss after mini-batch   150: 0.80702306\n",
            "Loss after mini-batch   200: 0.57367029\n",
            "Epoch  28 Total Loss  0.6151043018966982\n",
            "Starting epoch 29\n",
            "Loss after mini-batch    50: 0.59244030\n",
            "Loss after mini-batch   100: 0.55132421\n",
            "Loss after mini-batch   150: 0.57438936\n",
            "Loss after mini-batch   200: 0.54712725\n",
            "Epoch  29 Total Loss  0.5565122406814385\n",
            "Starting epoch 30\n",
            "Loss after mini-batch    50: 0.47359513\n",
            "Loss after mini-batch   100: 0.53906715\n",
            "Loss after mini-batch   150: 0.53768787\n",
            "Loss after mini-batch   200: 0.54420628\n",
            "Epoch  30 Total Loss  0.5132572064859742\n",
            "Starting epoch 31\n",
            "Loss after mini-batch    50: 0.44796894\n",
            "Loss after mini-batch   100: 0.54201621\n",
            "Loss after mini-batch   150: 0.50834109\n",
            "Loss after mini-batch   200: 0.54017941\n",
            "Epoch  31 Total Loss  0.5019396026362452\n",
            "Starting epoch 32\n",
            "Loss after mini-batch    50: 0.47729999\n",
            "Loss after mini-batch   100: 0.53943925\n",
            "Loss after mini-batch   150: 0.63512588\n",
            "Loss after mini-batch   200: 0.61272266\n",
            "Epoch  32 Total Loss  0.5576706907536766\n",
            "Starting epoch 33\n",
            "Loss after mini-batch    50: 0.41646175\n",
            "Loss after mini-batch   100: 0.53646941\n",
            "Loss after mini-batch   150: 0.55086668\n",
            "Loss after mini-batch   200: 0.56342043\n",
            "Epoch  33 Total Loss  0.506581575135538\n",
            "Starting epoch 34\n",
            "Loss after mini-batch    50: 0.42550679\n",
            "Loss after mini-batch   100: 0.57328006\n",
            "Loss after mini-batch   150: 0.53415351\n",
            "Loss after mini-batch   200: 0.58603425\n",
            "Epoch  34 Total Loss  0.5206551635381552\n",
            "Starting epoch 35\n",
            "Loss after mini-batch    50: 0.52597915\n",
            "Loss after mini-batch   100: 0.54370931\n",
            "Loss after mini-batch   150: 0.58305781\n",
            "Loss after mini-batch   200: 0.55633550\n",
            "Epoch  35 Total Loss  0.5384831421137061\n",
            "Starting epoch 36\n",
            "Loss after mini-batch    50: 0.53131799\n",
            "Loss after mini-batch   100: 0.56571873\n",
            "Loss after mini-batch   150: 0.57700619\n",
            "Loss after mini-batch   200: 0.57772024\n",
            "Epoch  36 Total Loss  0.5540403531214922\n",
            "Starting epoch 37\n",
            "Loss after mini-batch    50: 0.49200243\n",
            "Loss after mini-batch   100: 0.55814467\n",
            "Loss after mini-batch   150: 0.68865404\n",
            "Loss after mini-batch   200: 0.58320374\n",
            "Epoch  37 Total Loss  0.5667262966744602\n",
            "Starting epoch 38\n",
            "Loss after mini-batch    50: 0.42816270\n",
            "Loss after mini-batch   100: 0.53198829\n",
            "Loss after mini-batch   150: 0.51320181\n",
            "Loss after mini-batch   200: 0.54023795\n",
            "Epoch  38 Total Loss  0.4956142082425911\n",
            "Starting epoch 39\n",
            "Loss after mini-batch    50: 0.43599334\n",
            "Loss after mini-batch   100: 0.53737696\n",
            "Loss after mini-batch   150: 0.56102262\n",
            "Loss after mini-batch   200: 0.58317068\n",
            "Epoch  39 Total Loss  0.522464145534595\n",
            "Starting epoch 40\n",
            "Loss after mini-batch    50: 0.42748932\n",
            "Loss after mini-batch   100: 0.53000398\n",
            "Loss after mini-batch   150: 0.50002078\n",
            "Loss after mini-batch   200: 0.53341553\n",
            "Epoch  40 Total Loss  0.48863221751526\n",
            "Starting epoch 41\n",
            "Loss after mini-batch    50: 0.42768371\n",
            "Loss after mini-batch   100: 0.53710667\n",
            "Loss after mini-batch   150: 0.48119896\n",
            "Loss after mini-batch   200: 0.56374786\n",
            "Epoch  41 Total Loss  0.4952764290068811\n",
            "Starting epoch 42\n",
            "Loss after mini-batch    50: 0.42427254\n",
            "Loss after mini-batch   100: 0.52770421\n",
            "Loss after mini-batch   150: 0.50813478\n",
            "Loss after mini-batch   200: 0.56297312\n",
            "Epoch  42 Total Loss  0.49704183034872246\n",
            "Starting epoch 43\n",
            "Loss after mini-batch    50: 0.44907159\n",
            "Loss after mini-batch   100: 0.54007759\n",
            "Loss after mini-batch   150: 0.50479401\n",
            "Loss after mini-batch   200: 0.57583165\n",
            "Epoch  43 Total Loss  0.5089347180055984\n",
            "Starting epoch 44\n",
            "Loss after mini-batch    50: 0.42091502\n",
            "Loss after mini-batch   100: 0.54325441\n",
            "Loss after mini-batch   150: 0.49086565\n",
            "Loss after mini-batch   200: 0.56643974\n",
            "Epoch  44 Total Loss  0.4924030342745945\n",
            "Starting epoch 45\n",
            "Loss after mini-batch    50: 0.45836631\n",
            "Loss after mini-batch   100: 0.53211828\n",
            "Loss after mini-batch   150: 0.51054634\n",
            "Loss after mini-batch   200: 0.54496147\n",
            "Epoch  45 Total Loss  0.5016044986459913\n",
            "Starting epoch 46\n",
            "Loss after mini-batch    50: 0.39795441\n",
            "Loss after mini-batch   100: 0.52996810\n",
            "Loss after mini-batch   150: 0.55484582\n",
            "Loss after mini-batch   200: 0.58181245\n",
            "Epoch  46 Total Loss  0.507242555452357\n",
            "Starting epoch 47\n",
            "Loss after mini-batch    50: 0.45129881\n",
            "Loss after mini-batch   100: 0.54810609\n",
            "Loss after mini-batch   150: 0.53046610\n",
            "Loss after mini-batch   200: 0.52673223\n",
            "Epoch  47 Total Loss  0.5016614157074225\n",
            "Starting epoch 48\n",
            "Loss after mini-batch    50: 0.45226507\n",
            "Loss after mini-batch   100: 0.51704415\n",
            "Loss after mini-batch   150: 0.46867627\n",
            "Loss after mini-batch   200: 0.52019994\n",
            "Epoch  48 Total Loss  0.481175073824071\n",
            "Starting epoch 49\n",
            "Loss after mini-batch    50: 0.47406361\n",
            "Loss after mini-batch   100: 0.53603435\n",
            "Loss after mini-batch   150: 0.75487313\n",
            "Loss after mini-batch   200: 0.59113057\n",
            "Epoch  49 Total Loss  0.5717360410527454\n",
            "Starting epoch 50\n",
            "Loss after mini-batch    50: 0.50515408\n",
            "Loss after mini-batch   100: 0.55164966\n",
            "Loss after mini-batch   150: 0.50073267\n",
            "Loss after mini-batch   200: 0.53032176\n",
            "Epoch  50 Total Loss  0.5072183178058493\n",
            "Starting epoch 51\n",
            "Loss after mini-batch    50: 0.44925112\n",
            "Loss after mini-batch   100: 0.51555853\n",
            "Loss after mini-batch   150: 0.48519606\n",
            "Loss after mini-batch   200: 0.51861047\n",
            "Epoch  51 Total Loss  0.4807900998361169\n",
            "Starting epoch 52\n",
            "Loss after mini-batch    50: 0.39434556\n",
            "Loss after mini-batch   100: 0.50597431\n",
            "Loss after mini-batch   150: 0.46930312\n",
            "Loss after mini-batch   200: 0.52909758\n",
            "Epoch  52 Total Loss  0.4616381005008081\n",
            "Starting epoch 53\n",
            "Loss after mini-batch    50: 0.42645701\n",
            "Loss after mini-batch   100: 0.49920436\n",
            "Loss after mini-batch   150: 0.46330880\n",
            "Loss after mini-batch   200: 0.53484102\n",
            "Epoch  53 Total Loss  0.4685287372542357\n",
            "Starting epoch 54\n",
            "Loss after mini-batch    50: 0.39760374\n",
            "Loss after mini-batch   100: 0.52560655\n",
            "Loss after mini-batch   150: 0.49979194\n",
            "Loss after mini-batch   200: 0.52740792\n",
            "Epoch  54 Total Loss  0.48800418643844784\n",
            "Starting epoch 55\n",
            "Loss after mini-batch    50: 0.46412978\n",
            "Loss after mini-batch   100: 0.51922574\n",
            "Loss after mini-batch   150: 0.58001889\n",
            "Loss after mini-batch   200: 0.55407136\n",
            "Epoch  55 Total Loss  0.5169701796460794\n",
            "Starting epoch 56\n",
            "Loss after mini-batch    50: 0.46863001\n",
            "Loss after mini-batch   100: 0.52285671\n",
            "Loss after mini-batch   150: 0.48951659\n",
            "Loss after mini-batch   200: 0.50564913\n",
            "Epoch  56 Total Loss  0.48246314557303394\n",
            "Starting epoch 57\n",
            "Loss after mini-batch    50: 0.45779430\n",
            "Loss after mini-batch   100: 0.53612412\n",
            "Loss after mini-batch   150: 0.70256913\n",
            "Loss after mini-batch   200: 0.59021926\n",
            "Epoch  57 Total Loss  0.5516361894145813\n",
            "Starting epoch 58\n",
            "Loss after mini-batch    50: 0.40496557\n",
            "Loss after mini-batch   100: 0.57281119\n",
            "Loss after mini-batch   150: 0.53132685\n",
            "Loss after mini-batch   200: 0.52942939\n",
            "Epoch  58 Total Loss  0.49499128461994846\n",
            "Starting epoch 59\n",
            "Loss after mini-batch    50: 0.50545483\n",
            "Loss after mini-batch   100: 0.55154906\n",
            "Loss after mini-batch   150: 0.60525118\n",
            "Loss after mini-batch   200: 0.51452330\n",
            "Epoch  59 Total Loss  0.5267993348881329\n",
            "Starting epoch 60\n",
            "Loss after mini-batch    50: 0.46062195\n",
            "Loss after mini-batch   100: 0.51083511\n",
            "Loss after mini-batch   150: 0.51754923\n",
            "Loss after mini-batch   200: 0.52964141\n",
            "Epoch  60 Total Loss  0.4858809581513457\n",
            "Starting epoch 61\n",
            "Loss after mini-batch    50: 0.44272280\n",
            "Loss after mini-batch   100: 0.50573126\n",
            "Loss after mini-batch   150: 0.48772080\n",
            "Loss after mini-batch   200: 0.51639664\n",
            "Epoch  61 Total Loss  0.4699361869134009\n",
            "Starting epoch 62\n",
            "Loss after mini-batch    50: 0.40383072\n",
            "Loss after mini-batch   100: 0.50926854\n",
            "Loss after mini-batch   150: 0.50028451\n",
            "Loss after mini-batch   200: 0.51595946\n",
            "Epoch  62 Total Loss  0.4678883103124045\n",
            "Starting epoch 63\n",
            "Loss after mini-batch    50: 0.40591780\n",
            "Loss after mini-batch   100: 0.50680780\n",
            "Loss after mini-batch   150: 0.50101914\n",
            "Loss after mini-batch   200: 0.52970115\n",
            "Epoch  63 Total Loss  0.47297166806961827\n",
            "Starting epoch 64\n",
            "Loss after mini-batch    50: 0.41898312\n",
            "Loss after mini-batch   100: 0.50498691\n",
            "Loss after mini-batch   150: 0.47544981\n",
            "Loss after mini-batch   200: 0.51776559\n",
            "Epoch  64 Total Loss  0.4667237066522822\n",
            "Starting epoch 65\n",
            "Loss after mini-batch    50: 0.40438277\n",
            "Loss after mini-batch   100: 0.51268544\n",
            "Loss after mini-batch   150: 0.48685534\n",
            "Loss after mini-batch   200: 0.54486540\n",
            "Epoch  65 Total Loss  0.46933256214433305\n",
            "Starting epoch 66\n",
            "Loss after mini-batch    50: 0.38308920\n",
            "Loss after mini-batch   100: 0.52898051\n",
            "Loss after mini-batch   150: 0.48142742\n",
            "Loss after mini-batch   200: 0.50029867\n",
            "Epoch  66 Total Loss  0.4573127093784716\n",
            "Starting epoch 67\n",
            "Loss after mini-batch    50: 0.43152297\n",
            "Loss after mini-batch   100: 0.50181335\n",
            "Loss after mini-batch   150: 0.48502451\n",
            "Loss after mini-batch   200: 0.50682256\n",
            "Epoch  67 Total Loss  0.46427745227872846\n",
            "Starting epoch 68\n",
            "Loss after mini-batch    50: 0.43557503\n",
            "Loss after mini-batch   100: 0.52420033\n",
            "Loss after mini-batch   150: 0.55305452\n",
            "Loss after mini-batch   200: 0.54202425\n",
            "Epoch  68 Total Loss  0.4967068799967886\n",
            "Starting epoch 69\n",
            "Loss after mini-batch    50: 0.46074934\n",
            "Loss after mini-batch   100: 0.50989950\n",
            "Loss after mini-batch   150: 0.53664635\n",
            "Loss after mini-batch   200: 0.54081366\n",
            "Epoch  69 Total Loss  0.4940771648378342\n",
            "Starting epoch 70\n",
            "Loss after mini-batch    50: 0.44652652\n",
            "Loss after mini-batch   100: 0.52118770\n",
            "Loss after mini-batch   150: 0.55657071\n",
            "Loss after mini-batch   200: 0.51325161\n",
            "Epoch  70 Total Loss  0.4893189821789605\n",
            "Starting epoch 71\n",
            "Loss after mini-batch    50: 0.42194541\n",
            "Loss after mini-batch   100: 0.49833329\n",
            "Loss after mini-batch   150: 0.58343997\n",
            "Loss after mini-batch   200: 0.50892271\n",
            "Epoch  71 Total Loss  0.4852946526328221\n",
            "Starting epoch 72\n",
            "Loss after mini-batch    50: 0.36838026\n",
            "Loss after mini-batch   100: 0.48965629\n",
            "Loss after mini-batch   150: 0.49140863\n",
            "Loss after mini-batch   200: 0.49178234\n",
            "Epoch  72 Total Loss  0.4419182082424538\n",
            "Starting epoch 73\n",
            "Loss after mini-batch    50: 0.41807737\n",
            "Loss after mini-batch   100: 0.48242354\n",
            "Loss after mini-batch   150: 0.50998017\n",
            "Loss after mini-batch   200: 0.52680693\n",
            "Epoch  73 Total Loss  0.4740880322343576\n",
            "Starting epoch 74\n",
            "Loss after mini-batch    50: 0.40201043\n",
            "Loss after mini-batch   100: 0.51274874\n",
            "Loss after mini-batch   150: 0.61048717\n",
            "Loss after mini-batch   200: 0.51189886\n",
            "Epoch  74 Total Loss  0.4922093864436338\n",
            "Starting epoch 75\n",
            "Loss after mini-batch    50: 0.38221574\n",
            "Loss after mini-batch   100: 0.52013971\n",
            "Loss after mini-batch   150: 0.51649352\n",
            "Loss after mini-batch   200: 0.50942769\n",
            "Epoch  75 Total Loss  0.4617609776035293\n",
            "Starting epoch 76\n",
            "Loss after mini-batch    50: 0.40185162\n",
            "Loss after mini-batch   100: 0.50024685\n",
            "Loss after mini-batch   150: 0.47790156\n",
            "Loss after mini-batch   200: 0.47901076\n",
            "Epoch  76 Total Loss  0.44509048692651365\n",
            "Starting epoch 77\n",
            "Loss after mini-batch    50: 0.37011488\n",
            "Loss after mini-batch   100: 0.49194782\n",
            "Loss after mini-batch   150: 0.51404110\n",
            "Loss after mini-batch   200: 0.52051816\n",
            "Epoch  77 Total Loss  0.46482631357796833\n",
            "Starting epoch 78\n",
            "Loss after mini-batch    50: 0.44039186\n",
            "Loss after mini-batch   100: 0.48895049\n",
            "Loss after mini-batch   150: 0.54099784\n",
            "Loss after mini-batch   200: 0.49117310\n",
            "Epoch  78 Total Loss  0.469421966373066\n",
            "Starting epoch 79\n",
            "Loss after mini-batch    50: 0.43583428\n",
            "Loss after mini-batch   100: 0.51240473\n",
            "Loss after mini-batch   150: 0.54259163\n",
            "Loss after mini-batch   200: 0.48411629\n",
            "Epoch  79 Total Loss  0.4744167633551181\n",
            "Starting epoch 80\n",
            "Loss after mini-batch    50: 0.39785895\n",
            "Loss after mini-batch   100: 0.49114068\n",
            "Loss after mini-batch   150: 0.53369967\n",
            "Loss after mini-batch   200: 0.51715923\n",
            "Epoch  80 Total Loss  0.47211563223203934\n",
            "Starting epoch 81\n",
            "Loss after mini-batch    50: 0.43762990\n",
            "Loss after mini-batch   100: 0.50122987\n",
            "Loss after mini-batch   150: 0.49695243\n",
            "Loss after mini-batch   200: 0.49008044\n",
            "Epoch  81 Total Loss  0.46251240579642083\n",
            "Starting epoch 82\n",
            "Loss after mini-batch    50: 0.44468832\n",
            "Loss after mini-batch   100: 0.51691181\n",
            "Loss after mini-batch   150: 0.52664690\n",
            "Loss after mini-batch   200: 0.48763216\n",
            "Epoch  82 Total Loss  0.47244889949610747\n",
            "Starting epoch 83\n",
            "Loss after mini-batch    50: 0.39981723\n",
            "Loss after mini-batch   100: 0.46523076\n",
            "Loss after mini-batch   150: 0.47843551\n",
            "Loss after mini-batch   200: 0.48353684\n",
            "Epoch  83 Total Loss  0.446905633389796\n",
            "Starting epoch 84\n",
            "Loss after mini-batch    50: 0.39581042\n",
            "Loss after mini-batch   100: 0.51821433\n",
            "Loss after mini-batch   150: 0.56659930\n",
            "Loss after mini-batch   200: 0.50012856\n",
            "Epoch  84 Total Loss  0.4808484010864049\n",
            "Starting epoch 85\n",
            "Loss after mini-batch    50: 0.38394525\n",
            "Loss after mini-batch   100: 0.50370213\n",
            "Loss after mini-batch   150: 0.46901896\n",
            "Loss after mini-batch   200: 0.49553541\n",
            "Epoch  85 Total Loss  0.44380952758125003\n",
            "Starting epoch 86\n",
            "Loss after mini-batch    50: 0.36523930\n",
            "Loss after mini-batch   100: 0.48615514\n",
            "Loss after mini-batch   150: 0.46017335\n",
            "Loss after mini-batch   200: 0.46635946\n",
            "Epoch  86 Total Loss  0.4255284282887235\n",
            "Starting epoch 87\n",
            "Loss after mini-batch    50: 0.40050453\n",
            "Loss after mini-batch   100: 0.50376706\n",
            "Loss after mini-batch   150: 0.48048742\n",
            "Loss after mini-batch   200: 0.51071202\n",
            "Epoch  87 Total Loss  0.45219293503099284\n",
            "Starting epoch 88\n",
            "Loss after mini-batch    50: 0.40179475\n",
            "Loss after mini-batch   100: 0.46782542\n",
            "Loss after mini-batch   150: 0.47532621\n",
            "Loss after mini-batch   200: 0.50134385\n",
            "Epoch  88 Total Loss  0.44738701934746783\n",
            "Starting epoch 89\n",
            "Loss after mini-batch    50: 0.36616920\n",
            "Loss after mini-batch   100: 0.48703168\n",
            "Loss after mini-batch   150: 0.51522824\n",
            "Loss after mini-batch   200: 0.49799998\n",
            "Epoch  89 Total Loss  0.4477050469359468\n",
            "Starting epoch 90\n",
            "Loss after mini-batch    50: 0.41318220\n",
            "Loss after mini-batch   100: 0.51189338\n",
            "Loss after mini-batch   150: 0.51228974\n",
            "Loss after mini-batch   200: 0.47115166\n",
            "Epoch  90 Total Loss  0.46008901568044214\n",
            "Starting epoch 91\n",
            "Loss after mini-batch    50: 0.45638763\n",
            "Loss after mini-batch   100: 0.51809012\n",
            "Loss after mini-batch   150: 0.48911341\n",
            "Loss after mini-batch   200: 0.49604622\n",
            "Epoch  91 Total Loss  0.467746303151917\n",
            "Starting epoch 92\n",
            "Loss after mini-batch    50: 0.46164784\n",
            "Loss after mini-batch   100: 0.48199808\n",
            "Loss after mini-batch   150: 0.46153217\n",
            "Loss after mini-batch   200: 0.47513313\n",
            "Epoch  92 Total Loss  0.45764827793848556\n",
            "Starting epoch 93\n",
            "Loss after mini-batch    50: 0.41317779\n",
            "Loss after mini-batch   100: 0.47789370\n",
            "Loss after mini-batch   150: 0.51270640\n",
            "Loss after mini-batch   200: 0.46842883\n",
            "Epoch  93 Total Loss  0.44768899188744365\n",
            "Starting epoch 94\n",
            "Loss after mini-batch    50: 0.38249389\n",
            "Loss after mini-batch   100: 0.48911544\n",
            "Loss after mini-batch   150: 0.44558648\n",
            "Loss after mini-batch   200: 0.54119574\n",
            "Epoch  94 Total Loss  0.44569980545362475\n",
            "Starting epoch 95\n",
            "Loss after mini-batch    50: 0.35363473\n",
            "Loss after mini-batch   100: 0.50126249\n",
            "Loss after mini-batch   150: 0.44747162\n",
            "Loss after mini-batch   200: 0.49053496\n",
            "Epoch  95 Total Loss  0.43123417198145336\n",
            "Starting epoch 96\n",
            "Loss after mini-batch    50: 0.35739406\n",
            "Loss after mini-batch   100: 0.48222531\n",
            "Loss after mini-batch   150: 0.41611711\n",
            "Loss after mini-batch   200: 0.44174847\n",
            "Epoch  96 Total Loss  0.4073445186266727\n",
            "Starting epoch 97\n",
            "Loss after mini-batch    50: 0.35485657\n",
            "Loss after mini-batch   100: 0.47409259\n",
            "Loss after mini-batch   150: 0.43169117\n",
            "Loss after mini-batch   200: 0.44909447\n",
            "Epoch  97 Total Loss  0.40840457766872207\n",
            "Starting epoch 98\n",
            "Loss after mini-batch    50: 0.34072324\n",
            "Loss after mini-batch   100: 0.45361507\n",
            "Loss after mini-batch   150: 0.43578344\n",
            "Loss after mini-batch   200: 0.46339099\n",
            "Epoch  98 Total Loss  0.4130214966124419\n",
            "Starting epoch 99\n",
            "Loss after mini-batch    50: 0.37716121\n",
            "Loss after mini-batch   100: 0.48918760\n",
            "Loss after mini-batch   150: 0.46132100\n",
            "Loss after mini-batch   200: 0.52618280\n",
            "Epoch  99 Total Loss  0.44520671302965337\n",
            "Starting epoch 100\n",
            "Loss after mini-batch    50: 0.38478104\n",
            "Loss after mini-batch   100: 0.49333314\n",
            "Loss after mini-batch   150: 0.48872642\n",
            "Loss after mini-batch   200: 0.52132915\n",
            "Epoch  100 Total Loss  0.45053863200282146\n",
            "Starting epoch 101\n",
            "Loss after mini-batch    50: 0.42610107\n",
            "Loss after mini-batch   100: 0.47644554\n",
            "Loss after mini-batch   150: 0.47058669\n",
            "Loss after mini-batch   200: 0.44742382\n",
            "Epoch  101 Total Loss  0.44223980059625084\n",
            "Starting epoch 102\n",
            "Loss after mini-batch    50: 0.39490741\n",
            "Loss after mini-batch   100: 0.46862652\n",
            "Loss after mini-batch   150: 0.48687281\n",
            "Loss after mini-batch   200: 0.50446148\n",
            "Epoch  102 Total Loss  0.4424729875135107\n",
            "Starting epoch 103\n",
            "Loss after mini-batch    50: 0.37504661\n",
            "Loss after mini-batch   100: 0.45571996\n",
            "Loss after mini-batch   150: 0.45592629\n",
            "Loss after mini-batch   200: 0.49219620\n",
            "Epoch  103 Total Loss  0.42454314774004825\n",
            "Starting epoch 104\n",
            "Loss after mini-batch    50: 0.33898185\n",
            "Loss after mini-batch   100: 0.45157473\n",
            "Loss after mini-batch   150: 0.42187830\n",
            "Loss after mini-batch   200: 0.47120167\n",
            "Epoch  104 Total Loss  0.4040960266912749\n",
            "Starting epoch 105\n",
            "Loss after mini-batch    50: 0.32754930\n",
            "Loss after mini-batch   100: 0.48110077\n",
            "Loss after mini-batch   150: 0.42960933\n",
            "Loss after mini-batch   200: 0.44757267\n",
            "Epoch  105 Total Loss  0.40222240398731934\n",
            "Starting epoch 106\n",
            "Loss after mini-batch    50: 0.33122495\n",
            "Loss after mini-batch   100: 0.45790370\n",
            "Loss after mini-batch   150: 0.42956051\n",
            "Loss after mini-batch   200: 0.47846742\n",
            "Epoch  106 Total Loss  0.41561971938049574\n",
            "Starting epoch 107\n",
            "Loss after mini-batch    50: 0.36840953\n",
            "Loss after mini-batch   100: 0.45860410\n",
            "Loss after mini-batch   150: 0.49390537\n",
            "Loss after mini-batch   200: 0.47457801\n",
            "Epoch  107 Total Loss  0.4319874229071952\n",
            "Starting epoch 108\n",
            "Loss after mini-batch    50: 0.42057051\n",
            "Loss after mini-batch   100: 0.50740792\n",
            "Loss after mini-batch   150: 0.44950462\n",
            "Loss after mini-batch   200: 0.53220845\n",
            "Epoch  108 Total Loss  0.45559710479520876\n",
            "Starting epoch 109\n",
            "Loss after mini-batch    50: 0.39650938\n",
            "Loss after mini-batch   100: 0.43676956\n",
            "Loss after mini-batch   150: 0.42788502\n",
            "Loss after mini-batch   200: 0.43801419\n",
            "Epoch  109 Total Loss  0.41033406180133103\n",
            "Starting epoch 110\n",
            "Loss after mini-batch    50: 0.34265230\n",
            "Loss after mini-batch   100: 0.46777419\n",
            "Loss after mini-batch   150: 0.41190748\n",
            "Loss after mini-batch   200: 0.51570571\n",
            "Epoch  110 Total Loss  0.4152789367405578\n",
            "Starting epoch 111\n",
            "Loss after mini-batch    50: 0.37744212\n",
            "Loss after mini-batch   100: 0.46340202\n",
            "Loss after mini-batch   150: 0.42874261\n",
            "Loss after mini-batch   200: 0.48815216\n",
            "Epoch  111 Total Loss  0.4191457142075097\n",
            "Starting epoch 112\n",
            "Loss after mini-batch    50: 0.37685203\n",
            "Loss after mini-batch   100: 0.46850629\n",
            "Loss after mini-batch   150: 0.43702394\n",
            "Loss after mini-batch   200: 0.51306631\n",
            "Epoch  112 Total Loss  0.43090018789377915\n",
            "Starting epoch 113\n",
            "Loss after mini-batch    50: 0.34935483\n",
            "Loss after mini-batch   100: 0.44320472\n",
            "Loss after mini-batch   150: 0.45197624\n",
            "Loss after mini-batch   200: 0.47530351\n",
            "Epoch  113 Total Loss  0.4107403178707859\n",
            "Starting epoch 114\n",
            "Loss after mini-batch    50: 0.33482977\n",
            "Loss after mini-batch   100: 0.46255314\n",
            "Loss after mini-batch   150: 0.46437313\n",
            "Loss after mini-batch   200: 0.46891725\n",
            "Epoch  114 Total Loss  0.420869570708227\n",
            "Starting epoch 115\n",
            "Loss after mini-batch    50: 0.36906484\n",
            "Loss after mini-batch   100: 0.45012055\n",
            "Loss after mini-batch   150: 0.49783252\n",
            "Loss after mini-batch   200: 0.42867978\n",
            "Epoch  115 Total Loss  0.4181704948519553\n",
            "Starting epoch 116\n",
            "Loss after mini-batch    50: 0.35184030\n",
            "Loss after mini-batch   100: 0.46645521\n",
            "Loss after mini-batch   150: 0.43517181\n",
            "Loss after mini-batch   200: 0.46747925\n",
            "Epoch  116 Total Loss  0.41144950111148826\n",
            "Starting epoch 117\n",
            "Loss after mini-batch    50: 0.33769749\n",
            "Loss after mini-batch   100: 0.44602731\n",
            "Loss after mini-batch   150: 0.48991459\n",
            "Loss after mini-batch   200: 0.50784941\n",
            "Epoch  117 Total Loss  0.42567523365942844\n",
            "Starting epoch 118\n",
            "Loss after mini-batch    50: 0.34093802\n",
            "Loss after mini-batch   100: 0.46616639\n",
            "Loss after mini-batch   150: 0.52953454\n",
            "Loss after mini-batch   200: 0.45398309\n",
            "Epoch  118 Total Loss  0.4364276448213453\n",
            "Starting epoch 119\n",
            "Loss after mini-batch    50: 0.46961329\n",
            "Loss after mini-batch   100: 0.44921318\n",
            "Loss after mini-batch   150: 0.54893759\n",
            "Loss after mini-batch   200: 0.61553201\n",
            "Epoch  119 Total Loss  0.505333059738655\n",
            "Starting epoch 120\n",
            "Loss after mini-batch    50: 0.37974126\n",
            "Loss after mini-batch   100: 0.44377956\n",
            "Loss after mini-batch   150: 0.43187911\n",
            "Loss after mini-batch   200: 0.42022119\n",
            "Epoch  120 Total Loss  0.40154799037194305\n",
            "Starting epoch 121\n",
            "Loss after mini-batch    50: 0.40852187\n",
            "Loss after mini-batch   100: 0.47328228\n",
            "Loss after mini-batch   150: 0.48986757\n",
            "Loss after mini-batch   200: 0.48785495\n",
            "Epoch  121 Total Loss  0.44648228459265255\n",
            "Starting epoch 122\n",
            "Loss after mini-batch    50: 0.35323425\n",
            "Loss after mini-batch   100: 0.49035985\n",
            "Loss after mini-batch   150: 0.43905975\n",
            "Loss after mini-batch   200: 0.44227743\n",
            "Epoch  122 Total Loss  0.4131126149446456\n",
            "Starting epoch 123\n",
            "Loss after mini-batch    50: 0.34052944\n",
            "Loss after mini-batch   100: 0.44954980\n",
            "Loss after mini-batch   150: 0.40866826\n",
            "Loss after mini-batch   200: 0.44018408\n",
            "Epoch  123 Total Loss  0.39165664979666814\n",
            "Starting epoch 124\n",
            "Loss after mini-batch    50: 0.34958688\n",
            "Loss after mini-batch   100: 0.43723370\n",
            "Loss after mini-batch   150: 0.38422145\n",
            "Loss after mini-batch   200: 0.41293296\n",
            "Epoch  124 Total Loss  0.3787799078988236\n",
            "Starting epoch 125\n",
            "Loss after mini-batch    50: 0.31185150\n",
            "Loss after mini-batch   100: 0.41906828\n",
            "Loss after mini-batch   150: 0.39561032\n",
            "Loss after mini-batch   200: 0.40360323\n",
            "Epoch  125 Total Loss  0.3644248984835256\n",
            "Starting epoch 126\n",
            "Loss after mini-batch    50: 0.31770882\n",
            "Loss after mini-batch   100: 0.41875556\n",
            "Loss after mini-batch   150: 0.38881193\n",
            "Loss after mini-batch   200: 0.42020679\n",
            "Epoch  126 Total Loss  0.369406282273762\n",
            "Starting epoch 127\n",
            "Loss after mini-batch    50: 0.31893358\n",
            "Loss after mini-batch   100: 0.46451080\n",
            "Loss after mini-batch   150: 0.42015479\n",
            "Loss after mini-batch   200: 0.43841308\n",
            "Epoch  127 Total Loss  0.3924078742221613\n",
            "Starting epoch 128\n",
            "Loss after mini-batch    50: 0.35511088\n",
            "Loss after mini-batch   100: 0.45815382\n",
            "Loss after mini-batch   150: 0.41832249\n",
            "Loss after mini-batch   200: 0.57532636\n",
            "Epoch  128 Total Loss  0.4362094653556721\n",
            "Starting epoch 129\n",
            "Loss after mini-batch    50: 0.38862467\n",
            "Loss after mini-batch   100: 0.47864766\n",
            "Loss after mini-batch   150: 0.46910333\n",
            "Loss after mini-batch   200: 0.52285427\n",
            "Epoch  129 Total Loss  0.4444074001795518\n",
            "Starting epoch 130\n",
            "Loss after mini-batch    50: 0.37883884\n",
            "Loss after mini-batch   100: 0.49363673\n",
            "Loss after mini-batch   150: 0.48395123\n",
            "Loss after mini-batch   200: 0.48470797\n",
            "Epoch  130 Total Loss  0.4365356580371362\n",
            "Starting epoch 131\n",
            "Loss after mini-batch    50: 0.39777976\n",
            "Loss after mini-batch   100: 0.43962666\n",
            "Loss after mini-batch   150: 0.45358263\n",
            "Loss after mini-batch   200: 0.46311432\n",
            "Epoch  131 Total Loss  0.42468239534048174\n",
            "Starting epoch 132\n",
            "Loss after mini-batch    50: 0.39604593\n",
            "Loss after mini-batch   100: 0.45556899\n",
            "Loss after mini-batch   150: 0.55028071\n",
            "Loss after mini-batch   200: 0.46290920\n",
            "Epoch  132 Total Loss  0.44370471803180944\n",
            "Starting epoch 133\n",
            "Loss after mini-batch    50: 0.32978837\n",
            "Loss after mini-batch   100: 0.44551111\n",
            "Loss after mini-batch   150: 0.45510211\n",
            "Loss after mini-batch   200: 0.43509537\n",
            "Epoch  133 Total Loss  0.3998718479426082\n",
            "Starting epoch 134\n",
            "Loss after mini-batch    50: 0.33530143\n",
            "Loss after mini-batch   100: 0.42928547\n",
            "Loss after mini-batch   150: 0.40993827\n",
            "Loss after mini-batch   200: 0.41594964\n",
            "Epoch  134 Total Loss  0.3803595485306695\n",
            "Starting epoch 135\n",
            "Loss after mini-batch    50: 0.34505589\n",
            "Loss after mini-batch   100: 0.42038456\n",
            "Loss after mini-batch   150: 0.39537259\n",
            "Loss after mini-batch   200: 0.43274309\n",
            "Epoch  135 Total Loss  0.37988956141020724\n",
            "Starting epoch 136\n",
            "Loss after mini-batch    50: 0.32290748\n",
            "Loss after mini-batch   100: 0.43503031\n",
            "Loss after mini-batch   150: 0.43409128\n",
            "Loss after mini-batch   200: 0.46437730\n",
            "Epoch  136 Total Loss  0.3966429466723439\n",
            "Starting epoch 137\n",
            "Loss after mini-batch    50: 0.33151436\n",
            "Loss after mini-batch   100: 0.44796605\n",
            "Loss after mini-batch   150: 0.46663930\n",
            "Loss after mini-batch   200: 0.48282926\n",
            "Epoch  137 Total Loss  0.414509637503449\n",
            "Starting epoch 138\n",
            "Loss after mini-batch    50: 0.33751184\n",
            "Loss after mini-batch   100: 0.41954541\n",
            "Loss after mini-batch   150: 0.43925606\n",
            "Loss after mini-batch   200: 0.42994459\n",
            "Epoch  138 Total Loss  0.3869026895791907\n",
            "Starting epoch 139\n",
            "Loss after mini-batch    50: 0.31469723\n",
            "Loss after mini-batch   100: 0.42087426\n",
            "Loss after mini-batch   150: 0.40505271\n",
            "Loss after mini-batch   200: 0.41650405\n",
            "Epoch  139 Total Loss  0.37271308000925757\n",
            "Starting epoch 140\n",
            "Loss after mini-batch    50: 0.32104058\n",
            "Loss after mini-batch   100: 0.42169394\n",
            "Loss after mini-batch   150: 0.43532571\n",
            "Loss after mini-batch   200: 0.42047558\n",
            "Epoch  140 Total Loss  0.379471896976711\n",
            "Starting epoch 141\n",
            "Loss after mini-batch    50: 0.34767356\n",
            "Loss after mini-batch   100: 0.42087483\n",
            "Loss after mini-batch   150: 0.44898416\n",
            "Loss after mini-batch   200: 0.41773278\n",
            "Epoch  141 Total Loss  0.3897717442628372\n",
            "Starting epoch 142\n",
            "Loss after mini-batch    50: 0.33830008\n",
            "Loss after mini-batch   100: 0.40468308\n",
            "Loss after mini-batch   150: 0.37914170\n",
            "Loss after mini-batch   200: 0.44177281\n",
            "Epoch  142 Total Loss  0.372277082140566\n",
            "Starting epoch 143\n",
            "Loss after mini-batch    50: 0.34937702\n",
            "Loss after mini-batch   100: 0.41063277\n",
            "Loss after mini-batch   150: 0.43085004\n",
            "Loss after mini-batch   200: 0.41462759\n",
            "Epoch  143 Total Loss  0.38287879059245006\n",
            "Starting epoch 144\n",
            "Loss after mini-batch    50: 0.30620481\n",
            "Loss after mini-batch   100: 0.42044148\n",
            "Loss after mini-batch   150: 0.39807936\n",
            "Loss after mini-batch   200: 0.42151768\n",
            "Epoch  144 Total Loss  0.37495480532493064\n",
            "Starting epoch 145\n",
            "Loss after mini-batch    50: 0.32909336\n",
            "Loss after mini-batch   100: 0.42763858\n",
            "Loss after mini-batch   150: 0.43178318\n",
            "Loss after mini-batch   200: 0.50223968\n",
            "Epoch  145 Total Loss  0.40613072265554734\n",
            "Starting epoch 146\n",
            "Loss after mini-batch    50: 0.34336172\n",
            "Loss after mini-batch   100: 0.44532902\n",
            "Loss after mini-batch   150: 0.41253397\n",
            "Loss after mini-batch   200: 0.41414020\n",
            "Epoch  146 Total Loss  0.3839740899408561\n",
            "Starting epoch 147\n",
            "Loss after mini-batch    50: 0.33470084\n",
            "Loss after mini-batch   100: 0.43105252\n",
            "Loss after mini-batch   150: 0.39807229\n",
            "Loss after mini-batch   200: 0.40649021\n",
            "Epoch  147 Total Loss  0.37535279289041773\n",
            "Starting epoch 148\n",
            "Loss after mini-batch    50: 0.29926041\n",
            "Loss after mini-batch   100: 0.40405505\n",
            "Loss after mini-batch   150: 0.39867697\n",
            "Loss after mini-batch   200: 0.42940137\n",
            "Epoch  148 Total Loss  0.3671861653036754\n",
            "Starting epoch 149\n",
            "Loss after mini-batch    50: 0.29965158\n",
            "Loss after mini-batch   100: 0.42341500\n",
            "Loss after mini-batch   150: 0.44443115\n",
            "Loss after mini-batch   200: 0.50515049\n",
            "Epoch  149 Total Loss  0.4021273779046269\n",
            "Starting epoch 150\n",
            "Loss after mini-batch    50: 0.33981494\n",
            "Loss after mini-batch   100: 0.42622666\n",
            "Loss after mini-batch   150: 0.45261765\n",
            "Loss after mini-batch   200: 0.41472939\n",
            "Epoch  150 Total Loss  0.3928481198787484\n",
            "Starting epoch 151\n",
            "Loss after mini-batch    50: 0.38767837\n",
            "Loss after mini-batch   100: 0.43264502\n",
            "Loss after mini-batch   150: 0.44353893\n",
            "Loss after mini-batch   200: 0.44872871\n",
            "Epoch  151 Total Loss  0.41908950030957476\n",
            "Starting epoch 152\n",
            "Loss after mini-batch    50: 0.39061696\n",
            "Loss after mini-batch   100: 0.42411232\n",
            "Loss after mini-batch   150: 0.42826493\n",
            "Loss after mini-batch   200: 0.43740452\n",
            "Epoch  152 Total Loss  0.40781479844943097\n",
            "Starting epoch 153\n",
            "Loss after mini-batch    50: 0.35564755\n",
            "Loss after mini-batch   100: 0.42374063\n",
            "Loss after mini-batch   150: 0.42600089\n",
            "Loss after mini-batch   200: 0.46410872\n",
            "Epoch  153 Total Loss  0.3970100512010379\n",
            "Starting epoch 154\n",
            "Loss after mini-batch    50: 0.35034256\n",
            "Loss after mini-batch   100: 0.43156973\n",
            "Loss after mini-batch   150: 0.39991989\n",
            "Loss after mini-batch   200: 0.40207530\n",
            "Epoch  154 Total Loss  0.37603294003302007\n",
            "Starting epoch 155\n",
            "Loss after mini-batch    50: 0.32132123\n",
            "Loss after mini-batch   100: 0.39668799\n",
            "Loss after mini-batch   150: 0.42048280\n",
            "Loss after mini-batch   200: 0.40814650\n",
            "Epoch  155 Total Loss  0.36722698853855285\n",
            "Starting epoch 156\n",
            "Loss after mini-batch    50: 0.29156748\n",
            "Loss after mini-batch   100: 0.40263131\n",
            "Loss after mini-batch   150: 0.38889126\n",
            "Loss after mini-batch   200: 0.41655357\n",
            "Epoch  156 Total Loss  0.3579601733133607\n",
            "Starting epoch 157\n",
            "Loss after mini-batch    50: 0.29826360\n",
            "Loss after mini-batch   100: 0.40593006\n",
            "Loss after mini-batch   150: 0.39120062\n",
            "Loss after mini-batch   200: 0.43310239\n",
            "Epoch  157 Total Loss  0.3657858660786387\n",
            "Starting epoch 158\n",
            "Loss after mini-batch    50: 0.30920774\n",
            "Loss after mini-batch   100: 0.39335022\n",
            "Loss after mini-batch   150: 0.36076445\n",
            "Loss after mini-batch   200: 0.37956527\n",
            "Epoch  158 Total Loss  0.3439867429888098\n",
            "Starting epoch 159\n",
            "Loss after mini-batch    50: 0.33076404\n",
            "Loss after mini-batch   100: 0.40616417\n",
            "Loss after mini-batch   150: 0.39727556\n",
            "Loss after mini-batch   200: 0.41632097\n",
            "Epoch  159 Total Loss  0.3680945670428694\n",
            "Starting epoch 160\n",
            "Loss after mini-batch    50: 0.30700317\n",
            "Loss after mini-batch   100: 0.40640371\n",
            "Loss after mini-batch   150: 0.48161472\n",
            "Loss after mini-batch   200: 0.44133236\n",
            "Epoch  160 Total Loss  0.3894004974719308\n",
            "Starting epoch 161\n",
            "Loss after mini-batch    50: 0.29718893\n",
            "Loss after mini-batch   100: 0.51482710\n",
            "Loss after mini-batch   150: 0.46811452\n",
            "Loss after mini-batch   200: 0.46862740\n",
            "Epoch  161 Total Loss  0.414402599079073\n",
            "Starting epoch 162\n",
            "Loss after mini-batch    50: 0.29693743\n",
            "Loss after mini-batch   100: 0.40197970\n",
            "Loss after mini-batch   150: 0.41087757\n",
            "Loss after mini-batch   200: 0.37959230\n",
            "Epoch  162 Total Loss  0.3554613916844198\n",
            "Starting epoch 163\n",
            "Loss after mini-batch    50: 0.30159781\n",
            "Loss after mini-batch   100: 0.40899198\n",
            "Loss after mini-batch   150: 0.39709659\n",
            "Loss after mini-batch   200: 0.42329029\n",
            "Epoch  163 Total Loss  0.3662787120738024\n",
            "Starting epoch 164\n",
            "Loss after mini-batch    50: 0.29487555\n",
            "Loss after mini-batch   100: 0.40666442\n",
            "Loss after mini-batch   150: 0.41635200\n",
            "Loss after mini-batch   200: 0.46953562\n",
            "Epoch  164 Total Loss  0.38473276239862547\n",
            "Starting epoch 165\n",
            "Loss after mini-batch    50: 0.31131432\n",
            "Loss after mini-batch   100: 0.41504121\n",
            "Loss after mini-batch   150: 0.42169253\n",
            "Loss after mini-batch   200: 0.44307683\n",
            "Epoch  165 Total Loss  0.37924992508434374\n",
            "Starting epoch 166\n",
            "Loss after mini-batch    50: 0.37032806\n",
            "Loss after mini-batch   100: 0.41879221\n",
            "Loss after mini-batch   150: 0.39217305\n",
            "Loss after mini-batch   200: 0.38695616\n",
            "Epoch  166 Total Loss  0.37451966339722276\n",
            "Starting epoch 167\n",
            "Loss after mini-batch    50: 0.31467129\n",
            "Loss after mini-batch   100: 0.41624010\n",
            "Loss after mini-batch   150: 0.40424185\n",
            "Loss after mini-batch   200: 0.41689352\n",
            "Epoch  167 Total Loss  0.3707292243206036\n",
            "Starting epoch 168\n",
            "Loss after mini-batch    50: 0.28809273\n",
            "Loss after mini-batch   100: 0.39742391\n",
            "Loss after mini-batch   150: 0.41301956\n",
            "Loss after mini-batch   200: 0.38622116\n",
            "Epoch  168 Total Loss  0.35740097801397563\n",
            "Starting epoch 169\n",
            "Loss after mini-batch    50: 0.31221832\n",
            "Loss after mini-batch   100: 0.38754733\n",
            "Loss after mini-batch   150: 0.39190107\n",
            "Loss after mini-batch   200: 0.43096797\n",
            "Epoch  169 Total Loss  0.3641036866625788\n",
            "Starting epoch 170\n",
            "Loss after mini-batch    50: 0.33819768\n",
            "Loss after mini-batch   100: 0.37867511\n",
            "Loss after mini-batch   150: 0.38182777\n",
            "Loss after mini-batch   200: 0.37492856\n",
            "Epoch  170 Total Loss  0.3551424227595124\n",
            "Starting epoch 171\n",
            "Loss after mini-batch    50: 0.31171244\n",
            "Loss after mini-batch   100: 0.38165425\n",
            "Loss after mini-batch   150: 0.39599463\n",
            "Loss after mini-batch   200: 0.39978642\n",
            "Epoch  171 Total Loss  0.35485113609941044\n",
            "Starting epoch 172\n",
            "Loss after mini-batch    50: 0.29892218\n",
            "Loss after mini-batch   100: 0.40763893\n",
            "Loss after mini-batch   150: 0.42565288\n",
            "Loss after mini-batch   200: 0.37729251\n",
            "Epoch  172 Total Loss  0.358346229410568\n",
            "Starting epoch 173\n",
            "Loss after mini-batch    50: 0.28821298\n",
            "Loss after mini-batch   100: 0.39787533\n",
            "Loss after mini-batch   150: 0.37676440\n",
            "Loss after mini-batch   200: 0.40639752\n",
            "Epoch  173 Total Loss  0.3506268881509132\n",
            "Starting epoch 174\n",
            "Loss after mini-batch    50: 0.30160472\n",
            "Loss after mini-batch   100: 0.38453331\n",
            "Loss after mini-batch   150: 0.35431535\n",
            "Loss after mini-batch   200: 0.38495774\n",
            "Epoch  174 Total Loss  0.3415811888471998\n",
            "Starting epoch 175\n",
            "Loss after mini-batch    50: 0.29200463\n",
            "Loss after mini-batch   100: 0.38155516\n",
            "Loss after mini-batch   150: 0.35873790\n",
            "Loss after mini-batch   200: 0.40076767\n",
            "Epoch  175 Total Loss  0.3449947130588661\n",
            "Starting epoch 176\n",
            "Loss after mini-batch    50: 0.29557134\n",
            "Loss after mini-batch   100: 0.40595368\n",
            "Loss after mini-batch   150: 0.41421757\n",
            "Loss after mini-batch   200: 0.38308244\n",
            "Epoch  176 Total Loss  0.3582562293190885\n",
            "Starting epoch 177\n",
            "Loss after mini-batch    50: 0.27959650\n",
            "Loss after mini-batch   100: 0.40235965\n",
            "Loss after mini-batch   150: 0.41240857\n",
            "Loss after mini-batch   200: 0.38667882\n",
            "Epoch  177 Total Loss  0.35328855349707905\n",
            "Starting epoch 178\n",
            "Loss after mini-batch    50: 0.28077977\n",
            "Loss after mini-batch   100: 0.37460326\n",
            "Loss after mini-batch   150: 0.39861383\n",
            "Loss after mini-batch   200: 0.40058378\n",
            "Epoch  178 Total Loss  0.3493898024380959\n",
            "Starting epoch 179\n",
            "Loss after mini-batch    50: 0.29832398\n",
            "Loss after mini-batch   100: 0.38612170\n",
            "Loss after mini-batch   150: 0.42205879\n",
            "Loss after mini-batch   200: 0.37770109\n",
            "Epoch  179 Total Loss  0.3591703834234301\n",
            "Starting epoch 180\n",
            "Loss after mini-batch    50: 0.28704820\n",
            "Loss after mini-batch   100: 0.38466531\n",
            "Loss after mini-batch   150: 0.44730323\n",
            "Loss after mini-batch   200: 0.41637343\n",
            "Epoch  180 Total Loss  0.36867770582540044\n",
            "Starting epoch 181\n",
            "Loss after mini-batch    50: 0.29184057\n",
            "Loss after mini-batch   100: 0.39763721\n",
            "Loss after mini-batch   150: 0.38320461\n",
            "Loss after mini-batch   200: 0.42154176\n",
            "Epoch  181 Total Loss  0.360248580668976\n",
            "Starting epoch 182\n",
            "Loss after mini-batch    50: 0.31970683\n",
            "Loss after mini-batch   100: 0.37637871\n",
            "Loss after mini-batch   150: 0.36911174\n",
            "Loss after mini-batch   200: 0.43333379\n",
            "Epoch  182 Total Loss  0.35709479745955913\n",
            "Starting epoch 183\n",
            "Loss after mini-batch    50: 0.33252773\n",
            "Loss after mini-batch   100: 0.37303775\n",
            "Loss after mini-batch   150: 0.36154539\n",
            "Loss after mini-batch   200: 0.38452138\n",
            "Epoch  183 Total Loss  0.34650688659433926\n",
            "Starting epoch 184\n",
            "Loss after mini-batch    50: 0.28751952\n",
            "Loss after mini-batch   100: 0.39030899\n",
            "Loss after mini-batch   150: 0.39689564\n",
            "Loss after mini-batch   200: 0.39712123\n",
            "Epoch  184 Total Loss  0.3518343383933358\n",
            "Starting epoch 185\n",
            "Loss after mini-batch    50: 0.29146333\n",
            "Loss after mini-batch   100: 0.44028968\n",
            "Loss after mini-batch   150: 0.43329851\n",
            "Loss after mini-batch   200: 0.36938309\n",
            "Epoch  185 Total Loss  0.366296679537675\n",
            "Starting epoch 186\n",
            "Loss after mini-batch    50: 0.30660017\n",
            "Loss after mini-batch   100: 0.40095908\n",
            "Loss after mini-batch   150: 0.38659237\n",
            "Loss after mini-batch   200: 0.42488553\n",
            "Epoch  186 Total Loss  0.3635675626371158\n",
            "Starting epoch 187\n",
            "Loss after mini-batch    50: 0.28702869\n",
            "Loss after mini-batch   100: 0.41977614\n",
            "Loss after mini-batch   150: 0.46486074\n",
            "Loss after mini-batch   200: 0.48587916\n",
            "Epoch  187 Total Loss  0.39926354034692735\n",
            "Starting epoch 188\n",
            "Loss after mini-batch    50: 0.30475204\n",
            "Loss after mini-batch   100: 0.44012150\n",
            "Loss after mini-batch   150: 0.43984071\n",
            "Loss after mini-batch   200: 0.39936096\n",
            "Epoch  188 Total Loss  0.3793431296688291\n",
            "Starting epoch 189\n",
            "Loss after mini-batch    50: 0.31911838\n",
            "Loss after mini-batch   100: 0.39349342\n",
            "Loss after mini-batch   150: 0.41715088\n",
            "Loss after mini-batch   200: 0.43555703\n",
            "Epoch  189 Total Loss  0.37174636208016526\n",
            "Starting epoch 190\n",
            "Loss after mini-batch    50: 0.29740586\n",
            "Loss after mini-batch   100: 0.36857905\n",
            "Loss after mini-batch   150: 0.40039993\n",
            "Loss after mini-batch   200: 0.36789097\n",
            "Epoch  190 Total Loss  0.3409125031817944\n",
            "Starting epoch 191\n",
            "Loss after mini-batch    50: 0.27050906\n",
            "Loss after mini-batch   100: 0.36490848\n",
            "Loss after mini-batch   150: 0.38107504\n",
            "Loss after mini-batch   200: 0.36443376\n",
            "Epoch  191 Total Loss  0.3287245886434519\n",
            "Starting epoch 192\n",
            "Loss after mini-batch    50: 0.27291247\n",
            "Loss after mini-batch   100: 0.37006225\n",
            "Loss after mini-batch   150: 0.37186178\n",
            "Loss after mini-batch   200: 0.40973221\n",
            "Epoch  192 Total Loss  0.33926258333574194\n",
            "Starting epoch 193\n",
            "Loss after mini-batch    50: 0.28443280\n",
            "Loss after mini-batch   100: 0.37292583\n",
            "Loss after mini-batch   150: 0.38301438\n",
            "Loss after mini-batch   200: 0.42140295\n",
            "Epoch  193 Total Loss  0.34796136278881257\n",
            "Starting epoch 194\n",
            "Loss after mini-batch    50: 0.29632574\n",
            "Loss after mini-batch   100: 0.38774522\n",
            "Loss after mini-batch   150: 0.38522590\n",
            "Loss after mini-batch   200: 0.44533360\n",
            "Epoch  194 Total Loss  0.36306385435762073\n",
            "Starting epoch 195\n",
            "Loss after mini-batch    50: 0.31587212\n",
            "Loss after mini-batch   100: 0.38892705\n",
            "Loss after mini-batch   150: 0.39597368\n",
            "Loss after mini-batch   200: 0.37970318\n",
            "Epoch  195 Total Loss  0.35224902086438387\n",
            "Starting epoch 196\n",
            "Loss after mini-batch    50: 0.29326557\n",
            "Loss after mini-batch   100: 0.37094315\n",
            "Loss after mini-batch   150: 0.37343164\n",
            "Loss after mini-batch   200: 0.40550171\n",
            "Epoch  196 Total Loss  0.34513044458976305\n",
            "Starting epoch 197\n",
            "Loss after mini-batch    50: 0.29569566\n",
            "Loss after mini-batch   100: 0.36387016\n",
            "Loss after mini-batch   150: 0.35388891\n",
            "Loss after mini-batch   200: 0.36787772\n",
            "Epoch  197 Total Loss  0.328622465432032\n",
            "Starting epoch 198\n",
            "Loss after mini-batch    50: 0.31317158\n",
            "Loss after mini-batch   100: 0.36356613\n",
            "Loss after mini-batch   150: 0.35049946\n",
            "Loss after mini-batch   200: 0.33996299\n",
            "Epoch  198 Total Loss  0.325898562651982\n",
            "Starting epoch 199\n",
            "Loss after mini-batch    50: 0.32908825\n",
            "Loss after mini-batch   100: 0.37917396\n",
            "Loss after mini-batch   150: 0.37757615\n",
            "Loss after mini-batch   200: 0.41094285\n",
            "Epoch  199 Total Loss  0.35543299392036615\n",
            "Starting epoch 200\n",
            "Loss after mini-batch    50: 0.28008620\n",
            "Loss after mini-batch   100: 0.36533654\n",
            "Loss after mini-batch   150: 0.39149556\n",
            "Loss after mini-batch   200: 0.41784291\n",
            "Epoch  200 Total Loss  0.349199690838951\n",
            "Starting epoch 201\n",
            "Loss after mini-batch    50: 0.26641484\n",
            "Loss after mini-batch   100: 0.38663793\n",
            "Loss after mini-batch   150: 0.39705451\n",
            "Loss after mini-batch   200: 0.38674057\n",
            "Epoch  201 Total Loss  0.3449245264429018\n",
            "Starting epoch 202\n",
            "Loss after mini-batch    50: 0.28350518\n",
            "Loss after mini-batch   100: 0.42870567\n",
            "Loss after mini-batch   150: 0.36790685\n",
            "Loss after mini-batch   200: 0.36142444\n",
            "Epoch  202 Total Loss  0.3449546684160692\n",
            "Starting epoch 203\n",
            "Loss after mini-batch    50: 0.28071079\n",
            "Loss after mini-batch   100: 0.35797487\n",
            "Loss after mini-batch   150: 0.33725241\n",
            "Loss after mini-batch   200: 0.34012813\n",
            "Epoch  203 Total Loss  0.3166260486340509\n",
            "Starting epoch 204\n",
            "Loss after mini-batch    50: 0.27036776\n",
            "Loss after mini-batch   100: 0.36214858\n",
            "Loss after mini-batch   150: 0.34335767\n",
            "Loss after mini-batch   200: 0.34430594\n",
            "Epoch  204 Total Loss  0.317541746619166\n",
            "Starting epoch 205\n",
            "Loss after mini-batch    50: 0.27323521\n",
            "Loss after mini-batch   100: 0.33933301\n",
            "Loss after mini-batch   150: 0.35387483\n",
            "Loss after mini-batch   200: 0.34563261\n",
            "Epoch  205 Total Loss  0.31274867805421624\n",
            "Starting epoch 206\n",
            "Loss after mini-batch    50: 0.29240746\n",
            "Loss after mini-batch   100: 0.35649116\n",
            "Loss after mini-batch   150: 0.35964815\n",
            "Loss after mini-batch   200: 0.37019495\n",
            "Epoch  206 Total Loss  0.3281614211934764\n",
            "Starting epoch 207\n",
            "Loss after mini-batch    50: 0.26094157\n",
            "Loss after mini-batch   100: 0.39368616\n",
            "Loss after mini-batch   150: 0.40173228\n",
            "Loss after mini-batch   200: 0.41039791\n",
            "Epoch  207 Total Loss  0.34926416309242814\n",
            "Starting epoch 208\n",
            "Loss after mini-batch    50: 0.27924884\n",
            "Loss after mini-batch   100: 0.41229339\n",
            "Loss after mini-batch   150: 0.37119247\n",
            "Loss after mini-batch   200: 0.38418088\n",
            "Epoch  208 Total Loss  0.3438961980736242\n",
            "Starting epoch 209\n",
            "Loss after mini-batch    50: 0.32988705\n",
            "Loss after mini-batch   100: 0.37617522\n",
            "Loss after mini-batch   150: 0.36359838\n",
            "Loss after mini-batch   200: 0.36448949\n",
            "Epoch  209 Total Loss  0.3422412321509776\n",
            "Starting epoch 210\n",
            "Loss after mini-batch    50: 0.29609233\n",
            "Loss after mini-batch   100: 0.35525281\n",
            "Loss after mini-batch   150: 0.34797303\n",
            "Loss after mini-batch   200: 0.41234265\n",
            "Epoch  210 Total Loss  0.3355536043490155\n",
            "Starting epoch 211\n",
            "Loss after mini-batch    50: 0.28718698\n",
            "Loss after mini-batch   100: 0.38134962\n",
            "Loss after mini-batch   150: 0.40838537\n",
            "Loss after mini-batch   200: 0.34703938\n",
            "Epoch  211 Total Loss  0.3390821775276248\n",
            "Starting epoch 212\n",
            "Loss after mini-batch    50: 0.25434146\n",
            "Loss after mini-batch   100: 0.41394348\n",
            "Loss after mini-batch   150: 0.37303152\n",
            "Loss after mini-batch   200: 0.36598600\n",
            "Epoch  212 Total Loss  0.33565763489625183\n",
            "Starting epoch 213\n",
            "Loss after mini-batch    50: 0.28731390\n",
            "Loss after mini-batch   100: 0.35160124\n",
            "Loss after mini-batch   150: 0.36433606\n",
            "Loss after mini-batch   200: 0.35279962\n",
            "Epoch  213 Total Loss  0.3243236056093229\n",
            "Starting epoch 214\n",
            "Loss after mini-batch    50: 0.28380849\n",
            "Loss after mini-batch   100: 0.37529726\n",
            "Loss after mini-batch   150: 0.35401659\n",
            "Loss after mini-batch   200: 0.35553781\n",
            "Epoch  214 Total Loss  0.3273081712386306\n",
            "Starting epoch 215\n",
            "Loss after mini-batch    50: 0.28482447\n",
            "Loss after mini-batch   100: 0.38251932\n",
            "Loss after mini-batch   150: 0.34833404\n",
            "Loss after mini-batch   200: 0.33908809\n",
            "Epoch  215 Total Loss  0.323026915104342\n",
            "Starting epoch 216\n",
            "Loss after mini-batch    50: 0.27024447\n",
            "Loss after mini-batch   100: 0.38030137\n",
            "Loss after mini-batch   150: 0.41839314\n",
            "Loss after mini-batch   200: 0.39865365\n",
            "Epoch  216 Total Loss  0.3515218264560019\n",
            "Starting epoch 217\n",
            "Loss after mini-batch    50: 0.27988948\n",
            "Loss after mini-batch   100: 0.36697513\n",
            "Loss after mini-batch   150: 0.43885753\n",
            "Loss after mini-batch   200: 0.41722231\n",
            "Epoch  217 Total Loss  0.3598746187267227\n",
            "Starting epoch 218\n",
            "Loss after mini-batch    50: 0.27534906\n",
            "Loss after mini-batch   100: 0.37352347\n",
            "Loss after mini-batch   150: 0.36366679\n",
            "Loss after mini-batch   200: 0.36141808\n",
            "Epoch  218 Total Loss  0.32662167818789634\n",
            "Starting epoch 219\n",
            "Loss after mini-batch    50: 0.27717683\n",
            "Loss after mini-batch   100: 0.40550053\n",
            "Loss after mini-batch   150: 0.35774289\n",
            "Loss after mini-batch   200: 0.41784931\n",
            "Epoch  219 Total Loss  0.34668152336748914\n",
            "Starting epoch 220\n",
            "Loss after mini-batch    50: 0.25698918\n",
            "Loss after mini-batch   100: 0.36033441\n",
            "Loss after mini-batch   150: 0.36830158\n",
            "Loss after mini-batch   200: 0.36550657\n",
            "Epoch  220 Total Loss  0.3236359114152029\n",
            "Starting epoch 221\n",
            "Loss after mini-batch    50: 0.24471956\n",
            "Loss after mini-batch   100: 0.37469017\n",
            "Loss after mini-batch   150: 0.36303607\n",
            "Loss after mini-batch   200: 0.36134650\n",
            "Epoch  221 Total Loss  0.3202368881950719\n",
            "Starting epoch 222\n",
            "Loss after mini-batch    50: 0.27477502\n",
            "Loss after mini-batch   100: 0.34869863\n",
            "Loss after mini-batch   150: 0.32683424\n",
            "Loss after mini-batch   200: 0.32964223\n",
            "Epoch  222 Total Loss  0.3042857171668577\n",
            "Starting epoch 223\n",
            "Loss after mini-batch    50: 0.28252401\n",
            "Loss after mini-batch   100: 0.35580386\n",
            "Loss after mini-batch   150: 0.32949378\n",
            "Loss after mini-batch   200: 0.33732355\n",
            "Epoch  223 Total Loss  0.31116967125360023\n",
            "Starting epoch 224\n",
            "Loss after mini-batch    50: 0.24943988\n",
            "Loss after mini-batch   100: 0.36204196\n",
            "Loss after mini-batch   150: 0.35356905\n",
            "Loss after mini-batch   200: 0.34578918\n",
            "Epoch  224 Total Loss  0.31100760880054545\n",
            "Starting epoch 225\n",
            "Loss after mini-batch    50: 0.25481581\n",
            "Loss after mini-batch   100: 0.35237151\n",
            "Loss after mini-batch   150: 0.35320895\n",
            "Loss after mini-batch   200: 0.36679753\n",
            "Epoch  225 Total Loss  0.31575707057832714\n",
            "Starting epoch 226\n",
            "Loss after mini-batch    50: 0.27713984\n",
            "Loss after mini-batch   100: 0.37917034\n",
            "Loss after mini-batch   150: 0.39664309\n",
            "Loss after mini-batch   200: 0.40698847\n",
            "Epoch  226 Total Loss  0.3473964561152896\n",
            "Starting epoch 227\n",
            "Loss after mini-batch    50: 0.26199562\n",
            "Loss after mini-batch   100: 0.41400891\n",
            "Loss after mini-batch   150: 0.36280035\n",
            "Loss after mini-batch   200: 0.36292744\n",
            "Epoch  227 Total Loss  0.33907905172710434\n",
            "Starting epoch 228\n",
            "Loss after mini-batch    50: 0.25471106\n",
            "Loss after mini-batch   100: 0.36845017\n",
            "Loss after mini-batch   150: 0.36894240\n",
            "Loss after mini-batch   200: 0.35540573\n",
            "Epoch  228 Total Loss  0.32488935011003267\n",
            "Starting epoch 229\n",
            "Loss after mini-batch    50: 0.27798607\n",
            "Loss after mini-batch   100: 0.35816493\n",
            "Loss after mini-batch   150: 0.40198501\n",
            "Loss after mini-batch   200: 0.38085227\n",
            "Epoch  229 Total Loss  0.3391939306657317\n",
            "Starting epoch 230\n",
            "Loss after mini-batch    50: 0.31157777\n",
            "Loss after mini-batch   100: 0.40621878\n",
            "Loss after mini-batch   150: 0.42444070\n",
            "Loss after mini-batch   200: 0.42080024\n",
            "Epoch  230 Total Loss  0.3729386135611102\n",
            "Starting epoch 231\n",
            "Loss after mini-batch    50: 0.26939909\n",
            "Loss after mini-batch   100: 0.34715022\n",
            "Loss after mini-batch   150: 0.41525158\n",
            "Loss after mini-batch   200: 0.37185710\n",
            "Epoch  231 Total Loss  0.33634184000569334\n",
            "Starting epoch 232\n",
            "Loss after mini-batch    50: 0.28718885\n",
            "Loss after mini-batch   100: 0.36424621\n",
            "Loss after mini-batch   150: 0.37626777\n",
            "Loss after mini-batch   200: 0.36507818\n",
            "Epoch  232 Total Loss  0.3319687318791627\n",
            "Starting epoch 233\n",
            "Loss after mini-batch    50: 0.25368477\n",
            "Loss after mini-batch   100: 0.37037092\n",
            "Loss after mini-batch   150: 0.37052780\n",
            "Loss after mini-batch   200: 0.35687231\n",
            "Epoch  233 Total Loss  0.32107091824088346\n",
            "Starting epoch 234\n",
            "Loss after mini-batch    50: 0.24914051\n",
            "Loss after mini-batch   100: 0.40217530\n",
            "Loss after mini-batch   150: 0.35868208\n",
            "Loss after mini-batch   200: 0.33578857\n",
            "Epoch  234 Total Loss  0.31977806923218893\n",
            "Starting epoch 235\n",
            "Loss after mini-batch    50: 0.23936441\n",
            "Loss after mini-batch   100: 0.34051730\n",
            "Loss after mini-batch   150: 0.34519284\n",
            "Loss after mini-batch   200: 0.31869723\n",
            "Epoch  235 Total Loss  0.29605826114543604\n",
            "Starting epoch 236\n",
            "Loss after mini-batch    50: 0.23638545\n",
            "Loss after mini-batch   100: 0.33581185\n",
            "Loss after mini-batch   150: 0.31230975\n",
            "Loss after mini-batch   200: 0.34075301\n",
            "Epoch  236 Total Loss  0.2922695262203796\n",
            "Starting epoch 237\n",
            "Loss after mini-batch    50: 0.24847807\n",
            "Loss after mini-batch   100: 0.34272834\n",
            "Loss after mini-batch   150: 0.35178476\n",
            "Loss after mini-batch   200: 0.32211911\n",
            "Epoch  237 Total Loss  0.30125496189597\n",
            "Starting epoch 238\n",
            "Loss after mini-batch    50: 0.25239886\n",
            "Loss after mini-batch   100: 0.39081686\n",
            "Loss after mini-batch   150: 0.37822499\n",
            "Loss after mini-batch   200: 0.37827741\n",
            "Epoch  238 Total Loss  0.33249791292473674\n",
            "Starting epoch 239\n",
            "Loss after mini-batch    50: 0.25731915\n",
            "Loss after mini-batch   100: 0.43334280\n",
            "Loss after mini-batch   150: 0.38036430\n",
            "Loss after mini-batch   200: 0.46205455\n",
            "Epoch  239 Total Loss  0.3647722465182663\n",
            "Starting epoch 240\n",
            "Loss after mini-batch    50: 0.23886181\n",
            "Loss after mini-batch   100: 0.41361464\n",
            "Loss after mini-batch   150: 0.38642573\n",
            "Loss after mini-batch   200: 0.43533137\n",
            "Epoch  240 Total Loss  0.35029169092587104\n",
            "Starting epoch 241\n",
            "Loss after mini-batch    50: 0.26239910\n",
            "Loss after mini-batch   100: 0.36937386\n",
            "Loss after mini-batch   150: 0.35571836\n",
            "Loss after mini-batch   200: 0.39831976\n",
            "Epoch  241 Total Loss  0.32950290739151794\n",
            "Starting epoch 242\n",
            "Loss after mini-batch    50: 0.29295450\n",
            "Loss after mini-batch   100: 0.37196472\n",
            "Loss after mini-batch   150: 0.34973871\n",
            "Loss after mini-batch   200: 0.38341943\n",
            "Epoch  242 Total Loss  0.3320834510831521\n",
            "Starting epoch 243\n",
            "Loss after mini-batch    50: 0.27046976\n",
            "Loss after mini-batch   100: 0.36740445\n",
            "Loss after mini-batch   150: 0.33389209\n",
            "Loss after mini-batch   200: 0.37106941\n",
            "Epoch  243 Total Loss  0.31969195773073156\n",
            "Starting epoch 244\n",
            "Loss after mini-batch    50: 0.23880310\n",
            "Loss after mini-batch   100: 0.36268821\n",
            "Loss after mini-batch   150: 0.35325582\n",
            "Loss after mini-batch   200: 0.34542532\n",
            "Epoch  244 Total Loss  0.3091463898094969\n",
            "Starting epoch 245\n",
            "Loss after mini-batch    50: 0.23289868\n",
            "Loss after mini-batch   100: 0.40540088\n",
            "Loss after mini-batch   150: 0.35285898\n",
            "Loss after mini-batch   200: 0.32932549\n",
            "Epoch  245 Total Loss  0.31495602283660973\n",
            "Starting epoch 246\n",
            "Loss after mini-batch    50: 0.25756377\n",
            "Loss after mini-batch   100: 0.36298660\n",
            "Loss after mini-batch   150: 0.36049294\n",
            "Loss after mini-batch   200: 0.36161072\n",
            "Epoch  246 Total Loss  0.3211000630281729\n",
            "Starting epoch 247\n",
            "Loss after mini-batch    50: 0.25392747\n",
            "Loss after mini-batch   100: 0.35381603\n",
            "Loss after mini-batch   150: 0.38198563\n",
            "Loss after mini-batch   200: 0.35210775\n",
            "Epoch  247 Total Loss  0.3191826516813641\n",
            "Starting epoch 248\n",
            "Loss after mini-batch    50: 0.24969458\n",
            "Loss after mini-batch   100: 0.36287866\n",
            "Loss after mini-batch   150: 0.33314998\n",
            "Loss after mini-batch   200: 0.33223801\n",
            "Epoch  248 Total Loss  0.30556676548333206\n",
            "Starting epoch 249\n",
            "Loss after mini-batch    50: 0.24681052\n",
            "Loss after mini-batch   100: 0.33726693\n",
            "Loss after mini-batch   150: 0.33686365\n",
            "Loss after mini-batch   200: 0.33530366\n",
            "Epoch  249 Total Loss  0.2992427507757184\n",
            "Starting epoch 250\n",
            "Loss after mini-batch    50: 0.24408901\n",
            "Loss after mini-batch   100: 0.35013888\n",
            "Loss after mini-batch   150: 0.31225150\n",
            "Loss after mini-batch   200: 0.33762983\n",
            "Epoch  250 Total Loss  0.29593496300587685\n",
            "Starting epoch 251\n",
            "Loss after mini-batch    50: 0.22707092\n",
            "Loss after mini-batch   100: 0.32535998\n",
            "Loss after mini-batch   150: 0.33898995\n",
            "Loss after mini-batch   200: 0.34679138\n",
            "Epoch  251 Total Loss  0.2948311835980265\n",
            "Starting epoch 252\n",
            "Loss after mini-batch    50: 0.24461049\n",
            "Loss after mini-batch   100: 0.34205316\n",
            "Loss after mini-batch   150: 0.36307310\n",
            "Loss after mini-batch   200: 0.35407055\n",
            "Epoch  252 Total Loss  0.31165940111178325\n",
            "Starting epoch 253\n",
            "Loss after mini-batch    50: 0.23622331\n",
            "Loss after mini-batch   100: 0.40624781\n",
            "Loss after mini-batch   150: 0.36872460\n",
            "Loss after mini-batch   200: 0.36438504\n",
            "Epoch  253 Total Loss  0.3264105236157775\n",
            "Starting epoch 254\n",
            "Loss after mini-batch    50: 0.23151715\n",
            "Loss after mini-batch   100: 0.39566194\n",
            "Loss after mini-batch   150: 0.34255961\n",
            "Loss after mini-batch   200: 0.32438955\n",
            "Epoch  254 Total Loss  0.30856095451793264\n",
            "Starting epoch 255\n",
            "Loss after mini-batch    50: 0.21847141\n",
            "Loss after mini-batch   100: 0.35062147\n",
            "Loss after mini-batch   150: 0.33305653\n",
            "Loss after mini-batch   200: 0.32255898\n",
            "Epoch  255 Total Loss  0.2936123165071284\n",
            "Starting epoch 256\n",
            "Loss after mini-batch    50: 0.28644852\n",
            "Loss after mini-batch   100: 0.35066086\n",
            "Loss after mini-batch   150: 0.34803643\n",
            "Loss after mini-batch   200: 0.45147748\n",
            "Epoch  256 Total Loss  0.34082418664170583\n",
            "Starting epoch 257\n",
            "Loss after mini-batch    50: 0.26655248\n",
            "Loss after mini-batch   100: 0.38184851\n",
            "Loss after mini-batch   150: 0.37198538\n",
            "Loss after mini-batch   200: 0.38970252\n",
            "Epoch  257 Total Loss  0.33541643816909383\n",
            "Starting epoch 258\n",
            "Loss after mini-batch    50: 0.25316267\n",
            "Loss after mini-batch   100: 0.33794845\n",
            "Loss after mini-batch   150: 0.37058659\n",
            "Loss after mini-batch   200: 0.32869200\n",
            "Epoch  258 Total Loss  0.30781106128348806\n",
            "Starting epoch 259\n",
            "Loss after mini-batch    50: 0.22558602\n",
            "Loss after mini-batch   100: 0.33306731\n",
            "Loss after mini-batch   150: 0.36541351\n",
            "Loss after mini-batch   200: 0.32762259\n",
            "Epoch  259 Total Loss  0.297444900318365\n",
            "Starting epoch 260\n",
            "Loss after mini-batch    50: 0.23655199\n",
            "Loss after mini-batch   100: 0.34192434\n",
            "Loss after mini-batch   150: 0.33282792\n",
            "Loss after mini-batch   200: 0.37513200\n",
            "Epoch  260 Total Loss  0.3055863809203644\n",
            "Starting epoch 261\n",
            "Loss after mini-batch    50: 0.23326010\n",
            "Loss after mini-batch   100: 0.31138828\n",
            "Loss after mini-batch   150: 0.33849369\n",
            "Loss after mini-batch   200: 0.33815567\n",
            "Epoch  261 Total Loss  0.2913178779278805\n",
            "Starting epoch 262\n",
            "Loss after mini-batch    50: 0.23679447\n",
            "Loss after mini-batch   100: 0.36181685\n",
            "Loss after mini-batch   150: 0.35554278\n",
            "Loss after mini-batch   200: 0.32268097\n",
            "Epoch  262 Total Loss  0.3066900403349908\n",
            "Starting epoch 263\n",
            "Loss after mini-batch    50: 0.26404949\n",
            "Loss after mini-batch   100: 0.47138300\n",
            "Loss after mini-batch   150: 0.40672230\n",
            "Loss after mini-batch   200: 0.36414540\n",
            "Epoch  263 Total Loss  0.35774963339705257\n",
            "Starting epoch 264\n",
            "Loss after mini-batch    50: 0.25366697\n",
            "Loss after mini-batch   100: 0.43652155\n",
            "Loss after mini-batch   150: 0.34146243\n",
            "Loss after mini-batch   200: 0.32749449\n",
            "Epoch  264 Total Loss  0.32312487753817315\n",
            "Starting epoch 265\n",
            "Loss after mini-batch    50: 0.22689517\n",
            "Loss after mini-batch   100: 0.40151985\n",
            "Loss after mini-batch   150: 0.32784995\n",
            "Loss after mini-batch   200: 0.32619979\n",
            "Epoch  265 Total Loss  0.30415380702977746\n",
            "Starting epoch 266\n",
            "Loss after mini-batch    50: 0.21739046\n",
            "Loss after mini-batch   100: 0.37190089\n",
            "Loss after mini-batch   150: 0.34106125\n",
            "Loss after mini-batch   200: 0.33121005\n",
            "Epoch  266 Total Loss  0.30354326678901367\n",
            "Starting epoch 267\n",
            "Loss after mini-batch    50: 0.22512247\n",
            "Loss after mini-batch   100: 0.35969421\n",
            "Loss after mini-batch   150: 0.33095048\n",
            "Loss after mini-batch   200: 0.30811461\n",
            "Epoch  267 Total Loss  0.2925276153148451\n",
            "Starting epoch 268\n",
            "Loss after mini-batch    50: 0.23327715\n",
            "Loss after mini-batch   100: 0.37836439\n",
            "Loss after mini-batch   150: 0.33380447\n",
            "Loss after mini-batch   200: 0.31886885\n",
            "Epoch  268 Total Loss  0.3030164056807893\n",
            "Starting epoch 269\n",
            "Loss after mini-batch    50: 0.22488843\n",
            "Loss after mini-batch   100: 0.38849306\n",
            "Loss after mini-batch   150: 0.31021051\n",
            "Loss after mini-batch   200: 0.32109063\n",
            "Epoch  269 Total Loss  0.29606478470762115\n",
            "Starting epoch 270\n",
            "Loss after mini-batch    50: 0.22230055\n",
            "Loss after mini-batch   100: 0.33924648\n",
            "Loss after mini-batch   150: 0.30640132\n",
            "Loss after mini-batch   200: 0.30848245\n",
            "Epoch  270 Total Loss  0.2804622188586434\n",
            "Starting epoch 271\n",
            "Loss after mini-batch    50: 0.22107155\n",
            "Loss after mini-batch   100: 0.33221652\n",
            "Loss after mini-batch   150: 0.32565286\n",
            "Loss after mini-batch   200: 0.33900680\n",
            "Epoch  271 Total Loss  0.29088259241629155\n",
            "Starting epoch 272\n",
            "Loss after mini-batch    50: 0.22687546\n",
            "Loss after mini-batch   100: 0.35839641\n",
            "Loss after mini-batch   150: 0.31409377\n",
            "Loss after mini-batch   200: 0.32948883\n",
            "Epoch  272 Total Loss  0.2955151058801817\n",
            "Starting epoch 273\n",
            "Loss after mini-batch    50: 0.24629148\n",
            "Loss after mini-batch   100: 0.32684600\n",
            "Loss after mini-batch   150: 0.33445872\n",
            "Loss after mini-batch   200: 0.37746713\n",
            "Epoch  273 Total Loss  0.3064026383324227\n",
            "Starting epoch 274\n",
            "Loss after mini-batch    50: 0.23044961\n",
            "Loss after mini-batch   100: 0.33951380\n",
            "Loss after mini-batch   150: 0.40791928\n",
            "Loss after mini-batch   200: 0.38568779\n",
            "Epoch  274 Total Loss  0.32679140171326626\n",
            "Starting epoch 275\n",
            "Loss after mini-batch    50: 0.22386666\n",
            "Loss after mini-batch   100: 0.47542409\n",
            "Loss after mini-batch   150: 0.39722263\n",
            "Loss after mini-batch   200: 0.38532027\n",
            "Epoch  275 Total Loss  0.3509843113849264\n",
            "Starting epoch 276\n",
            "Loss after mini-batch    50: 0.24334008\n",
            "Loss after mini-batch   100: 0.40901096\n",
            "Loss after mini-batch   150: 0.35412766\n",
            "Loss after mini-batch   200: 0.35640000\n",
            "Epoch  276 Total Loss  0.3225869485647033\n",
            "Starting epoch 277\n",
            "Loss after mini-batch    50: 0.23316305\n",
            "Loss after mini-batch   100: 0.39038610\n",
            "Loss after mini-batch   150: 0.32412705\n",
            "Loss after mini-batch   200: 0.32249532\n",
            "Epoch  277 Total Loss  0.30347117172007304\n",
            "Starting epoch 278\n",
            "Loss after mini-batch    50: 0.24177111\n",
            "Loss after mini-batch   100: 0.34544060\n",
            "Loss after mini-batch   150: 0.32629332\n",
            "Loss after mini-batch   200: 0.30459792\n",
            "Epoch  278 Total Loss  0.289395790213907\n",
            "Starting epoch 279\n",
            "Loss after mini-batch    50: 0.21637339\n",
            "Loss after mini-batch   100: 0.33745167\n",
            "Loss after mini-batch   150: 0.30118521\n",
            "Loss after mini-batch   200: 0.31137122\n",
            "Epoch  279 Total Loss  0.27826749058782096\n",
            "Starting epoch 280\n",
            "Loss after mini-batch    50: 0.21623571\n",
            "Loss after mini-batch   100: 0.34290850\n",
            "Loss after mini-batch   150: 0.33715689\n",
            "Loss after mini-batch   200: 0.33795953\n",
            "Epoch  280 Total Loss  0.29361969302622004\n",
            "Starting epoch 281\n",
            "Loss after mini-batch    50: 0.20927222\n",
            "Loss after mini-batch   100: 0.32980321\n",
            "Loss after mini-batch   150: 0.32002288\n",
            "Loss after mini-batch   200: 0.31634856\n",
            "Epoch  281 Total Loss  0.28086347816887935\n",
            "Starting epoch 282\n",
            "Loss after mini-batch    50: 0.20141905\n",
            "Loss after mini-batch   100: 0.32854830\n",
            "Loss after mini-batch   150: 0.31601451\n",
            "Loss after mini-batch   200: 0.30104443\n",
            "Epoch  282 Total Loss  0.27428040334766884\n",
            "Starting epoch 283\n",
            "Loss after mini-batch    50: 0.22716060\n",
            "Loss after mini-batch   100: 0.31130296\n",
            "Loss after mini-batch   150: 0.31032418\n",
            "Loss after mini-batch   200: 0.35486214\n",
            "Epoch  283 Total Loss  0.28685067350967625\n",
            "Starting epoch 284\n",
            "Loss after mini-batch    50: 0.23852765\n",
            "Loss after mini-batch   100: 0.34069260\n",
            "Loss after mini-batch   150: 0.31739164\n",
            "Loss after mini-batch   200: 0.40525183\n",
            "Epoch  284 Total Loss  0.31167080330474456\n",
            "Starting epoch 285\n",
            "Loss after mini-batch    50: 0.26040690\n",
            "Loss after mini-batch   100: 0.36516629\n",
            "Loss after mini-batch   150: 0.35356652\n",
            "Loss after mini-batch   200: 0.38588635\n",
            "Epoch  285 Total Loss  0.3240655968521098\n",
            "Starting epoch 286\n",
            "Loss after mini-batch    50: 0.26355400\n",
            "Loss after mini-batch   100: 0.39303167\n",
            "Loss after mini-batch   150: 0.34022004\n",
            "Loss after mini-batch   200: 0.33143923\n",
            "Epoch  286 Total Loss  0.31625185940209605\n",
            "Starting epoch 287\n",
            "Loss after mini-batch    50: 0.22142162\n",
            "Loss after mini-batch   100: 0.33012878\n",
            "Loss after mini-batch   150: 0.34937711\n",
            "Loss after mini-batch   200: 0.32591692\n",
            "Epoch  287 Total Loss  0.29599261992619535\n",
            "Starting epoch 288\n",
            "Loss after mini-batch    50: 0.21425371\n",
            "Loss after mini-batch   100: 0.34914498\n",
            "Loss after mini-batch   150: 0.34755718\n",
            "Loss after mini-batch   200: 0.30856319\n",
            "Epoch  288 Total Loss  0.28985920920968056\n",
            "Starting epoch 289\n",
            "Loss after mini-batch    50: 0.22038137\n",
            "Loss after mini-batch   100: 0.42075469\n",
            "Loss after mini-batch   150: 0.32091758\n",
            "Loss after mini-batch   200: 0.33569663\n",
            "Epoch  289 Total Loss  0.3071370591717097\n",
            "Starting epoch 290\n",
            "Loss after mini-batch    50: 0.20960858\n",
            "Loss after mini-batch   100: 0.39489920\n",
            "Loss after mini-batch   150: 0.32070956\n",
            "Loss after mini-batch   200: 0.31368115\n",
            "Epoch  290 Total Loss  0.29406846315736684\n",
            "Starting epoch 291\n",
            "Loss after mini-batch    50: 0.19567119\n",
            "Loss after mini-batch   100: 0.35986069\n",
            "Loss after mini-batch   150: 0.31913818\n",
            "Loss after mini-batch   200: 0.32931021\n",
            "Epoch  291 Total Loss  0.28739008891669643\n",
            "Starting epoch 292\n",
            "Loss after mini-batch    50: 0.22135049\n",
            "Loss after mini-batch   100: 0.32127225\n",
            "Loss after mini-batch   150: 0.30815334\n",
            "Loss after mini-batch   200: 0.31196166\n",
            "Epoch  292 Total Loss  0.2784961725674877\n",
            "Starting epoch 293\n",
            "Loss after mini-batch    50: 0.23483051\n",
            "Loss after mini-batch   100: 0.30336411\n",
            "Loss after mini-batch   150: 0.30341554\n",
            "Loss after mini-batch   200: 0.33706409\n",
            "Epoch  293 Total Loss  0.28011820491813466\n",
            "Starting epoch 294\n",
            "Loss after mini-batch    50: 0.22366893\n",
            "Loss after mini-batch   100: 0.29665349\n",
            "Loss after mini-batch   150: 0.29900543\n",
            "Loss after mini-batch   200: 0.34656532\n",
            "Epoch  294 Total Loss  0.2795267306447644\n",
            "Starting epoch 295\n",
            "Loss after mini-batch    50: 0.20509832\n",
            "Loss after mini-batch   100: 0.32099370\n",
            "Loss after mini-batch   150: 0.34282085\n",
            "Loss after mini-batch   200: 0.34719353\n",
            "Epoch  295 Total Loss  0.29137338735291374\n",
            "Starting epoch 296\n",
            "Loss after mini-batch    50: 0.20034724\n",
            "Loss after mini-batch   100: 0.37535296\n",
            "Loss after mini-batch   150: 0.32156701\n",
            "Loss after mini-batch   200: 0.31802733\n",
            "Epoch  296 Total Loss  0.28938811646620616\n",
            "Starting epoch 297\n",
            "Loss after mini-batch    50: 0.20577722\n",
            "Loss after mini-batch   100: 0.29983467\n",
            "Loss after mini-batch   150: 0.30211260\n",
            "Loss after mini-batch   200: 0.32671853\n",
            "Epoch  297 Total Loss  0.26986799759516883\n",
            "Starting epoch 298\n",
            "Loss after mini-batch    50: 0.20195475\n",
            "Loss after mini-batch   100: 0.32065823\n",
            "Loss after mini-batch   150: 0.32532918\n",
            "Loss after mini-batch   200: 0.30840983\n",
            "Epoch  298 Total Loss  0.27628199955226357\n",
            "Starting epoch 299\n",
            "Loss after mini-batch    50: 0.21992783\n",
            "Loss after mini-batch   100: 0.45105516\n",
            "Loss after mini-batch   150: 0.36542046\n",
            "Loss after mini-batch   200: 0.41907834\n",
            "Epoch  299 Total Loss  0.347729937974449\n",
            "Starting epoch 300\n",
            "Loss after mini-batch    50: 0.22130321\n",
            "Loss after mini-batch   100: 0.44319999\n",
            "Loss after mini-batch   150: 0.33386062\n",
            "Loss after mini-batch   200: 0.39438401\n",
            "Epoch  300 Total Loss  0.3310247859490369\n",
            "Starting epoch 301\n",
            "Loss after mini-batch    50: 0.21884115\n",
            "Loss after mini-batch   100: 0.42631583\n",
            "Loss after mini-batch   150: 0.32096814\n",
            "Loss after mini-batch   200: 0.38823087\n",
            "Epoch  301 Total Loss  0.32268230006132925\n",
            "Starting epoch 302\n",
            "Loss after mini-batch    50: 0.22916374\n",
            "Loss after mini-batch   100: 0.39861749\n",
            "Loss after mini-batch   150: 0.32309204\n",
            "Loss after mini-batch   200: 0.33947890\n",
            "Epoch  302 Total Loss  0.30598464424918415\n",
            "Starting epoch 303\n",
            "Loss after mini-batch    50: 0.20972140\n",
            "Loss after mini-batch   100: 0.35801303\n",
            "Loss after mini-batch   150: 0.32857996\n",
            "Loss after mini-batch   200: 0.34378621\n",
            "Epoch  303 Total Loss  0.2937212737379681\n",
            "Starting epoch 304\n",
            "Loss after mini-batch    50: 0.19686259\n",
            "Loss after mini-batch   100: 0.31168157\n",
            "Loss after mini-batch   150: 0.30712280\n",
            "Loss after mini-batch   200: 0.30341254\n",
            "Epoch  304 Total Loss  0.2664152834342297\n",
            "Starting epoch 305\n",
            "Loss after mini-batch    50: 0.20764393\n",
            "Loss after mini-batch   100: 0.36952765\n",
            "Loss after mini-batch   150: 0.30984066\n",
            "Loss after mini-batch   200: 0.31779827\n",
            "Epoch  305 Total Loss  0.28565713003653725\n",
            "Starting epoch 306\n",
            "Loss after mini-batch    50: 0.21899487\n",
            "Loss after mini-batch   100: 0.39280826\n",
            "Loss after mini-batch   150: 0.34026980\n",
            "Loss after mini-batch   200: 0.31406330\n",
            "Epoch  306 Total Loss  0.30344680504053145\n",
            "Starting epoch 307\n",
            "Loss after mini-batch    50: 0.22483343\n",
            "Loss after mini-batch   100: 0.33008007\n",
            "Loss after mini-batch   150: 0.32476490\n",
            "Loss after mini-batch   200: 0.33810114\n",
            "Epoch  307 Total Loss  0.29012224117054713\n",
            "Starting epoch 308\n",
            "Loss after mini-batch    50: 0.21585732\n",
            "Loss after mini-batch   100: 0.29093797\n",
            "Loss after mini-batch   150: 0.33264373\n",
            "Loss after mini-batch   200: 0.29436166\n",
            "Epoch  308 Total Loss  0.27012508521881806\n",
            "Starting epoch 309\n",
            "Loss after mini-batch    50: 0.21323902\n",
            "Loss after mini-batch   100: 0.29246514\n",
            "Loss after mini-batch   150: 0.33398687\n",
            "Loss after mini-batch   200: 0.31671567\n",
            "Epoch  309 Total Loss  0.27517102895637785\n",
            "Starting epoch 310\n",
            "Loss after mini-batch    50: 0.22828154\n",
            "Loss after mini-batch   100: 0.33323950\n",
            "Loss after mini-batch   150: 0.32142123\n",
            "Loss after mini-batch   200: 0.31887601\n",
            "Epoch  310 Total Loss  0.28623330961793253\n",
            "Starting epoch 311\n",
            "Loss after mini-batch    50: 0.20079991\n",
            "Loss after mini-batch   100: 0.28959912\n"
          ]
        }
      ],
      "source": [
        "x_data_plot=[]\n",
        "y_data_all_plot=[]\n",
        "y_data_1_plot=[]\n",
        "y_data_2_plot=[]\n",
        "\n",
        "# Set fixed random number seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "Epochs = 1000\n",
        "for epoch in range(Epochs):\n",
        "    # Print epoch\n",
        "    print(f'Starting epoch {epoch}')\n",
        "    \n",
        "    # Set current and total loss value\n",
        "    current_loss = 0.0\n",
        "    total_loss = 0.0\n",
        "    total_loss1 = 0.0\n",
        "    total_loss2 = 0.0\n",
        "\n",
        "    for i, data in enumerate(train_loader,0):\n",
        "        #print(i)\n",
        "\n",
        "        x_batch, y_batch, delta_t = data\n",
        "        #print('data', x_batch.shape)\n",
        "        if batch_size == 1:\n",
        "          x_batch=torch.squeeze(x_batch)\n",
        "          y_batch=torch.squeeze(y_batch)\n",
        "          delta_t=torch.squeeze(delta_t)\n",
        "\n",
        "        # Ground-truth temperature\n",
        "        true_temp = x_batch[:,4].detach().clone()\n",
        "\n",
        "        input0 = x_batch[0].detach().clone()\n",
        "\n",
        "        # Predicted temperature using model prediction and forward euler method\n",
        "        #pred_temp = torch.zeros(x_batch.shape[0])\n",
        "        #pred_temp[0]=true_temp[0].detach().clone().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        prediction = model(x_batch.to(device))\n",
        "     \n",
        "        #for j in range(0, x_batch.shape[0] - 1):\n",
        "        #  input0[4] = torch.tensor(pred_temp[j]).detach().clone()\n",
        "        #  pred = model(input0.to(device))/normalize\n",
        "        #  pred_temp[j + 1] = pred_temp[j] + pred*delta_t[j]\n",
        " \n",
        "        loss = criterion(prediction,y_batch.to(device))\n",
        "        #loss = criterion(pred_temp.to(device),true_temp.to(device))\n",
        "        #loss = loss1+loss2\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        current_loss += loss.item()\n",
        "        total_loss += loss.item()\n",
        "        #total_loss1 += loss1.item()\n",
        "        #total_loss2 += loss2.item()\n",
        "\n",
        "        if i % 50 == 49:\n",
        "            print('Loss after mini-batch %5d: %.8f' %\n",
        "                  (i + 1, current_loss / 50))\n",
        "            current_loss = 0.0\n",
        "\n",
        "    print(\"Epoch \", epoch, \"Total Loss \", total_loss/(i+1))\n",
        "    #print(\"Epoch \", epoch, \"Loss 1 \", total_loss1/(i+1))\n",
        "    #print(\"Epoch \", epoch, \"Loss 2\", total_loss2/(i+1))\n",
        "\n",
        "    x_data_plot.append(epoch)\n",
        "    y_data_all_plot.append(total_loss/(i+1))\n",
        "    #y_data_1_plot.append(total_loss1/(i+1))\n",
        "    #y_data_2_plot.append(total_loss2/(i+1))\n",
        "\n",
        "# Make the plot of Total Loss vs epochs\n",
        "plt.plot(x_data_plot,y_data_all_plot)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Total Loss')\n",
        "plt.show()\n",
        "\n",
        "# Make the plot of the supervised loss\n",
        "#plt.plot(x_data_plot,y_data_1_plot)\n",
        "#plt.xlabel('Epoch')\n",
        "#plt.ylabel('Loss1')\n",
        "#plt.show()\n",
        "\n",
        "# Make the plot of time stability loss\n",
        "#plt.plot(x_data_plot,y_data_2_plot)\n",
        "#plt.xlabel('Epoch')\n",
        "#plt.ylabel('Loss2')\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLIbXG2Adlf3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "0G9EHlOOxzfQ",
        "outputId": "5e663c8d-21f7-4150-bcdd-ce4578b90858"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e78281a0abe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data_plot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_data_all_plot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Total Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "plt.plot(x_data_plot,y_data_all_plot)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Total Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvmxR1iFxyVJ"
      },
      "outputs": [],
      "source": [
        "#torch.save(model.state_dict(),  'reltimealldamodelfor_' + str(normalize) + str(batch_size)+'.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "loVO9lty_1mH",
        "outputId": "ffba2dae-3223-4727-cc62-40febfc23600"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-1388c234dcff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot of last 100 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data_plot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_data_all_plot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "# Plot of last 100 epochs\n",
        "plt.plot(x_data_plot[-200:],y_data_all_plot[-200:])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# Plot of last 100 epochs\n",
        "#plt.plot(x_data_plot[-200:],y_data_1_plot[-200:])\n",
        "#plt.xlabel('Epoch')\n",
        "#plt.ylabel('Loss 1')\n",
        "#plt.show()\n",
        "\n",
        "# Plot of last 100 epochs\n",
        "#plt.plot(x_data_plot[-200:],y_data_2_plot[-200:])\n",
        "#plt.xlabel('Epoch')\n",
        "#plt.ylabel('Loss 2')\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2CKaap0Aauy"
      },
      "outputs": [],
      "source": [
        "# Make a prediction\n",
        "pred = model(ds.x.float().to(device)) #GPU\n",
        "pred = pred.detach().cpu().numpy()/normalize\n",
        "\n",
        "# ground-truth\n",
        "df_y_tensor_np = ds.y.numpy()/normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP7kZ5jnjTTy"
      },
      "outputs": [],
      "source": [
        "# Some statistics on the model performance on all of dataset\n",
        "mae = np.sum(np.abs(pred- df_y_tensor_np).mean(axis=None))\n",
        "print('MAE:', mae)\n",
        "\n",
        "mse = ((df_y_tensor_np - pred)**2).mean(axis=None)\n",
        "print('MSE:', mse)\n",
        "\n",
        "rel_error = np.linalg.norm(pred - df_y_tensor_np) / np.linalg.norm(df_y_tensor_np)\n",
        "print('Relative error (%):', rel_error*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtDt8BXiDYrZ"
      },
      "outputs": [],
      "source": [
        "figure(figsize=(10, 8), dpi= 360)\n",
        "\n",
        "plt.plot(pred, '--')\n",
        "plt.plot(df_y_tensor_np, '-')\n",
        "plt.legend(['Prediction', 'ground-truth'])\n",
        "plt.xlabel('t-idx')\n",
        "plt.ylabel('Delta Temperature/delta time')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3ZQU5JyDrEq"
      },
      "outputs": [],
      "source": [
        "figure(figsize=(10, 8), dpi= 360)\n",
        "\n",
        "#time\n",
        "t=ds.t\n",
        "\n",
        "plt.plot(t,pred, '--')\n",
        "plt.plot(t,df_y_tensor_np, '-')\n",
        "plt.legend(['Prediction', 'ground-truth'])\n",
        "plt.xlabel('time / seconds')\n",
        "plt.ylabel('Delta Temperature/delta time')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_y38GoxFDeJ"
      },
      "outputs": [],
      "source": [
        "# Import a slice of the datase (based on drive-id) for further analysis\n",
        "ds = TeslaDatasetSlice(device = device, ID = 30, rel_time= True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08d4xtKcGONJ"
      },
      "outputs": [],
      "source": [
        "#Forward Euler method with fixed initial conditions but with updated \n",
        "#Temperature from the prediction of the model at previous iteration\n",
        "#with generated temporally equidistant time steps\n",
        "\n",
        "rel_t = ds.rel_t\n",
        "\n",
        "# ground-truth time\n",
        "t=ds.t\n",
        "max_t = t.max()\n",
        "\n",
        "# Ground-truth temperature\n",
        "true_temp = ds.x[:,4].numpy()\n",
        "\n",
        "# Predicted temperature using model prediction and forward euler method\n",
        "pred_temp = np.zeros((ds.x.shape[0]))\n",
        "pred_temp = true_temp.copy()\n",
        "\n",
        "# Fixed initial conditions for all environmental conditions\n",
        "input = ds.x[0].detach().clone()\n",
        "\n",
        "# temporally equdistant time steps\n",
        "tt = np.linspace(0,max_t,ds.x.shape[0])\n",
        "step_size=tt[2]-tt[1]\n",
        "\n",
        "#ODE\n",
        "for i in range(0, ds.x.shape[0] - 1):\n",
        "      input[4] = torch.tensor(pred_temp[i]).detach().clone()\n",
        "      pred = model(input.to(device))\n",
        "      pred = pred.detach().cpu().numpy()/normalize\n",
        "      pred_temp[i + 1] = pred_temp[i] + pred*step_size\n",
        "\n",
        "\n",
        "#MAE\n",
        "mae = np.sum(np.abs(pred_temp- true_temp).mean(axis=None))\n",
        "print('MAE:', mae)\n",
        "\n",
        "#MSE\n",
        "mse = ((true_temp - pred_temp)**2).mean(axis=None)\n",
        "print('MSE:', mse)\n",
        "\n",
        "#Relative error\n",
        "rel_error = np.linalg.norm(pred_temp - true_temp) / np.linalg.norm(true_temp)\n",
        "print('Relative error (%):', rel_error*100)\n",
        "\n",
        "plt.figure(figsize = (12, 8))\n",
        "plt.plot(tt, pred_temp, '-', label='ODE approximation')\n",
        "plt.plot(t, true_temp, '--', label='Exact')\n",
        "plt.title('Approximate and True Solution (temporally equidistant step size)')\n",
        "plt.xlabel('t (seconds)')\n",
        "plt.ylabel('Temperature')\n",
        "plt.grid()\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx9KVaQyGSYb"
      },
      "outputs": [],
      "source": [
        "#Forward Euler method with fixed initial conditions but with updated \n",
        "#Temperature from the prediction of the model at previous iteration\n",
        "#with true step sizes\n",
        "\n",
        "# ground-truth time\n",
        "t=ds.t\n",
        "max_t = t.max()\n",
        "t=t.numpy()\n",
        "\n",
        "# Ground-truth temperature\n",
        "true_temp = ds.x[:,4].numpy()\n",
        "\n",
        "# Predicted temperature using model prediction and forward euler method\n",
        "#pred_temp = np.zeros((ds.x.shape[0]))\n",
        "pred_temp[0] = true_temp[0].copy()\n",
        "\n",
        "# Fixed initial conditions for all environmental conditions\n",
        "input = ds.x[0].detach().clone()\n",
        "\n",
        "# temporally equdistant time steps\n",
        "tt = np.linspace(0,max_t,ds.x.shape[0])\n",
        "step_size=tt[2]-tt[1]\n",
        "\n",
        "# ODE\n",
        "for i in range(0, ds.x.shape[0] - 1):\n",
        "      #input = df_xx_tensor[0]\n",
        "      input[4] = torch.tensor(pred_temp[i]).detach().clone()\n",
        "      pred = model(input.to(device))\n",
        "      pred = pred.detach().cpu().numpy()/normalize\n",
        "      pred_temp[i + 1] = pred_temp[i] + pred*(t[i+1]-t[i])\n",
        "      \n",
        "#MAE \n",
        "mae = np.sum(np.abs(pred_temp- true_temp).mean(axis=None))\n",
        "print('MAE:', mae)\n",
        "\n",
        "#MSE\n",
        "mse = ((true_temp - pred_temp)**2).mean(axis=None)\n",
        "print('MSE:', mse)\n",
        "\n",
        "# Relative error\n",
        "rel_error = np.linalg.norm(pred_temp - true_temp) / np.linalg.norm(true_temp)\n",
        "print('Relative error (%):', rel_error*100)\n",
        "\n",
        "plt.figure(figsize = (12, 8))\n",
        "plt.plot(t, pred_temp, '-', label='ODE approximation')\n",
        "plt.plot(t, true_temp, '--', label='Exact')\n",
        "plt.title('Approximate and True Solution (true step size)')\n",
        "plt.xlabel('t (seconds)')\n",
        "plt.ylabel('Temperature')\n",
        "plt.grid()\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGT9rCWvKyZ4"
      },
      "outputs": [],
      "source": [
        " \n",
        "\n",
        "rel_t = ds.rel_t\n",
        "\n",
        "# ground-truth time\n",
        "t=ds.t\n",
        "max_t = t.max()\n",
        "t=t.numpy()\n",
        "\n",
        "# Ground-truth temperature\n",
        "true_temp = ds.x[:,4].numpy()\n",
        "\n",
        "# Predicted temperature using model prediction and forward euler method\n",
        "pred_temp = np.zeros((ds.x.shape[0]))\n",
        "pred_temp = true_temp.copy()\n",
        "\n",
        "# Fixed initial conditions for all environmental conditions\n",
        "input = ds.x[0].detach().clone()\n",
        "\n",
        "# temporally equdistant time steps\n",
        "tt = np.linspace(0,max_t,ds.x.shape[0])\n",
        "step_size=tt[2]-tt[1]\n",
        "\n",
        "#ODE\n",
        "for i in range(0, ds.x.shape[0] - 1):\n",
        "      input[4] = torch.tensor(pred_temp[i]).detach().clone()\n",
        "      input[5] = torch.tensor(rel_t[i]).detach().clone()\n",
        "      pred = model(input.to(device))\n",
        "      pred = pred.detach().cpu().numpy()/normalize\n",
        "      pred_temp[i + 1] = pred_temp[i] + pred*step_size\n",
        "\n",
        "\n",
        "#MAE\n",
        "mae = np.sum(np.abs(pred_temp- true_temp).mean(axis=None))\n",
        "print('MAE:', mae)\n",
        "\n",
        "#MSE\n",
        "mse = ((true_temp - pred_temp)**2).mean(axis=None)\n",
        "print('MSE:', mse)\n",
        "\n",
        "#Relative error\n",
        "rel_error = np.linalg.norm(pred_temp - true_temp) / np.linalg.norm(true_temp)\n",
        "print('Relative error (%):', rel_error*100)\n",
        "\n",
        "plt.figure(figsize = (12, 8))\n",
        "plt.plot(tt, pred_temp, '-', label='ODE approximation')\n",
        "plt.plot(t, true_temp, '--', label='Exact')\n",
        "plt.title('Approximate(with updated rel time) and True Solution (temporally equidistant step size)')\n",
        "plt.xlabel('t (seconds)')\n",
        "plt.ylabel('Temperature')\n",
        "plt.grid()\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFmgP4Qz6Gll"
      },
      "outputs": [],
      "source": [
        "#Forward Euler method with updated environmental conditions from the dataset at each iteration\n",
        "#But with updated temperature from the prediction of the model at previous iteration\n",
        "#with true step sizes\n",
        "\n",
        "# ground-truth time\n",
        "t=ds.t\n",
        "max_t = t.max()\n",
        "t=t.numpy()\n",
        "\n",
        "# Ground-truth temperature\n",
        "true_temp = ds.x[:,4].numpy()\n",
        "\n",
        "# Predicted temperature using model prediction and forward euler method\n",
        "pred_temp = np.zeros((ds.x.shape[0]))\n",
        "pred_temp[0] = true_temp[0].copy()\n",
        "\n",
        "\n",
        "for i in range(0, ds.x.shape[0] - 1):\n",
        "      input = ds.x[i].detach().clone()\n",
        "      input[4] = torch.tensor(pred_temp[i]).detach().clone()\n",
        "      pred = model(input.to(device))\n",
        "      pred = pred.detach().cpu().numpy()/normalize\n",
        "      pred_temp[i + 1] = pred_temp[i] + pred*(t[i+1]-t[i])\n",
        "\n",
        "#MAE \n",
        "mae = np.sum(np.abs(pred_temp- true_temp).mean(axis=None))\n",
        "print('MAE:', mae)\n",
        "\n",
        "#MSE\n",
        "mse = ((true_temp - pred_temp)**2).mean(axis=None)\n",
        "print('MSE:', mse)\n",
        "\n",
        "# Relative error\n",
        "rel_error = np.linalg.norm(pred_temp - true_temp) / np.linalg.norm(true_temp)\n",
        "print('Relative error (%):', rel_error*100)\n",
        "\n",
        "plt.figure(figsize = (12, 8))\n",
        "plt.plot(t, pred_temp, '-', label='ODE approximation')\n",
        "plt.plot(t, true_temp, '--', label='Exact')\n",
        "plt.title('Approximate (with updated env conditions) and True Solution (true step size)')\n",
        "plt.xlabel('t (seconds)')\n",
        "plt.ylabel('Temperature')\n",
        "plt.grid()\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2ppJIkXGVM7"
      },
      "outputs": [],
      "source": [
        "plt.plot(tt, '-', label='my step size')\n",
        "plt.plot(t, '--', label='true step-size')\n",
        "plt.legend(loc='lower right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yuf54BmGW-P"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.diff(t.reshape(-1)))\n",
        "plt.plot(np.diff(tt.reshape(-1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK2F9VaIkRUj"
      },
      "outputs": [],
      "source": [
        "# Checking if vectorised version of the loss works\n",
        "\n",
        "true_temp = ds.x[:,4].numpy()\n",
        "pred_temp = true_temp.copy()\n",
        "\n",
        "prediction = model(ds.x.to(device)) \n",
        "\n",
        "deltaTemp = prediction.reshape(-1)*ds.dt.reshape(-1)/normalize\n",
        "\n",
        "temp0 = pred_temp + deltaTemp.detach().cpu().numpy()\n",
        "pred_temp[1:] = temp0[:-1]\n",
        "\n",
        "#MAE\n",
        "mae = np.sum(np.abs(pred_temp- true_temp).mean(axis=None))\n",
        "print('MAE:', mae)\n",
        "\n",
        "#MSE\n",
        "mse = ((true_temp - pred_temp)**2).mean(axis=None)\n",
        "print('MSE:', mse)\n",
        "\n",
        "#Relative error\n",
        "rel_error = np.linalg.norm(pred_temp - true_temp) / np.linalg.norm(true_temp)\n",
        "print('Relative error (%):', rel_error*100)\n",
        "\n",
        "plt.figure(figsize = (12, 8))\n",
        "plt.plot(t, pred_temp, '-', label='ODE approximation')\n",
        "plt.plot(t, true_temp, '--', label='Exact')\n",
        "plt.title('Approximate (vectorised) and True Solution (true step size)')\n",
        "plt.xlabel('t (seconds)')\n",
        "plt.ylabel('Temperature')\n",
        "plt.grid()\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obHI0si1x6pQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "#model.load_state_dict(torch.load( 'modelfor_' + str(normalize) + str(batch_size)+'.pt'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of Copy of reltime For loop of Vectorised time stability of 1D_Heat_Eqn_Diff_Operator v2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
