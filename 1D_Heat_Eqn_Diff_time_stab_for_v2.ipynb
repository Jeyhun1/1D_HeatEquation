{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XqFaBeDXyHq",
        "outputId": "2e4a7783-5a43-4f61-80be-ab8c15655ae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Was not able to import Horovod. Thus Horovod support is not enabled\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import Tensor, ones, stack, load\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from matplotlib.pyplot import figure\n",
        "import pandas as pd\n",
        "from torch.nn import Module\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from scipy import stats\n",
        "from tesladatadiff4 import TeslaDatasetAll, TeslaDatasetSlice\n",
        "\n",
        "sys.path.append(\"/content/drive/MyDrive/NeuralSolvers-heat-eqn\") \n",
        "import PINNFramework as pf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEqKEcG8YMpg",
        "outputId": "895c1d2f-5901-4697-91ea-33b431114a53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Use cuda if it is available, else use the cpu\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ildyl-FvX2kQ"
      },
      "outputs": [],
      "source": [
        "normalize = 1000\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWB2kB0jYYb-",
        "outputId": "e9d89063-3241-4fb0-c5fd-3335650f3f6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2301, 10)\n"
          ]
        }
      ],
      "source": [
        "# Create instance of the dataset\n",
        "#ds = TeslaDatasetAll(device = device, normalize = normalize)\n",
        "ds = TeslaDatasetSlice(device = device, normalize = normalize, ID = 30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Qto5MS9dlclL"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
        "#next(iter(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "a2RpWmEp6Zk2"
      },
      "outputs": [],
      "source": [
        "model = pf.models.MLP(input_size=5,\n",
        "                      output_size=1, \n",
        "                      hidden_size=100, \n",
        "                      num_hidden=4, \n",
        "                      lb=ds.lb, \n",
        "                      ub=ds.ub,\n",
        "                      activation = torch.relu\n",
        "                      )\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VXZeiNIRoK2K"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "criterion = torch.nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9p01JZkSBvB4",
        "outputId": "5ef06c26-4b7b-4bb1-a1c3-6a32d926cf55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting epoch 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss after mini-batch    50: 0.00001279\n",
            "Epoch  0 Total Loss  1.2055067811076211e-05\n",
            "Starting epoch 1\n",
            "Loss after mini-batch    50: 0.00001473\n",
            "Epoch  1 Total Loss  1.5293982072315e-05\n",
            "Starting epoch 2\n",
            "Loss after mini-batch    50: 0.00001275\n",
            "Epoch  2 Total Loss  1.285998189586795e-05\n",
            "Starting epoch 3\n",
            "Loss after mini-batch    50: 0.00001202\n",
            "Epoch  3 Total Loss  1.2413777449316044e-05\n",
            "Starting epoch 4\n",
            "Loss after mini-batch    50: 0.00001267\n",
            "Epoch  4 Total Loss  1.4367266736092787e-05\n",
            "Starting epoch 5\n",
            "Loss after mini-batch    50: 0.00001211\n",
            "Epoch  5 Total Loss  1.2012099145290259e-05\n",
            "Starting epoch 6\n",
            "Loss after mini-batch    50: 0.00001000\n",
            "Epoch  6 Total Loss  9.191350302812435e-06\n",
            "Starting epoch 7\n",
            "Loss after mini-batch    50: 0.00001000\n",
            "Epoch  7 Total Loss  9.205489767251231e-06\n",
            "Starting epoch 8\n",
            "Loss after mini-batch    50: 0.00001024\n",
            "Epoch  8 Total Loss  8.920818907008786e-06\n",
            "Starting epoch 9\n",
            "Loss after mini-batch    50: 0.00001032\n",
            "Epoch  9 Total Loss  8.61547481193862e-06\n",
            "Starting epoch 10\n",
            "Loss after mini-batch    50: 0.00000997\n",
            "Epoch  10 Total Loss  8.087518585867906e-06\n",
            "Starting epoch 11\n",
            "Loss after mini-batch    50: 0.00001003\n",
            "Epoch  11 Total Loss  8.149081502064862e-06\n",
            "Starting epoch 12\n",
            "Loss after mini-batch    50: 0.00000982\n",
            "Epoch  12 Total Loss  8.01173601246615e-06\n",
            "Starting epoch 13\n",
            "Loss after mini-batch    50: 0.00000983\n",
            "Epoch  13 Total Loss  7.96234758006066e-06\n",
            "Starting epoch 14\n",
            "Loss after mini-batch    50: 0.00000973\n",
            "Epoch  14 Total Loss  7.937592912905029e-06\n",
            "Starting epoch 15\n",
            "Loss after mini-batch    50: 0.00000977\n",
            "Epoch  15 Total Loss  8.032065282461945e-06\n",
            "Starting epoch 16\n",
            "Loss after mini-batch    50: 0.00000969\n",
            "Epoch  16 Total Loss  7.847815615580345e-06\n",
            "Starting epoch 17\n",
            "Loss after mini-batch    50: 0.00000996\n",
            "Epoch  17 Total Loss  8.064606785855114e-06\n",
            "Starting epoch 18\n",
            "Loss after mini-batch    50: 0.00000963\n",
            "Epoch  18 Total Loss  7.912211551151947e-06\n",
            "Starting epoch 19\n",
            "Loss after mini-batch    50: 0.00000971\n",
            "Epoch  19 Total Loss  7.932461183591934e-06\n",
            "Starting epoch 20\n",
            "Loss after mini-batch    50: 0.00000948\n",
            "Epoch  20 Total Loss  7.69058353755625e-06\n",
            "Starting epoch 21\n",
            "Loss after mini-batch    50: 0.00000932\n",
            "Epoch  21 Total Loss  7.631950626807438e-06\n",
            "Starting epoch 22\n",
            "Loss after mini-batch    50: 0.00000945\n",
            "Epoch  22 Total Loss  7.894968814506046e-06\n",
            "Starting epoch 23\n",
            "Loss after mini-batch    50: 0.00000948\n",
            "Epoch  23 Total Loss  7.94499425585096e-06\n",
            "Starting epoch 24\n",
            "Loss after mini-batch    50: 0.00000939\n",
            "Epoch  24 Total Loss  7.612819650226754e-06\n",
            "Starting epoch 25\n",
            "Loss after mini-batch    50: 0.00000944\n",
            "Epoch  25 Total Loss  7.567934992581806e-06\n",
            "Starting epoch 26\n",
            "Loss after mini-batch    50: 0.00000915\n",
            "Epoch  26 Total Loss  7.470415175644026e-06\n",
            "Starting epoch 27\n",
            "Loss after mini-batch    50: 0.00000990\n",
            "Epoch  27 Total Loss  8.124992289218172e-06\n",
            "Starting epoch 28\n",
            "Loss after mini-batch    50: 0.00000904\n",
            "Epoch  28 Total Loss  7.504770095318531e-06\n",
            "Starting epoch 29\n",
            "Loss after mini-batch    50: 0.00000843\n",
            "Epoch  29 Total Loss  7.056911465877194e-06\n",
            "Starting epoch 30\n",
            "Loss after mini-batch    50: 0.00000905\n",
            "Epoch  30 Total Loss  7.312764596114876e-06\n",
            "Starting epoch 31\n",
            "Loss after mini-batch    50: 0.00000817\n",
            "Epoch  31 Total Loss  6.736092566480186e-06\n",
            "Starting epoch 32\n",
            "Loss after mini-batch    50: 0.00000830\n",
            "Epoch  32 Total Loss  6.489736528836332e-06\n",
            "Starting epoch 33\n",
            "Loss after mini-batch    50: 0.00000791\n",
            "Epoch  33 Total Loss  6.034580404687715e-06\n",
            "Starting epoch 34\n",
            "Loss after mini-batch    50: 0.00000802\n",
            "Epoch  34 Total Loss  6.695983186539362e-06\n",
            "Starting epoch 35\n",
            "Loss after mini-batch    50: 0.00000747\n",
            "Epoch  35 Total Loss  5.976320681296461e-06\n",
            "Starting epoch 36\n",
            "Loss after mini-batch    50: 0.00000792\n",
            "Epoch  36 Total Loss  6.031704667927882e-06\n",
            "Starting epoch 37\n",
            "Loss after mini-batch    50: 0.00000780\n",
            "Epoch  37 Total Loss  6.198822828472493e-06\n",
            "Starting epoch 38\n",
            "Loss after mini-batch    50: 0.00000870\n",
            "Epoch  38 Total Loss  7.521850423532052e-06\n",
            "Starting epoch 39\n",
            "Loss after mini-batch    50: 0.00000743\n",
            "Epoch  39 Total Loss  6.47941932142694e-06\n",
            "Starting epoch 40\n",
            "Loss after mini-batch    50: 0.00000806\n",
            "Epoch  40 Total Loss  6.6396950167845065e-06\n",
            "Starting epoch 41\n",
            "Loss after mini-batch    50: 0.00000856\n",
            "Epoch  41 Total Loss  6.501115649173686e-06\n",
            "Starting epoch 42\n",
            "Loss after mini-batch    50: 0.00000885\n",
            "Epoch  42 Total Loss  7.857390354779367e-06\n",
            "Starting epoch 43\n",
            "Loss after mini-batch    50: 0.00000903\n",
            "Epoch  43 Total Loss  7.022802819451397e-06\n",
            "Starting epoch 44\n",
            "Loss after mini-batch    50: 0.00000787\n",
            "Epoch  44 Total Loss  5.992486944908195e-06\n",
            "Starting epoch 45\n",
            "Loss after mini-batch    50: 0.00000750\n",
            "Epoch  45 Total Loss  6.148502758263064e-06\n",
            "Starting epoch 46\n",
            "Loss after mini-batch    50: 0.00000709\n",
            "Epoch  46 Total Loss  5.6071946905106926e-06\n",
            "Starting epoch 47\n",
            "Loss after mini-batch    50: 0.00000697\n",
            "Epoch  47 Total Loss  5.207607624359136e-06\n",
            "Starting epoch 48\n",
            "Loss after mini-batch    50: 0.00000726\n",
            "Epoch  48 Total Loss  5.501715016379786e-06\n",
            "Starting epoch 49\n",
            "Loss after mini-batch    50: 0.00000658\n",
            "Epoch  49 Total Loss  5.232471463914281e-06\n",
            "Starting epoch 50\n",
            "Loss after mini-batch    50: 0.00000678\n",
            "Epoch  50 Total Loss  5.05661977018907e-06\n",
            "Starting epoch 51\n",
            "Loss after mini-batch    50: 0.00000664\n",
            "Epoch  51 Total Loss  5.0637118250090255e-06\n",
            "Starting epoch 52\n",
            "Loss after mini-batch    50: 0.00000742\n",
            "Epoch  52 Total Loss  5.97750523594836e-06\n",
            "Starting epoch 53\n",
            "Loss after mini-batch    50: 0.00000699\n",
            "Epoch  53 Total Loss  5.431568634886277e-06\n",
            "Starting epoch 54\n",
            "Loss after mini-batch    50: 0.00000679\n",
            "Epoch  54 Total Loss  5.110411604928681e-06\n",
            "Starting epoch 55\n",
            "Loss after mini-batch    50: 0.00000621\n",
            "Epoch  55 Total Loss  4.992447841677282e-06\n",
            "Starting epoch 56\n",
            "Loss after mini-batch    50: 0.00000641\n",
            "Epoch  56 Total Loss  4.745694411100053e-06\n",
            "Starting epoch 57\n",
            "Loss after mini-batch    50: 0.00000633\n",
            "Epoch  57 Total Loss  4.7824759582660986e-06\n",
            "Starting epoch 58\n",
            "Loss after mini-batch    50: 0.00000704\n",
            "Epoch  58 Total Loss  5.64177936151427e-06\n",
            "Starting epoch 59\n",
            "Loss after mini-batch    50: 0.00000661\n",
            "Epoch  59 Total Loss  5.216331064748934e-06\n",
            "Starting epoch 60\n",
            "Loss after mini-batch    50: 0.00000642\n",
            "Epoch  60 Total Loss  4.816048356238846e-06\n",
            "Starting epoch 61\n",
            "Loss after mini-batch    50: 0.00001094\n",
            "Epoch  61 Total Loss  8.850418800869045e-06\n",
            "Starting epoch 62\n",
            "Loss after mini-batch    50: 0.00000809\n",
            "Epoch  62 Total Loss  6.329438643155112e-06\n",
            "Starting epoch 63\n",
            "Loss after mini-batch    50: 0.00000623\n",
            "Epoch  63 Total Loss  4.8641671448468315e-06\n",
            "Starting epoch 64\n",
            "Loss after mini-batch    50: 0.00000599\n",
            "Epoch  64 Total Loss  4.532881323133293e-06\n",
            "Starting epoch 65\n",
            "Loss after mini-batch    50: 0.00000569\n",
            "Epoch  65 Total Loss  4.332100117249777e-06\n",
            "Starting epoch 66\n",
            "Loss after mini-batch    50: 0.00000658\n",
            "Epoch  66 Total Loss  5.030389364020967e-06\n",
            "Starting epoch 67\n",
            "Loss after mini-batch    50: 0.00000543\n",
            "Epoch  67 Total Loss  4.215230913577154e-06\n",
            "Starting epoch 68\n",
            "Loss after mini-batch    50: 0.00000576\n",
            "Epoch  68 Total Loss  4.292510187791581e-06\n",
            "Starting epoch 69\n",
            "Loss after mini-batch    50: 0.00000590\n",
            "Epoch  69 Total Loss  4.527465258277432e-06\n",
            "Starting epoch 70\n",
            "Loss after mini-batch    50: 0.00000522\n",
            "Epoch  70 Total Loss  3.9271158325495904e-06\n",
            "Starting epoch 71\n",
            "Loss after mini-batch    50: 0.00000873\n",
            "Epoch  71 Total Loss  6.5363648836185776e-06\n",
            "Starting epoch 72\n",
            "Loss after mini-batch    50: 0.00000680\n",
            "Epoch  72 Total Loss  5.1999910117099075e-06\n",
            "Starting epoch 73\n",
            "Loss after mini-batch    50: 0.00000560\n",
            "Epoch  73 Total Loss  4.34277412199159e-06\n",
            "Starting epoch 74\n",
            "Loss after mini-batch    50: 0.00000484\n",
            "Epoch  74 Total Loss  3.7447790538121715e-06\n",
            "Starting epoch 75\n",
            "Loss after mini-batch    50: 0.00000523\n",
            "Epoch  75 Total Loss  3.926401229654215e-06\n",
            "Starting epoch 76\n",
            "Loss after mini-batch    50: 0.00000443\n",
            "Epoch  76 Total Loss  3.3650989598794153e-06\n",
            "Starting epoch 77\n",
            "Loss after mini-batch    50: 0.00000514\n",
            "Epoch  77 Total Loss  3.7866519178278126e-06\n",
            "Starting epoch 78\n",
            "Loss after mini-batch    50: 0.00000408\n",
            "Epoch  78 Total Loss  3.1702203654819052e-06\n",
            "Starting epoch 79\n",
            "Loss after mini-batch    50: 0.00000494\n",
            "Epoch  79 Total Loss  3.7026377873111075e-06\n",
            "Starting epoch 80\n",
            "Loss after mini-batch    50: 0.00000395\n",
            "Epoch  80 Total Loss  2.9943693200529253e-06\n",
            "Starting epoch 81\n",
            "Loss after mini-batch    50: 0.00000502\n",
            "Epoch  81 Total Loss  3.829563038336579e-06\n",
            "Starting epoch 82\n",
            "Loss after mini-batch    50: 0.00000610\n",
            "Epoch  82 Total Loss  4.519122846799759e-06\n",
            "Starting epoch 83\n",
            "Loss after mini-batch    50: 0.00000541\n",
            "Epoch  83 Total Loss  3.976787876015643e-06\n",
            "Starting epoch 84\n",
            "Loss after mini-batch    50: 0.00000492\n",
            "Epoch  84 Total Loss  3.7541293608593187e-06\n",
            "Starting epoch 85\n",
            "Loss after mini-batch    50: 0.00000778\n",
            "Epoch  85 Total Loss  5.734475853749446e-06\n",
            "Starting epoch 86\n",
            "Loss after mini-batch    50: 0.00000647\n",
            "Epoch  86 Total Loss  4.802915682376883e-06\n",
            "Starting epoch 87\n",
            "Loss after mini-batch    50: 0.00000565\n",
            "Epoch  87 Total Loss  4.436073470978938e-06\n",
            "Starting epoch 88\n",
            "Loss after mini-batch    50: 0.00000539\n",
            "Epoch  88 Total Loss  3.9973115405612785e-06\n",
            "Starting epoch 89\n",
            "Loss after mini-batch    50: 0.00000579\n",
            "Epoch  89 Total Loss  4.400329976814474e-06\n",
            "Starting epoch 90\n",
            "Loss after mini-batch    50: 0.00000654\n",
            "Epoch  90 Total Loss  5.016638737408306e-06\n",
            "Starting epoch 91\n",
            "Loss after mini-batch    50: 0.00000592\n",
            "Epoch  91 Total Loss  5.065610376818617e-06\n",
            "Starting epoch 92\n",
            "Loss after mini-batch    50: 0.00000571\n",
            "Epoch  92 Total Loss  4.380170897075628e-06\n",
            "Starting epoch 93\n",
            "Loss after mini-batch    50: 0.00000467\n",
            "Epoch  93 Total Loss  3.5199602699265142e-06\n",
            "Starting epoch 94\n",
            "Loss after mini-batch    50: 0.00001436\n",
            "Epoch  94 Total Loss  1.0396944418363438e-05\n",
            "Starting epoch 95\n",
            "Loss after mini-batch    50: 0.00001008\n",
            "Epoch  95 Total Loss  8.666030245950572e-06\n",
            "Starting epoch 96\n",
            "Loss after mini-batch    50: 0.00000893\n",
            "Epoch  96 Total Loss  7.159517771946098e-06\n",
            "Starting epoch 97\n",
            "Loss after mini-batch    50: 0.00000803\n",
            "Epoch  97 Total Loss  6.543420069461527e-06\n",
            "Starting epoch 98\n",
            "Loss after mini-batch    50: 0.00000729\n",
            "Epoch  98 Total Loss  5.977792232597028e-06\n",
            "Starting epoch 99\n",
            "Loss after mini-batch    50: 0.00000759\n",
            "Epoch  99 Total Loss  5.8306484440359994e-06\n",
            "Starting epoch 100\n",
            "Loss after mini-batch    50: 0.00000634\n",
            "Epoch  100 Total Loss  5.2262998952014155e-06\n",
            "Starting epoch 101\n",
            "Loss after mini-batch    50: 0.00000737\n",
            "Epoch  101 Total Loss  6.163845154234022e-06\n",
            "Starting epoch 102\n",
            "Loss after mini-batch    50: 0.00000679\n",
            "Epoch  102 Total Loss  5.239562499796217e-06\n",
            "Starting epoch 103\n",
            "Loss after mini-batch    50: 0.00000622\n",
            "Epoch  103 Total Loss  4.6954623557452875e-06\n",
            "Starting epoch 104\n",
            "Loss after mini-batch    50: 0.00000806\n",
            "Epoch  104 Total Loss  6.294357724688658e-06\n",
            "Starting epoch 105\n",
            "Loss after mini-batch    50: 0.00000709\n",
            "Epoch  105 Total Loss  5.2033030199824835e-06\n",
            "Starting epoch 106\n",
            "Loss after mini-batch    50: 0.00000668\n",
            "Epoch  106 Total Loss  4.980190099497979e-06\n",
            "Starting epoch 107\n",
            "Loss after mini-batch    50: 0.00000500\n",
            "Epoch  107 Total Loss  3.817992698415848e-06\n",
            "Starting epoch 108\n",
            "Loss after mini-batch    50: 0.00000510\n",
            "Epoch  108 Total Loss  3.818891094168701e-06\n",
            "Starting epoch 109\n",
            "Loss after mini-batch    50: 0.00000452\n",
            "Epoch  109 Total Loss  3.485677367977126e-06\n",
            "Starting epoch 110\n",
            "Loss after mini-batch    50: 0.00000457\n",
            "Epoch  110 Total Loss  3.3964294483792813e-06\n",
            "Starting epoch 111\n",
            "Loss after mini-batch    50: 0.00000421\n",
            "Epoch  111 Total Loss  3.197362004418262e-06\n",
            "Starting epoch 112\n",
            "Loss after mini-batch    50: 0.00000407\n",
            "Epoch  112 Total Loss  3.1366902688074247e-06\n",
            "Starting epoch 113\n",
            "Loss after mini-batch    50: 0.00000391\n",
            "Epoch  113 Total Loss  3.0592014373819076e-06\n",
            "Starting epoch 114\n",
            "Loss after mini-batch    50: 0.00000402\n",
            "Epoch  114 Total Loss  3.0995280428195806e-06\n",
            "Starting epoch 115\n",
            "Loss after mini-batch    50: 0.00000386\n",
            "Epoch  115 Total Loss  2.911264278957246e-06\n",
            "Starting epoch 116\n",
            "Loss after mini-batch    50: 0.00000367\n",
            "Epoch  116 Total Loss  2.760742750382603e-06\n",
            "Starting epoch 117\n",
            "Loss after mini-batch    50: 0.00000362\n",
            "Epoch  117 Total Loss  2.7423436003765686e-06\n",
            "Starting epoch 118\n",
            "Loss after mini-batch    50: 0.00000462\n",
            "Epoch  118 Total Loss  3.465752683531578e-06\n",
            "Starting epoch 119\n",
            "Loss after mini-batch    50: 0.00000362\n",
            "Epoch  119 Total Loss  2.8046124082889732e-06\n",
            "Starting epoch 120\n",
            "Loss after mini-batch    50: 0.00000359\n",
            "Epoch  120 Total Loss  2.7811746216145473e-06\n",
            "Starting epoch 121\n",
            "Loss after mini-batch    50: 0.00000355\n",
            "Epoch  121 Total Loss  2.7651246951402677e-06\n",
            "Starting epoch 122\n",
            "Loss after mini-batch    50: 0.00000374\n",
            "Epoch  122 Total Loss  2.8792663335034754e-06\n",
            "Starting epoch 123\n",
            "Loss after mini-batch    50: 0.00000394\n",
            "Epoch  123 Total Loss  3.0382909365448776e-06\n",
            "Starting epoch 124\n",
            "Loss after mini-batch    50: 0.00000722\n",
            "Epoch  124 Total Loss  5.357012403056021e-06\n",
            "Starting epoch 125\n",
            "Loss after mini-batch    50: 0.00000677\n",
            "Epoch  125 Total Loss  5.696617746828478e-06\n",
            "Starting epoch 126\n",
            "Loss after mini-batch    50: 0.00000529\n",
            "Epoch  126 Total Loss  3.9193126318467124e-06\n",
            "Starting epoch 127\n",
            "Loss after mini-batch    50: 0.00000524\n",
            "Epoch  127 Total Loss  4.091156929996052e-06\n",
            "Starting epoch 128\n",
            "Loss after mini-batch    50: 0.00000394\n",
            "Epoch  128 Total Loss  2.9415090162644553e-06\n",
            "Starting epoch 129\n",
            "Loss after mini-batch    50: 0.00000515\n",
            "Epoch  129 Total Loss  4.021905281999712e-06\n",
            "Starting epoch 130\n",
            "Loss after mini-batch    50: 0.00000393\n",
            "Epoch  130 Total Loss  3.0432067289599808e-06\n",
            "Starting epoch 131\n",
            "Loss after mini-batch    50: 0.00000630\n",
            "Epoch  131 Total Loss  4.640784970163144e-06\n",
            "Starting epoch 132\n",
            "Loss after mini-batch    50: 0.00000364\n",
            "Epoch  132 Total Loss  2.7488528774938855e-06\n",
            "Starting epoch 133\n",
            "Loss after mini-batch    50: 0.00000339\n",
            "Epoch  133 Total Loss  2.527473293397383e-06\n",
            "Starting epoch 134\n",
            "Loss after mini-batch    50: 0.00000359\n",
            "Epoch  134 Total Loss  2.6607706053511434e-06\n",
            "Starting epoch 135\n",
            "Loss after mini-batch    50: 0.00000352\n",
            "Epoch  135 Total Loss  2.714291189241812e-06\n",
            "Starting epoch 136\n",
            "Loss after mini-batch    50: 0.00000337\n",
            "Epoch  136 Total Loss  2.5159096284828784e-06\n",
            "Starting epoch 137\n",
            "Loss after mini-batch    50: 0.00000353\n",
            "Epoch  137 Total Loss  2.625460694948166e-06\n",
            "Starting epoch 138\n",
            "Loss after mini-batch    50: 0.00000347\n",
            "Epoch  138 Total Loss  2.5713827375521228e-06\n",
            "Starting epoch 139\n",
            "Loss after mini-batch    50: 0.00000338\n",
            "Epoch  139 Total Loss  2.500886430315911e-06\n",
            "Starting epoch 140\n",
            "Loss after mini-batch    50: 0.00000328\n",
            "Epoch  140 Total Loss  2.4426689359844132e-06\n",
            "Starting epoch 141\n",
            "Loss after mini-batch    50: 0.00000329\n",
            "Epoch  141 Total Loss  2.4871180971509766e-06\n",
            "Starting epoch 142\n",
            "Loss after mini-batch    50: 0.00000341\n",
            "Epoch  142 Total Loss  2.521894860397901e-06\n",
            "Starting epoch 143\n",
            "Loss after mini-batch    50: 0.00000345\n",
            "Epoch  143 Total Loss  2.5450633601339e-06\n",
            "Starting epoch 144\n",
            "Loss after mini-batch    50: 0.00000322\n",
            "Epoch  144 Total Loss  2.3942373304682483e-06\n",
            "Starting epoch 145\n",
            "Loss after mini-batch    50: 0.00000344\n",
            "Epoch  145 Total Loss  2.5212305907462707e-06\n",
            "Starting epoch 146\n",
            "Loss after mini-batch    50: 0.00000356\n",
            "Epoch  146 Total Loss  2.7668576350552912e-06\n",
            "Starting epoch 147\n",
            "Loss after mini-batch    50: 0.00000331\n",
            "Epoch  147 Total Loss  2.5338348783598974e-06\n",
            "Starting epoch 148\n",
            "Loss after mini-batch    50: 0.00000398\n",
            "Epoch  148 Total Loss  2.9525089671898918e-06\n",
            "Starting epoch 149\n",
            "Loss after mini-batch    50: 0.00000360\n",
            "Epoch  149 Total Loss  2.7277098000569013e-06\n",
            "Starting epoch 150\n",
            "Loss after mini-batch    50: 0.00000342\n",
            "Epoch  150 Total Loss  2.6038086056661628e-06\n",
            "Starting epoch 151\n",
            "Loss after mini-batch    50: 0.00000333\n",
            "Epoch  151 Total Loss  2.6084287972535e-06\n",
            "Starting epoch 152\n",
            "Loss after mini-batch    50: 0.00000373\n",
            "Epoch  152 Total Loss  2.8902011658517147e-06\n",
            "Starting epoch 153\n",
            "Loss after mini-batch    50: 0.00000366\n",
            "Epoch  153 Total Loss  2.8627043636091856e-06\n",
            "Starting epoch 154\n",
            "Loss after mini-batch    50: 0.00000385\n",
            "Epoch  154 Total Loss  3.000559022433199e-06\n",
            "Starting epoch 155\n",
            "Loss after mini-batch    50: 0.00000350\n",
            "Epoch  155 Total Loss  2.6031950431320497e-06\n",
            "Starting epoch 156\n",
            "Loss after mini-batch    50: 0.00000408\n",
            "Epoch  156 Total Loss  3.0931321264304756e-06\n",
            "Starting epoch 157\n",
            "Loss after mini-batch    50: 0.00000346\n",
            "Epoch  157 Total Loss  2.8049012082112214e-06\n",
            "Starting epoch 158\n",
            "Loss after mini-batch    50: 0.00000398\n",
            "Epoch  158 Total Loss  2.926149559348248e-06\n",
            "Starting epoch 159\n",
            "Loss after mini-batch    50: 0.00000375\n",
            "Epoch  159 Total Loss  2.752902528256761e-06\n",
            "Starting epoch 160\n",
            "Loss after mini-batch    50: 0.00000386\n",
            "Epoch  160 Total Loss  3.0782088089351432e-06\n",
            "Starting epoch 161\n",
            "Loss after mini-batch    50: 0.00000335\n",
            "Epoch  161 Total Loss  2.5023788366132195e-06\n",
            "Starting epoch 162\n",
            "Loss after mini-batch    50: 0.00000325\n",
            "Epoch  162 Total Loss  2.4686006653901812e-06\n",
            "Starting epoch 163\n",
            "Loss after mini-batch    50: 0.00000318\n",
            "Epoch  163 Total Loss  2.431003302685555e-06\n",
            "Starting epoch 164\n",
            "Loss after mini-batch    50: 0.00000312\n",
            "Epoch  164 Total Loss  2.3298166574499866e-06\n",
            "Starting epoch 165\n",
            "Loss after mini-batch    50: 0.00000344\n",
            "Epoch  165 Total Loss  2.5137393954278842e-06\n",
            "Starting epoch 166\n",
            "Loss after mini-batch    50: 0.00000344\n",
            "Epoch  166 Total Loss  2.5427493551709467e-06\n",
            "Starting epoch 167\n",
            "Loss after mini-batch    50: 0.00000318\n",
            "Epoch  167 Total Loss  2.4258076951610974e-06\n",
            "Starting epoch 168\n",
            "Loss after mini-batch    50: 0.00000412\n",
            "Epoch  168 Total Loss  3.094237327398256e-06\n",
            "Starting epoch 169\n",
            "Loss after mini-batch    50: 0.00000456\n",
            "Epoch  169 Total Loss  3.361914451962791e-06\n",
            "Starting epoch 170\n",
            "Loss after mini-batch    50: 0.00000639\n",
            "Epoch  170 Total Loss  4.632062624157187e-06\n",
            "Starting epoch 171\n",
            "Loss after mini-batch    50: 0.00000707\n",
            "Epoch  171 Total Loss  5.145187731401022e-06\n",
            "Starting epoch 172\n",
            "Loss after mini-batch    50: 0.00000422\n",
            "Epoch  172 Total Loss  3.1377358438660566e-06\n",
            "Starting epoch 173\n",
            "Loss after mini-batch    50: 0.00001087\n",
            "Epoch  173 Total Loss  7.735081897614688e-06\n",
            "Starting epoch 174\n",
            "Loss after mini-batch    50: 0.00001273\n",
            "Epoch  174 Total Loss  9.122152337627378e-06\n",
            "Starting epoch 175\n",
            "Loss after mini-batch    50: 0.00000577\n",
            "Epoch  175 Total Loss  4.139328635830195e-06\n",
            "Starting epoch 176\n",
            "Loss after mini-batch    50: 0.00000476\n",
            "Epoch  176 Total Loss  3.4344576695439285e-06\n",
            "Starting epoch 177\n",
            "Loss after mini-batch    50: 0.00000470\n",
            "Epoch  177 Total Loss  3.423755048852999e-06\n",
            "Starting epoch 178\n",
            "Loss after mini-batch    50: 0.00000431\n",
            "Epoch  178 Total Loss  3.1096314399508488e-06\n",
            "Starting epoch 179\n",
            "Loss after mini-batch    50: 0.00000410\n",
            "Epoch  179 Total Loss  2.9864103093431757e-06\n",
            "Starting epoch 180\n",
            "Loss after mini-batch    50: 0.00000423\n",
            "Epoch  180 Total Loss  3.0373342115740264e-06\n",
            "Starting epoch 181\n",
            "Loss after mini-batch    50: 0.00000449\n",
            "Epoch  181 Total Loss  3.295595677515411e-06\n",
            "Starting epoch 182\n",
            "Loss after mini-batch    50: 0.00000433\n",
            "Epoch  182 Total Loss  3.138892180374084e-06\n",
            "Starting epoch 183\n",
            "Loss after mini-batch    50: 0.00000465\n",
            "Epoch  183 Total Loss  3.3603489227010088e-06\n",
            "Starting epoch 184\n",
            "Loss after mini-batch    50: 0.00000341\n",
            "Epoch  184 Total Loss  2.4724686884525227e-06\n",
            "Starting epoch 185\n",
            "Loss after mini-batch    50: 0.00000325\n",
            "Epoch  185 Total Loss  2.393476751633767e-06\n",
            "Starting epoch 186\n",
            "Loss after mini-batch    50: 0.00000329\n",
            "Epoch  186 Total Loss  2.4133180795230893e-06\n",
            "Starting epoch 187\n",
            "Loss after mini-batch    50: 0.00000333\n",
            "Epoch  187 Total Loss  2.485806559235668e-06\n",
            "Starting epoch 188\n",
            "Loss after mini-batch    50: 0.00000335\n",
            "Epoch  188 Total Loss  2.4634373004940436e-06\n",
            "Starting epoch 189\n",
            "Loss after mini-batch    50: 0.00000328\n",
            "Epoch  189 Total Loss  2.3842063190886934e-06\n",
            "Starting epoch 190\n",
            "Loss after mini-batch    50: 0.00000327\n",
            "Epoch  190 Total Loss  2.3842229244971e-06\n",
            "Starting epoch 191\n",
            "Loss after mini-batch    50: 0.00000329\n",
            "Epoch  191 Total Loss  2.4075286983510976e-06\n",
            "Starting epoch 192\n",
            "Loss after mini-batch    50: 0.00000313\n",
            "Epoch  192 Total Loss  2.3428503404717325e-06\n",
            "Starting epoch 193\n",
            "Loss after mini-batch    50: 0.00000318\n",
            "Epoch  193 Total Loss  2.289369483148778e-06\n",
            "Starting epoch 194\n",
            "Loss after mini-batch    50: 0.00000324\n",
            "Epoch  194 Total Loss  2.3656574681890022e-06\n",
            "Starting epoch 195\n",
            "Loss after mini-batch    50: 0.00000346\n",
            "Epoch  195 Total Loss  2.500101230090084e-06\n",
            "Starting epoch 196\n",
            "Loss after mini-batch    50: 0.00000321\n",
            "Epoch  196 Total Loss  2.377176539244038e-06\n",
            "Starting epoch 197\n",
            "Loss after mini-batch    50: 0.00000318\n",
            "Epoch  197 Total Loss  2.3151563108589655e-06\n",
            "Starting epoch 198\n",
            "Loss after mini-batch    50: 0.00000374\n",
            "Epoch  198 Total Loss  2.878049886264241e-06\n",
            "Starting epoch 199\n",
            "Loss after mini-batch    50: 0.00000345\n",
            "Epoch  199 Total Loss  2.4869906202824096e-06\n",
            "Starting epoch 200\n",
            "Loss after mini-batch    50: 0.00000357\n",
            "Epoch  200 Total Loss  2.613495655865192e-06\n",
            "Starting epoch 201\n",
            "Loss after mini-batch    50: 0.00000359\n",
            "Epoch  201 Total Loss  2.6594284416623125e-06\n",
            "Starting epoch 202\n",
            "Loss after mini-batch    50: 0.00000337\n",
            "Epoch  202 Total Loss  2.4760115088599357e-06\n",
            "Starting epoch 203\n",
            "Loss after mini-batch    50: 0.00000354\n",
            "Epoch  203 Total Loss  2.539524886873639e-06\n",
            "Starting epoch 204\n",
            "Loss after mini-batch    50: 0.00000355\n",
            "Epoch  204 Total Loss  2.612619700729605e-06\n",
            "Starting epoch 205\n",
            "Loss after mini-batch    50: 0.00000319\n",
            "Epoch  205 Total Loss  2.364619680826784e-06\n",
            "Starting epoch 206\n",
            "Loss after mini-batch    50: 0.00000311\n",
            "Epoch  206 Total Loss  2.2229799863845065e-06\n",
            "Starting epoch 207\n",
            "Loss after mini-batch    50: 0.00000352\n",
            "Epoch  207 Total Loss  2.52057912828205e-06\n",
            "Starting epoch 208\n",
            "Loss after mini-batch    50: 0.00000366\n",
            "Epoch  208 Total Loss  2.6618835312507293e-06\n",
            "Starting epoch 209\n",
            "Loss after mini-batch    50: 0.00000324\n",
            "Epoch  209 Total Loss  2.3410094398299307e-06\n",
            "Starting epoch 210\n",
            "Loss after mini-batch    50: 0.00000328\n",
            "Epoch  210 Total Loss  2.3514124492221257e-06\n",
            "Starting epoch 211\n",
            "Loss after mini-batch    50: 0.00000322\n",
            "Epoch  211 Total Loss  2.3140479243652677e-06\n",
            "Starting epoch 212\n",
            "Loss after mini-batch    50: 0.00000311\n",
            "Epoch  212 Total Loss  2.2409905032155167e-06\n",
            "Starting epoch 213\n",
            "Loss after mini-batch    50: 0.00000309\n",
            "Epoch  213 Total Loss  2.2152453419287276e-06\n",
            "Starting epoch 214\n",
            "Loss after mini-batch    50: 0.00000296\n",
            "Epoch  214 Total Loss  2.1438423737486623e-06\n",
            "Starting epoch 215\n",
            "Loss after mini-batch    50: 0.00000312\n",
            "Epoch  215 Total Loss  2.2535819271190463e-06\n",
            "Starting epoch 216\n",
            "Loss after mini-batch    50: 0.00000329\n",
            "Epoch  216 Total Loss  2.399404150008364e-06\n",
            "Starting epoch 217\n",
            "Loss after mini-batch    50: 0.00000343\n",
            "Epoch  217 Total Loss  2.4830742251482875e-06\n",
            "Starting epoch 218\n",
            "Loss after mini-batch    50: 0.00000362\n",
            "Epoch  218 Total Loss  2.6220517195745744e-06\n",
            "Starting epoch 219\n",
            "Loss after mini-batch    50: 0.00000354\n",
            "Epoch  219 Total Loss  2.6130812953933316e-06\n",
            "Starting epoch 220\n",
            "Loss after mini-batch    50: 0.00000349\n",
            "Epoch  220 Total Loss  2.4959243827776193e-06\n",
            "Starting epoch 221\n",
            "Loss after mini-batch    50: 0.00000331\n",
            "Epoch  221 Total Loss  2.404897820193794e-06\n",
            "Starting epoch 222\n",
            "Loss after mini-batch    50: 0.00000337\n",
            "Epoch  222 Total Loss  2.50780202243956e-06\n",
            "Starting epoch 223\n",
            "Loss after mini-batch    50: 0.00000421\n",
            "Epoch  223 Total Loss  3.031731299563134e-06\n",
            "Starting epoch 224\n",
            "Loss after mini-batch    50: 0.00000614\n",
            "Epoch  224 Total Loss  4.439542568383341e-06\n",
            "Starting epoch 225\n",
            "Loss after mini-batch    50: 0.00000529\n",
            "Epoch  225 Total Loss  3.7895609537041963e-06\n",
            "Starting epoch 226\n",
            "Loss after mini-batch    50: 0.00000480\n",
            "Epoch  226 Total Loss  3.4142133968166157e-06\n",
            "Starting epoch 227\n",
            "Loss after mini-batch    50: 0.00000476\n",
            "Epoch  227 Total Loss  3.3726440704404424e-06\n",
            "Starting epoch 228\n",
            "Loss after mini-batch    50: 0.00000435\n",
            "Epoch  228 Total Loss  3.104531546850432e-06\n",
            "Starting epoch 229\n",
            "Loss after mini-batch    50: 0.00000827\n",
            "Epoch  229 Total Loss  6.0077479240552535e-06\n",
            "Starting epoch 230\n",
            "Loss after mini-batch    50: 0.00000351\n",
            "Epoch  230 Total Loss  2.55503651458433e-06\n",
            "Starting epoch 231\n",
            "Loss after mini-batch    50: 0.00000330\n",
            "Epoch  231 Total Loss  2.3792900184875046e-06\n",
            "Starting epoch 232\n",
            "Loss after mini-batch    50: 0.00000338\n",
            "Epoch  232 Total Loss  2.485320789021491e-06\n",
            "Starting epoch 233\n",
            "Loss after mini-batch    50: 0.00000333\n",
            "Epoch  233 Total Loss  2.490826960096134e-06\n",
            "Starting epoch 234\n",
            "Loss after mini-batch    50: 0.00000309\n",
            "Epoch  234 Total Loss  2.2001224022134066e-06\n",
            "Starting epoch 235\n",
            "Loss after mini-batch    50: 0.00000315\n",
            "Epoch  235 Total Loss  2.248202754209474e-06\n",
            "Starting epoch 236\n",
            "Loss after mini-batch    50: 0.00000316\n",
            "Epoch  236 Total Loss  2.2504234154939617e-06\n",
            "Starting epoch 237\n",
            "Loss after mini-batch    50: 0.00000343\n",
            "Epoch  237 Total Loss  2.540895875628261e-06\n",
            "Starting epoch 238\n",
            "Loss after mini-batch    50: 0.00000307\n",
            "Epoch  238 Total Loss  2.1998465005168555e-06\n",
            "Starting epoch 239\n",
            "Loss after mini-batch    50: 0.00000395\n",
            "Epoch  239 Total Loss  3.197987248842398e-06\n",
            "Starting epoch 240\n",
            "Loss after mini-batch    50: 0.00000335\n",
            "Epoch  240 Total Loss  2.3770856357370937e-06\n",
            "Starting epoch 241\n",
            "Loss after mini-batch    50: 0.00000318\n",
            "Epoch  241 Total Loss  2.2402273249048457e-06\n",
            "Starting epoch 242\n",
            "Loss after mini-batch    50: 0.00000316\n",
            "Epoch  242 Total Loss  2.2373457174830314e-06\n",
            "Starting epoch 243\n",
            "Loss after mini-batch    50: 0.00000363\n",
            "Epoch  243 Total Loss  2.6148431989546516e-06\n",
            "Starting epoch 244\n",
            "Loss after mini-batch    50: 0.00000362\n",
            "Epoch  244 Total Loss  2.6518633589785643e-06\n",
            "Starting epoch 245\n",
            "Loss after mini-batch    50: 0.00000349\n",
            "Epoch  245 Total Loss  2.479549474015346e-06\n",
            "Starting epoch 246\n",
            "Loss after mini-batch    50: 0.00000343\n",
            "Epoch  246 Total Loss  2.4183114039436995e-06\n",
            "Starting epoch 247\n",
            "Loss after mini-batch    50: 0.00000337\n",
            "Epoch  247 Total Loss  2.421495635168074e-06\n",
            "Starting epoch 248\n",
            "Loss after mini-batch    50: 0.00000328\n",
            "Epoch  248 Total Loss  2.325291320554652e-06\n",
            "Starting epoch 249\n",
            "Loss after mini-batch    50: 0.00000323\n",
            "Epoch  249 Total Loss  2.2969953537504354e-06\n",
            "Starting epoch 250\n",
            "Loss after mini-batch    50: 0.00000320\n",
            "Epoch  250 Total Loss  2.258560954778351e-06\n",
            "Starting epoch 251\n",
            "Loss after mini-batch    50: 0.00000320\n",
            "Epoch  251 Total Loss  2.281584522546688e-06\n",
            "Starting epoch 252\n",
            "Loss after mini-batch    50: 0.00000347\n",
            "Epoch  252 Total Loss  2.441339328005141e-06\n",
            "Starting epoch 253\n",
            "Loss after mini-batch    50: 0.00000359\n",
            "Epoch  253 Total Loss  2.5616420945685626e-06\n",
            "Starting epoch 254\n",
            "Loss after mini-batch    50: 0.00000341\n",
            "Epoch  254 Total Loss  2.4522478549757508e-06\n",
            "Starting epoch 255\n",
            "Loss after mini-batch    50: 0.00000331\n",
            "Epoch  255 Total Loss  2.375143280349453e-06\n",
            "Starting epoch 256\n",
            "Loss after mini-batch    50: 0.00000334\n",
            "Epoch  256 Total Loss  2.4067511998410792e-06\n",
            "Starting epoch 257\n",
            "Loss after mini-batch    50: 0.00000460\n",
            "Epoch  257 Total Loss  3.3692165016930965e-06\n",
            "Starting epoch 258\n",
            "Loss after mini-batch    50: 0.00001966\n",
            "Epoch  258 Total Loss  1.4076282570638395e-05\n",
            "Starting epoch 259\n",
            "Loss after mini-batch    50: 0.00001322\n",
            "Epoch  259 Total Loss  1.0876302037739668e-05\n",
            "Starting epoch 260\n",
            "Loss after mini-batch    50: 0.00001039\n",
            "Epoch  260 Total Loss  9.741945690292889e-06\n",
            "Starting epoch 261\n",
            "Loss after mini-batch    50: 0.00000915\n",
            "Epoch  261 Total Loss  7.1167001861234995e-06\n",
            "Starting epoch 262\n",
            "Loss after mini-batch    50: 0.00000817\n",
            "Epoch  262 Total Loss  6.561005873459733e-06\n",
            "Starting epoch 263\n",
            "Loss after mini-batch    50: 0.00000801\n",
            "Epoch  263 Total Loss  6.358942854130707e-06\n",
            "Starting epoch 264\n",
            "Loss after mini-batch    50: 0.00001038\n",
            "Epoch  264 Total Loss  8.009878985834914e-06\n",
            "Starting epoch 265\n",
            "Loss after mini-batch    50: 0.00000783\n",
            "Epoch  265 Total Loss  6.25071499284048e-06\n",
            "Starting epoch 266\n",
            "Loss after mini-batch    50: 0.00000633\n",
            "Epoch  266 Total Loss  5.191652924254855e-06\n",
            "Starting epoch 267\n",
            "Loss after mini-batch    50: 0.00000583\n",
            "Epoch  267 Total Loss  4.662232377445403e-06\n",
            "Starting epoch 268\n",
            "Loss after mini-batch    50: 0.00000568\n",
            "Epoch  268 Total Loss  4.473747515258235e-06\n",
            "Starting epoch 269\n",
            "Loss after mini-batch    50: 0.00000543\n",
            "Epoch  269 Total Loss  4.273796038697005e-06\n",
            "Starting epoch 270\n",
            "Loss after mini-batch    50: 0.00000435\n",
            "Epoch  270 Total Loss  3.594272282494673e-06\n",
            "Starting epoch 271\n",
            "Loss after mini-batch    50: 0.00000412\n",
            "Epoch  271 Total Loss  3.273100618193957e-06\n",
            "Starting epoch 272\n",
            "Loss after mini-batch    50: 0.00000476\n",
            "Epoch  272 Total Loss  3.849037743765117e-06\n",
            "Starting epoch 273\n",
            "Loss after mini-batch    50: 0.00000555\n",
            "Epoch  273 Total Loss  4.728164296900299e-06\n",
            "Starting epoch 274\n",
            "Loss after mini-batch    50: 0.00000480\n",
            "Epoch  274 Total Loss  3.5710900391104793e-06\n",
            "Starting epoch 275\n",
            "Loss after mini-batch    50: 0.00000352\n",
            "Epoch  275 Total Loss  2.8539567380470032e-06\n",
            "Starting epoch 276\n",
            "Loss after mini-batch    50: 0.00000333\n",
            "Epoch  276 Total Loss  2.5958115109266564e-06\n",
            "Starting epoch 277\n",
            "Loss after mini-batch    50: 0.00000355\n",
            "Epoch  277 Total Loss  2.667362523226894e-06\n",
            "Starting epoch 278\n",
            "Loss after mini-batch    50: 0.00000377\n",
            "Epoch  278 Total Loss  2.9377127895240876e-06\n",
            "Starting epoch 279\n",
            "Loss after mini-batch    50: 0.00000333\n",
            "Epoch  279 Total Loss  2.536254443180002e-06\n",
            "Starting epoch 280\n",
            "Loss after mini-batch    50: 0.00000342\n",
            "Epoch  280 Total Loss  2.53588998755383e-06\n",
            "Starting epoch 281\n",
            "Loss after mini-batch    50: 0.00000322\n",
            "Epoch  281 Total Loss  2.5000417931655662e-06\n",
            "Starting epoch 282\n",
            "Loss after mini-batch    50: 0.00000347\n",
            "Epoch  282 Total Loss  2.549367408757577e-06\n",
            "Starting epoch 283\n",
            "Loss after mini-batch    50: 0.00000335\n",
            "Epoch  283 Total Loss  2.531441376666758e-06\n",
            "Starting epoch 284\n",
            "Loss after mini-batch    50: 0.00000328\n",
            "Epoch  284 Total Loss  2.459122841259721e-06\n",
            "Starting epoch 285\n",
            "Loss after mini-batch    50: 0.00000360\n",
            "Epoch  285 Total Loss  2.7652799958261595e-06\n",
            "Starting epoch 286\n",
            "Loss after mini-batch    50: 0.00000320\n",
            "Epoch  286 Total Loss  2.401456730188261e-06\n",
            "Starting epoch 287\n",
            "Loss after mini-batch    50: 0.00000332\n",
            "Epoch  287 Total Loss  2.499122283561306e-06\n",
            "Starting epoch 288\n",
            "Loss after mini-batch    50: 0.00000338\n",
            "Epoch  288 Total Loss  2.5303323051901258e-06\n",
            "Starting epoch 289\n",
            "Loss after mini-batch    50: 0.00000327\n",
            "Epoch  289 Total Loss  2.4331772362372483e-06\n",
            "Starting epoch 290\n",
            "Loss after mini-batch    50: 0.00000349\n",
            "Epoch  290 Total Loss  2.609635143486284e-06\n",
            "Starting epoch 291\n",
            "Loss after mini-batch    50: 0.00000329\n",
            "Epoch  291 Total Loss  2.47569854118347e-06\n",
            "Starting epoch 292\n",
            "Loss after mini-batch    50: 0.00000322\n",
            "Epoch  292 Total Loss  2.426143193900998e-06\n",
            "Starting epoch 293\n",
            "Loss after mini-batch    50: 0.00000359\n",
            "Epoch  293 Total Loss  2.633613165794715e-06\n",
            "Starting epoch 294\n",
            "Loss after mini-batch    50: 0.00000369\n",
            "Epoch  294 Total Loss  2.8335301926535947e-06\n",
            "Starting epoch 295\n",
            "Loss after mini-batch    50: 0.00000298\n",
            "Epoch  295 Total Loss  2.213458937521611e-06\n",
            "Starting epoch 296\n",
            "Loss after mini-batch    50: 0.00000329\n",
            "Epoch  296 Total Loss  2.4150051477119425e-06\n",
            "Starting epoch 297\n",
            "Loss after mini-batch    50: 0.00000341\n",
            "Epoch  297 Total Loss  2.4675240159045665e-06\n",
            "Starting epoch 298\n",
            "Loss after mini-batch    50: 0.00000338\n",
            "Epoch  298 Total Loss  2.452702880301811e-06\n",
            "Starting epoch 299\n",
            "Loss after mini-batch    50: 0.00000314\n",
            "Epoch  299 Total Loss  2.2800170182474885e-06\n",
            "Starting epoch 300\n",
            "Loss after mini-batch    50: 0.00000306\n",
            "Epoch  300 Total Loss  2.2265482892228996e-06\n",
            "Starting epoch 301\n",
            "Loss after mini-batch    50: 0.00000300\n",
            "Epoch  301 Total Loss  2.1634154413351247e-06\n",
            "Starting epoch 302\n",
            "Loss after mini-batch    50: 0.00000327\n",
            "Epoch  302 Total Loss  2.3833834210180213e-06\n",
            "Starting epoch 303\n",
            "Loss after mini-batch    50: 0.00000301\n",
            "Epoch  303 Total Loss  2.2234422445175127e-06\n",
            "Starting epoch 304\n",
            "Loss after mini-batch    50: 0.00000353\n",
            "Epoch  304 Total Loss  2.7207915275588115e-06\n",
            "Starting epoch 305\n",
            "Loss after mini-batch    50: 0.00000442\n",
            "Epoch  305 Total Loss  3.1946173318549618e-06\n",
            "Starting epoch 306\n",
            "Loss after mini-batch    50: 0.00000444\n",
            "Epoch  306 Total Loss  3.280354786324343e-06\n",
            "Starting epoch 307\n",
            "Loss after mini-batch    50: 0.00001488\n",
            "Epoch  307 Total Loss  1.052976305704334e-05\n",
            "Starting epoch 308\n",
            "Loss after mini-batch    50: 0.00000557\n",
            "Epoch  308 Total Loss  4.093591295248642e-06\n",
            "Starting epoch 309\n",
            "Loss after mini-batch    50: 0.00000548\n",
            "Epoch  309 Total Loss  4.1675254576952385e-06\n",
            "Starting epoch 310\n",
            "Loss after mini-batch    50: 0.00000363\n",
            "Epoch  310 Total Loss  2.6586664924425443e-06\n",
            "Starting epoch 311\n",
            "Loss after mini-batch    50: 0.00000449\n",
            "Epoch  311 Total Loss  3.348218031767263e-06\n",
            "Starting epoch 312\n",
            "Loss after mini-batch    50: 0.00000377\n",
            "Epoch  312 Total Loss  2.7432345901523547e-06\n",
            "Starting epoch 313\n",
            "Loss after mini-batch    50: 0.00000368\n",
            "Epoch  313 Total Loss  2.761795452308233e-06\n",
            "Starting epoch 314\n",
            "Loss after mini-batch    50: 0.00000338\n",
            "Epoch  314 Total Loss  2.44149276001298e-06\n",
            "Starting epoch 315\n",
            "Loss after mini-batch    50: 0.00000347\n",
            "Epoch  315 Total Loss  2.5040413543416426e-06\n",
            "Starting epoch 316\n",
            "Loss after mini-batch    50: 0.00000321\n",
            "Epoch  316 Total Loss  2.3256502291365186e-06\n",
            "Starting epoch 317\n",
            "Loss after mini-batch    50: 0.00000325\n",
            "Epoch  317 Total Loss  2.3410608182623035e-06\n",
            "Starting epoch 318\n",
            "Loss after mini-batch    50: 0.00000319\n",
            "Epoch  318 Total Loss  2.2841285114921165e-06\n",
            "Starting epoch 319\n",
            "Loss after mini-batch    50: 0.00000317\n",
            "Epoch  319 Total Loss  2.2781060460521886e-06\n",
            "Starting epoch 320\n",
            "Loss after mini-batch    50: 0.00000305\n",
            "Epoch  320 Total Loss  2.1825010006656007e-06\n",
            "Starting epoch 321\n",
            "Loss after mini-batch    50: 0.00000308\n",
            "Epoch  321 Total Loss  2.2054806025659544e-06\n",
            "Starting epoch 322\n",
            "Loss after mini-batch    50: 0.00000309\n",
            "Epoch  322 Total Loss  2.2012251141598525e-06\n",
            "Starting epoch 323\n",
            "Loss after mini-batch    50: 0.00000300\n",
            "Epoch  323 Total Loss  2.1520232392398717e-06\n",
            "Starting epoch 324\n",
            "Loss after mini-batch    50: 0.00000606\n",
            "Epoch  324 Total Loss  4.322015295284605e-06\n",
            "Starting epoch 325\n",
            "Loss after mini-batch    50: 0.00000478\n",
            "Epoch  325 Total Loss  3.527921817446532e-06\n",
            "Starting epoch 326\n",
            "Loss after mini-batch    50: 0.00000375\n",
            "Epoch  326 Total Loss  2.710646100665157e-06\n",
            "Starting epoch 327\n",
            "Loss after mini-batch    50: 0.00000328\n",
            "Epoch  327 Total Loss  2.396635682479066e-06\n",
            "Starting epoch 328\n",
            "Loss after mini-batch    50: 0.00000344\n",
            "Epoch  328 Total Loss  2.5010254913502038e-06\n",
            "Starting epoch 329\n",
            "Loss after mini-batch    50: 0.00000291\n",
            "Epoch  329 Total Loss  2.110611705457504e-06\n",
            "Starting epoch 330\n",
            "Loss after mini-batch    50: 0.00000309\n",
            "Epoch  330 Total Loss  2.2464247886322397e-06\n",
            "Starting epoch 331\n",
            "Loss after mini-batch    50: 0.00000332\n",
            "Epoch  331 Total Loss  2.4528433324455534e-06\n",
            "Starting epoch 332\n",
            "Loss after mini-batch    50: 0.00000358\n",
            "Epoch  332 Total Loss  2.5877353943116866e-06\n",
            "Starting epoch 333\n",
            "Loss after mini-batch    50: 0.00000321\n",
            "Epoch  333 Total Loss  2.3458120087897307e-06\n",
            "Starting epoch 334\n",
            "Loss after mini-batch    50: 0.00000337\n",
            "Epoch  334 Total Loss  2.427581336617024e-06\n",
            "Starting epoch 335\n",
            "Loss after mini-batch    50: 0.00000355\n",
            "Epoch  335 Total Loss  2.5432280224249283e-06\n",
            "Starting epoch 336\n",
            "Loss after mini-batch    50: 0.00000320\n",
            "Epoch  336 Total Loss  2.2870913325176743e-06\n",
            "Starting epoch 337\n",
            "Loss after mini-batch    50: 0.00000288\n",
            "Epoch  337 Total Loss  2.0538793387271504e-06\n",
            "Starting epoch 338\n",
            "Loss after mini-batch    50: 0.00000304\n",
            "Epoch  338 Total Loss  2.1661716847908174e-06\n",
            "Starting epoch 339\n",
            "Loss after mini-batch    50: 0.00000300\n",
            "Epoch  339 Total Loss  2.12937196374262e-06\n",
            "Starting epoch 340\n",
            "Loss after mini-batch    50: 0.00000292\n",
            "Epoch  340 Total Loss  2.0678142403301395e-06\n",
            "Starting epoch 341\n",
            "Loss after mini-batch    50: 0.00000279\n",
            "Epoch  341 Total Loss  1.9912114759021747e-06\n",
            "Starting epoch 342\n",
            "Loss after mini-batch    50: 0.00000333\n",
            "Epoch  342 Total Loss  2.359804970912075e-06\n",
            "Starting epoch 343\n",
            "Loss after mini-batch    50: 0.00000321\n",
            "Epoch  343 Total Loss  2.320486922317381e-06\n",
            "Starting epoch 344\n",
            "Loss after mini-batch    50: 0.00000388\n",
            "Epoch  344 Total Loss  2.8087556305974224e-06\n",
            "Starting epoch 345\n",
            "Loss after mini-batch    50: 0.00000350\n",
            "Epoch  345 Total Loss  2.50728233794659e-06\n",
            "Starting epoch 346\n",
            "Loss after mini-batch    50: 0.00000722\n",
            "Epoch  346 Total Loss  5.152632294490465e-06\n",
            "Starting epoch 347\n",
            "Loss after mini-batch    50: 0.00000691\n",
            "Epoch  347 Total Loss  4.9780438835522745e-06\n",
            "Starting epoch 348\n",
            "Loss after mini-batch    50: 0.00000538\n",
            "Epoch  348 Total Loss  3.8076054649967737e-06\n",
            "Starting epoch 349\n",
            "Loss after mini-batch    50: 0.00000373\n",
            "Epoch  349 Total Loss  2.7375636133777605e-06\n",
            "Starting epoch 350\n",
            "Loss after mini-batch    50: 0.00000328\n",
            "Epoch  350 Total Loss  2.4053262300939416e-06\n",
            "Starting epoch 351\n",
            "Loss after mini-batch    50: 0.00000308\n",
            "Epoch  351 Total Loss  2.1993169908215058e-06\n",
            "Starting epoch 352\n",
            "Loss after mini-batch    50: 0.00000318\n",
            "Epoch  352 Total Loss  2.4179585616451108e-06\n",
            "Starting epoch 353\n",
            "Loss after mini-batch    50: 0.00000333\n",
            "Epoch  353 Total Loss  2.3826255554699257e-06\n",
            "Starting epoch 354\n",
            "Loss after mini-batch    50: 0.00000326\n",
            "Epoch  354 Total Loss  2.324627396577128e-06\n",
            "Starting epoch 355\n",
            "Loss after mini-batch    50: 0.00000305\n",
            "Epoch  355 Total Loss  2.200503823157618e-06\n",
            "Starting epoch 356\n",
            "Loss after mini-batch    50: 0.00000299\n",
            "Epoch  356 Total Loss  2.107217422257316e-06\n",
            "Starting epoch 357\n",
            "Loss after mini-batch    50: 0.00000284\n",
            "Epoch  357 Total Loss  2.025016404709016e-06\n",
            "Starting epoch 358\n",
            "Loss after mini-batch    50: 0.00000272\n",
            "Epoch  358 Total Loss  1.9602662913392125e-06\n",
            "Starting epoch 359\n",
            "Loss after mini-batch    50: 0.00000282\n",
            "Epoch  359 Total Loss  2.0018838905085565e-06\n",
            "Starting epoch 360\n",
            "Loss after mini-batch    50: 0.00000289\n",
            "Epoch  360 Total Loss  2.055070922052988e-06\n",
            "Starting epoch 361\n",
            "Loss after mini-batch    50: 0.00000317\n",
            "Epoch  361 Total Loss  2.444084664407795e-06\n",
            "Starting epoch 362\n",
            "Loss after mini-batch    50: 0.00000514\n",
            "Epoch  362 Total Loss  3.9400666366824e-06\n",
            "Starting epoch 363\n",
            "Loss after mini-batch    50: 0.00000570\n",
            "Epoch  363 Total Loss  4.077543362833601e-06\n",
            "Starting epoch 364\n",
            "Loss after mini-batch    50: 0.00000560\n",
            "Epoch  364 Total Loss  3.958665432346464e-06\n",
            "Starting epoch 365\n",
            "Loss after mini-batch    50: 0.00000338\n",
            "Epoch  365 Total Loss  2.411219684929201e-06\n",
            "Starting epoch 366\n",
            "Loss after mini-batch    50: 0.00000336\n",
            "Epoch  366 Total Loss  2.377665659124438e-06\n",
            "Starting epoch 367\n",
            "Loss after mini-batch    50: 0.00000321\n",
            "Epoch  367 Total Loss  2.2761506709804515e-06\n",
            "Starting epoch 368\n",
            "Loss after mini-batch    50: 0.00000310\n",
            "Epoch  368 Total Loss  2.191221628564419e-06\n",
            "Starting epoch 369\n",
            "Loss after mini-batch    50: 0.00000300\n",
            "Epoch  369 Total Loss  2.1332726869931976e-06\n",
            "Starting epoch 370\n",
            "Loss after mini-batch    50: 0.00000298\n",
            "Epoch  370 Total Loss  2.096678091762567e-06\n",
            "Starting epoch 371\n",
            "Loss after mini-batch    50: 0.00000283\n",
            "Epoch  371 Total Loss  1.9955237008496674e-06\n",
            "Starting epoch 372\n",
            "Loss after mini-batch    50: 0.00000291\n",
            "Epoch  372 Total Loss  2.0618544013374496e-06\n",
            "Starting epoch 373\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8d385caf6b83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m           \u001b[0minput0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m           \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0mpred_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_temp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdelta_t\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "x_data_plot=[]\n",
        "y_data_all_plot=[]\n",
        "y_data_1_plot=[]\n",
        "y_data_2_plot=[]\n",
        "\n",
        "# Set fixed random number seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "Epochs = 1000\n",
        "for epoch in range(Epochs):\n",
        "    # Print epoch\n",
        "    print(f'Starting epoch {epoch}')\n",
        "    \n",
        "    # Set current and total loss value\n",
        "    current_loss = 0.0\n",
        "    total_loss = 0.0\n",
        "    total_loss1 = 0.0\n",
        "    total_loss2 = 0.0\n",
        "\n",
        "    for i, data in enumerate(train_loader,0):\n",
        "        #print(i)\n",
        "\n",
        "        x_batch, y_batch, delta_t = data\n",
        "        #print('data', x_batch.shape)\n",
        "        if batch_size == 1:\n",
        "          x_batch=torch.squeeze(x_batch)\n",
        "          y_batch=torch.squeeze(y_batch)\n",
        "          delta_t=torch.squeeze(delta_t)\n",
        "        #x_batch=torch.squeeze(x_batch)\n",
        "        ##x_batch=x_batch.reshape(-1,1)\n",
        "        #y_batch=torch.squeeze(y_batch)\n",
        "        #delta_t=torch.squeeze(delta_t)\n",
        "        #print('data', x_batch.shape)\n",
        "        # Ground-truth temperature\n",
        "        true_temp = x_batch[:,4].detach().clone()\n",
        "\n",
        "        input0 = x_batch[0].detach().clone()\n",
        "\n",
        "        # Predicted temperature using model prediction and forward euler method\n",
        "        #pred_temp = true_temp.detach().clone().to(device)\n",
        "        #\n",
        "        pred_temp = torch.zeros(x_batch.shape[0])\n",
        "        #print('pred_temp shape',pred_temp.shape)\n",
        "        pred_temp[0]=true_temp[0].detach().clone().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        #prediction = model(x_batch.to(device))\n",
        "     \n",
        "        for j in range(0, x_batch.shape[0] - 1):\n",
        "          input0[4] = torch.tensor(pred_temp[j]).detach().clone()\n",
        "          pred = model(input0.to(device))/normalize\n",
        "          pred_temp[j + 1] = pred_temp[j] + pred*delta_t[j]\n",
        " \n",
        "        #loss1 = criterion(prediction,y_batch.to(device))\n",
        "        loss = criterion(pred_temp.to(device),true_temp.to(device))\n",
        "        #loss = loss1+loss2\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print statistics\n",
        "        current_loss += loss.item()\n",
        "        total_loss += loss.item()\n",
        "        #total_loss1 += loss1.item()\n",
        "        #total_loss2 += loss2.item()\n",
        "\n",
        "        if i % 50 == 49:\n",
        "            print('Loss after mini-batch %5d: %.8f' %\n",
        "                  (i + 1, current_loss / 50))\n",
        "            current_loss = 0.0\n",
        "\n",
        "    print(\"Epoch \", epoch, \"Total Loss \", total_loss/(i+1))\n",
        "    #print(\"Epoch \", epoch, \"Loss 1 \", total_loss1/(i+1))\n",
        "    #print(\"Epoch \", epoch, \"Loss 2\", total_loss2/(i+1))\n",
        "\n",
        "    x_data_plot.append(epoch)\n",
        "    y_data_all_plot.append(total_loss/(i+1))\n",
        "    #y_data_1_plot.append(total_loss1/(i+1))\n",
        "    #y_data_2_plot.append(total_loss2/(i+1))\n",
        "\n",
        "# Make the plot of Total Loss vs epochs\n",
        "plt.plot(x_data_plot,y_data_all_plot)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Total Loss')\n",
        "plt.show()\n",
        "\n",
        "# Make the plot of the supervised loss\n",
        "#plt.plot(x_data_plot,y_data_1_plot)\n",
        "#plt.xlabel('Epoch')\n",
        "#plt.ylabel('Loss1')\n",
        "#plt.show()\n",
        "\n",
        "# Make the plot of time stability loss\n",
        "#plt.plot(x_data_plot,y_data_2_plot)\n",
        "#plt.xlabel('Epoch')\n",
        "#plt.ylabel('Loss2')\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLIbXG2Adlf3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0G9EHlOOxzfQ"
      },
      "outputs": [],
      "source": [
        "plt.plot(x_data_plot,y_data_all_plot)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Total Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvmxR1iFxyVJ"
      },
      "outputs": [],
      "source": [
        "#torch.save(model.state_dict(),  'modelfor_' + str(normalize) + str(batch_size)+'.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loVO9lty_1mH"
      },
      "outputs": [],
      "source": [
        "# Plot of last 100 epochs\n",
        "plt.plot(x_data_plot[-200:],y_data_all_plot[-200:])\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()\n",
        "\n",
        "# Plot of last 100 epochs\n",
        "#plt.plot(x_data_plot[-200:],y_data_1_plot[-200:])\n",
        "#plt.xlabel('Epoch')\n",
        "#plt.ylabel('Loss 1')\n",
        "#plt.show()\n",
        "\n",
        "# Plot of last 100 epochs\n",
        "#plt.plot(x_data_plot[-200:],y_data_2_plot[-200:])\n",
        "#plt.xlabel('Epoch')\n",
        "#plt.ylabel('Loss 2')\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2CKaap0Aauy"
      },
      "outputs": [],
      "source": [
        "# Make a prediction\n",
        "pred = model(ds.x.float().to(device)) #GPU\n",
        "pred = pred.detach().cpu().numpy()/normalize\n",
        "\n",
        "# ground-truth\n",
        "df_y_tensor_np = ds.y.numpy()/normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CP7kZ5jnjTTy"
      },
      "outputs": [],
      "source": [
        "# Some statistics on the model performance on all of dataset\n",
        "mae = np.sum(np.abs(pred- df_y_tensor_np).mean(axis=None))\n",
        "print('MAE:', mae)\n",
        "\n",
        "mse = ((df_y_tensor_np - pred)**2).mean(axis=None)\n",
        "print('MSE:', mse)\n",
        "\n",
        "rel_error = np.linalg.norm(pred - df_y_tensor_np) / np.linalg.norm(df_y_tensor_np)\n",
        "print('Relative error (%):', rel_error*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtDt8BXiDYrZ"
      },
      "outputs": [],
      "source": [
        "figure(figsize=(10, 8), dpi= 360)\n",
        "\n",
        "plt.plot(pred, '--')\n",
        "plt.plot(df_y_tensor_np, '-')\n",
        "plt.legend(['Prediction', 'ground-truth'])\n",
        "plt.xlabel('t-idx')\n",
        "plt.ylabel('Delta Temperature/delta time')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3ZQU5JyDrEq"
      },
      "outputs": [],
      "source": [
        "figure(figsize=(10, 8), dpi= 360)\n",
        "\n",
        "#time\n",
        "t=ds.t\n",
        "\n",
        "plt.plot(t,pred, '--')\n",
        "plt.plot(t,df_y_tensor_np, '-')\n",
        "plt.legend(['Prediction', 'ground-truth'])\n",
        "plt.xlabel('time / seconds')\n",
        "plt.ylabel('Delta Temperature/delta time')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_y38GoxFDeJ"
      },
      "outputs": [],
      "source": [
        "# Import a slice of the datase (based on drive-id) for further analysis\n",
        "ds = TeslaDatasetSlice(device = device, ID = 30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08d4xtKcGONJ"
      },
      "outputs": [],
      "source": [
        "#Forward Euler method with fixed initial conditions but with updated \n",
        "#Temperature from the prediction of the model at previous iteration\n",
        "#with generated temporally equidistant time steps\n",
        "\n",
        "# ground-truth time\n",
        "t=ds.t\n",
        "max_t = t.max()\n",
        "\n",
        "# Ground-truth temperature\n",
        "true_temp = ds.x[:,4].numpy()\n",
        "\n",
        "# Predicted temperature using model prediction and forward euler method\n",
        "pred_temp = np.zeros((ds.x.shape[0]))\n",
        "pred_temp = true_temp.copy()\n",
        "\n",
        "# Fixed initial conditions for all environmental conditions\n",
        "input = ds.x[0].detach().clone()\n",
        "\n",
        "# temporally equdistant time steps\n",
        "tt = np.linspace(0,max_t,ds.x.shape[0])\n",
        "step_size=tt[2]-tt[1]\n",
        "\n",
        "#ODE\n",
        "for i in range(0, ds.x.shape[0] - 1):\n",
        "      input[4] = torch.tensor(pred_temp[i]).detach().clone()\n",
        "      pred = model(input.to(device))\n",
        "      pred = pred.detach().cpu().numpy()/normalize\n",
        "      pred_temp[i + 1] = pred_temp[i] + pred*step_size\n",
        "\n",
        "\n",
        "#MAE\n",
        "mae = np.sum(np.abs(pred_temp- true_temp).mean(axis=None))\n",
        "print('MAE:', mae)\n",
        "\n",
        "#MSE\n",
        "mse = ((true_temp - pred_temp)**2).mean(axis=None)\n",
        "print('MSE:', mse)\n",
        "\n",
        "#Relative error\n",
        "rel_error = np.linalg.norm(pred_temp - true_temp) / np.linalg.norm(true_temp)\n",
        "print('Relative error (%):', rel_error*100)\n",
        "\n",
        "plt.figure(figsize = (12, 8))\n",
        "plt.plot(tt, pred_temp, '-', label='ODE approximation')\n",
        "plt.plot(t, true_temp, '--', label='Exact')\n",
        "plt.title('Approximate and True Solution (temporally equidistant step size)')\n",
        "plt.xlabel('t (seconds)')\n",
        "plt.ylabel('Temperature')\n",
        "plt.grid()\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx9KVaQyGSYb"
      },
      "outputs": [],
      "source": [
        "#Forward Euler method with fixed initial conditions but with updated \n",
        "#Temperature from the prediction of the model at previous iteration\n",
        "#with true step sizes\n",
        "\n",
        "# ground-truth time\n",
        "t=ds.t\n",
        "max_t = t.max()\n",
        "t=t.numpy()\n",
        "\n",
        "# Ground-truth temperature\n",
        "true_temp = ds.x[:,4].numpy()\n",
        "\n",
        "# Predicted temperature using model prediction and forward euler method\n",
        "#pred_temp = np.zeros((ds.x.shape[0]))\n",
        "pred_temp[0] = true_temp[0].copy()\n",
        "\n",
        "# Fixed initial conditions for all environmental conditions\n",
        "input = ds.x[0].detach().clone()\n",
        "\n",
        "# temporally equdistant time steps\n",
        "tt = np.linspace(0,max_t,ds.x.shape[0])\n",
        "step_size=tt[2]-tt[1]\n",
        "\n",
        "# ODE\n",
        "for i in range(0, ds.x.shape[0] - 1):\n",
        "      #input = df_xx_tensor[0]\n",
        "      input[4] = torch.tensor(pred_temp[i]).detach().clone()\n",
        "      pred = model(input.to(device))\n",
        "      pred = pred.detach().cpu().numpy()/normalize\n",
        "      pred_temp[i + 1] = pred_temp[i] + pred*(t[i+1]-t[i])\n",
        "      \n",
        "#MAE \n",
        "mae = np.sum(np.abs(pred_temp- true_temp).mean(axis=None))\n",
        "print('MAE:', mae)\n",
        "\n",
        "#MSE\n",
        "mse = ((true_temp - pred_temp)**2).mean(axis=None)\n",
        "print('MSE:', mse)\n",
        "\n",
        "# Relative error\n",
        "rel_error = np.linalg.norm(pred_temp - true_temp) / np.linalg.norm(true_temp)\n",
        "print('Relative error (%):', rel_error*100)\n",
        "\n",
        "plt.figure(figsize = (12, 8))\n",
        "plt.plot(t, pred_temp, '-', label='ODE approximation')\n",
        "plt.plot(t, true_temp, '--', label='Exact')\n",
        "plt.title('Approximate and True Solution (true step size)')\n",
        "plt.xlabel('t (seconds)')\n",
        "plt.ylabel('Temperature')\n",
        "plt.grid()\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFmgP4Qz6Gll"
      },
      "outputs": [],
      "source": [
        "#Forward Euler method with updated environmental conditions from the dataset at each iteration\n",
        "#But with updated temperature from the prediction of the model at previous iteration\n",
        "#with true step sizes\n",
        "\n",
        "# ground-truth time\n",
        "t=ds.t\n",
        "max_t = t.max()\n",
        "t=t.numpy()\n",
        "\n",
        "# Ground-truth temperature\n",
        "true_temp = ds.x[:,4].numpy()\n",
        "\n",
        "# Predicted temperature using model prediction and forward euler method\n",
        "pred_temp = np.zeros((ds.x.shape[0]))\n",
        "pred_temp[0] = true_temp[0].copy()\n",
        "\n",
        "\n",
        "for i in range(0, ds.x.shape[0] - 1):\n",
        "      input = ds.x[i].detach().clone()\n",
        "      input[4] = torch.tensor(pred_temp[i]).detach().clone()\n",
        "      pred = model(input.to(device))\n",
        "      pred = pred.detach().cpu().numpy()/normalize\n",
        "      pred_temp[i + 1] = pred_temp[i] + pred*(t[i+1]-t[i])\n",
        "\n",
        "#MAE \n",
        "mae = np.sum(np.abs(pred_temp- true_temp).mean(axis=None))\n",
        "print('MAE:', mae)\n",
        "\n",
        "#MSE\n",
        "mse = ((true_temp - pred_temp)**2).mean(axis=None)\n",
        "print('MSE:', mse)\n",
        "\n",
        "# Relative error\n",
        "rel_error = np.linalg.norm(pred_temp - true_temp) / np.linalg.norm(true_temp)\n",
        "print('Relative error (%):', rel_error*100)\n",
        "\n",
        "plt.figure(figsize = (12, 8))\n",
        "plt.plot(t, pred_temp, '-', label='ODE approximation')\n",
        "plt.plot(t, true_temp, '--', label='Exact')\n",
        "plt.title('Approximate (with updated env conditions) and True Solution (true step size)')\n",
        "plt.xlabel('t (seconds)')\n",
        "plt.ylabel('Temperature')\n",
        "plt.grid()\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2ppJIkXGVM7"
      },
      "outputs": [],
      "source": [
        "plt.plot(tt, '-', label='my step size')\n",
        "plt.plot(t, '--', label='true step-size')\n",
        "plt.legend(loc='lower right')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yuf54BmGW-P"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.diff(t.reshape(-1)))\n",
        "plt.plot(np.diff(tt.reshape(-1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aK2F9VaIkRUj"
      },
      "outputs": [],
      "source": [
        "# Checking if vectorised version of the loss works\n",
        "\n",
        "true_temp = ds.x[:,4].numpy()\n",
        "pred_temp = true_temp.copy()\n",
        "\n",
        "prediction = model(ds.x.to(device)) \n",
        "\n",
        "deltaTemp = prediction.reshape(-1)*ds.dt.reshape(-1)/normalize\n",
        "\n",
        "temp0 = pred_temp + deltaTemp.detach().cpu().numpy()\n",
        "pred_temp[1:] = temp0[:-1]\n",
        "\n",
        "#MAE\n",
        "mae = np.sum(np.abs(pred_temp- true_temp).mean(axis=None))\n",
        "print('MAE:', mae)\n",
        "\n",
        "#MSE\n",
        "mse = ((true_temp - pred_temp)**2).mean(axis=None)\n",
        "print('MSE:', mse)\n",
        "\n",
        "#Relative error\n",
        "rel_error = np.linalg.norm(pred_temp - true_temp) / np.linalg.norm(true_temp)\n",
        "print('Relative error (%):', rel_error*100)\n",
        "\n",
        "plt.figure(figsize = (12, 8))\n",
        "plt.plot(t, pred_temp, '-', label='ODE approximation')\n",
        "plt.plot(t, true_temp, '--', label='Exact')\n",
        "plt.title('Approximate (vectorised) and True Solution (true step size)')\n",
        "plt.xlabel('t (seconds)')\n",
        "plt.ylabel('Temperature')\n",
        "plt.grid()\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "obHI0si1x6pQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.load_state_dict(torch.load( 'modelfor_' + str(normalize) + str(batch_size)+'.pt'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Copy of For loop of Vectorised time stability of 1D_Heat_Eqn_Diff_Operator v2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
