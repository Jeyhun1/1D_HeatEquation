{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5XqFaBeDXyHq",
    "outputId": "640f796d-576e-4588-f1af-8f7c8e3394fe"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor, ones, stack, load\n",
    "from torch.autograd import grad\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas as pd\n",
    "from torch.nn import Module\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from scipy import stats\n",
    "from pathlib import Path\n",
    "import wandb\n",
    "import time\n",
    "from tesladatano import TeslaDatasetNo\n",
    "\n",
    "from tqdm import trange\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# Import PINNFramework etc.\n",
    "# https://github.com/ComputationalRadiationPhysics/NeuralSolvers.git\n",
    "# sys.path.append(\"...\")# PINNFramework etc.\n",
    "# sys.path.append(\"/home/hoffmnic/Code/NeuralSolvers\")\n",
    "sys.path.append(\"NeuralSolvers\")  \n",
    "import PINNFramework as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWHGXtmGxoEU"
   },
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEqKEcG8YMpg",
    "outputId": "a3107294-aca3-4cc8-aac5-34b5dbe9619b"
   },
   "outputs": [],
   "source": [
    "# Use cuda if it is available, else use the cpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# login wandb (optional)\n",
    "wandb.login()\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'grid'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'Validation Loss',\n",
    "    'goal': 'minimize'  \n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "\n",
    "parameters_dict = {\n",
    "    \n",
    "        'alpha': {\n",
    "        'values': [1e-3,1e-2,0.1,1,10,100,1000,10**4,10**5]\n",
    "        },   \n",
    "        'normalize': {\n",
    "        'values': [1000]\n",
    "        },\n",
    "        'batch_size': {\n",
    "        'values': [4096]\n",
    "        },\n",
    "        'lr': {\n",
    "        'values': [1e-4]\n",
    "        },\n",
    "        'input_size': {\n",
    "        'values': [6]\n",
    "        },\n",
    "        'output_size': {\n",
    "        'values': [1]\n",
    "        },\n",
    "        'hidden_size': {\n",
    "        'values': [100]\n",
    "        },\n",
    "        'num_hidden': {\n",
    "        'values': [4]\n",
    "        },\n",
    "        'epochs': {\n",
    "        'values': [5]\n",
    "        },\n",
    "    }\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict\n",
    "\n",
    "print('hey')\n",
    "\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"Neural_Operator_project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(x, u):\n",
    "\n",
    "    grads = ones(u.shape, device=u.device) # move to the same device as prediction\n",
    "    grad_u = grad(u, x, create_graph=True, grad_outputs=grads )[0]\n",
    "\n",
    "    \n",
    "    return grad_u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uk8L53ZG1YTF"
   },
   "source": [
    "## Function for saving a checkpoint during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXZeiNIRoK2K"
   },
   "outputs": [],
   "source": [
    "def write_checkpoint(checkpoint_path, epoch, min_mlp_loss, optimizer):\n",
    "    checkpoint = {}\n",
    "    checkpoint[\"epoch\"] = epoch\n",
    "    checkpoint[\"minimum_pinn_loss\"] = min_mlp_loss\n",
    "    checkpoint[\"optimizer\"] = optimizer.state_dict()\n",
    "    checkpoint[\"mlp_model\"] = model.state_dict()\n",
    "    torch.save(checkpoint, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRwCCUarwUVp"
   },
   "source": [
    "## Function for evaluating the performance on the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtDt8BXiDYrZ"
   },
   "outputs": [],
   "source": [
    "def evaluate(model,idd,rel_time,diff,normalize):\n",
    "    print('ID = ', idd)\n",
    "    # import test data\n",
    "    ds_test = TeslaDatasetNo(device = device, ID = idd, data = \"test\",rel_time = rel_time, diff = diff)\n",
    "\n",
    "    # Prediction accuracy of the Neural Operator\n",
    "    print('Prediction accuracy of the Neural Operator (NO)')\n",
    "    begin = time.time()\n",
    "    pred_der = model(ds_test.x.to(device))\n",
    "    pred_der = pred_der.detach().cpu().numpy()/normalize\n",
    "    end = time.time()\n",
    "    true_der = ds_test.y.cpu().numpy()\n",
    "    print(\"time:\", end - begin)\n",
    "\n",
    "    # relative time\n",
    "    t=ds_test.t\n",
    "\n",
    "    #MAE\n",
    "    mae_der = np.sum(np.abs(pred_der- true_der).mean(axis=None))\n",
    "    print('MAE:', mae_der)\n",
    "\n",
    "    #MSE\n",
    "    mse_der = ((true_der - pred_der)**2).mean(axis=None)\n",
    "    print('MSE:', mse_der)\n",
    "\n",
    "    #Relative error\n",
    "    rel_error_der = np.linalg.norm(pred_der - true_der) / np.linalg.norm(true_der)*100\n",
    "    print('Relative error (%):', rel_error_der)\n",
    "\n",
    "#     # Plot\n",
    "#     plt.figure(figsize = (12, 8))\n",
    "#     plt.plot(t, pred_der, '-', label='Prediction')\n",
    "#     plt.plot(t, true_der, '--', label='Ground-truth')\n",
    "#     plt.title('Prediction accuracy of Neural Operator vs ground-truth for drive-ID = {}'.format(idd))\n",
    "#     plt.xlabel('t (seconds)')\n",
    "#     plt.ylabel('ΔTemp/Δt (°C/s)')\n",
    "#     plt.grid()\n",
    "#     plt.legend(loc='lower right')\n",
    "#     plt.show()\n",
    "\n",
    "    print('########################################################')\n",
    "\n",
    "    #3)Forward Euler method with fixed initial env. conditions but with updated \n",
    "    #Temperature (and rel time) from the prediction of the model at previous iteration\n",
    "    #with generated temporally equidistant time steps\n",
    "\n",
    "    print('Forwad Euler method with fixed initial env conditions')\n",
    "    rel_t = ds_test.rel_t\n",
    "\n",
    "    # ground-truth time\n",
    "    t=ds_test.t\n",
    "    max_t = t.max()\n",
    "    t=t.cpu().numpy()\n",
    "\n",
    "    # Ground-truth temperature\n",
    "    true_temp = ds_test.x[:,4].cpu().numpy()\n",
    "\n",
    "    # Predicted temperature using model prediction and forward euler method\n",
    "    pred_temp = np.zeros((ds_test.x.shape[0]))\n",
    "    pred_temp = true_temp.copy()\n",
    "\n",
    "    # Fixed initial conditions for all environmental conditions\n",
    "    input = ds_test.x[0].detach().clone()\n",
    "\n",
    "    # temporally equdistant time steps\n",
    "    tt = np.linspace(0,max_t,ds_test.x.shape[0])\n",
    "    step_size=tt[2]-tt[1]\n",
    "\n",
    "    #ODE\n",
    "    begin = time.time()\n",
    "\n",
    "    for i in range(0, ds_test.x.shape[0] - 1):\n",
    "        input[4] = torch.tensor(pred_temp[i]).detach().clone()\n",
    "        if rel_time == True:\n",
    "            input[5] = torch.tensor(rel_t[i]).detach().clone()\n",
    "        pred = model(input.to(device))\n",
    "        pred = pred.detach().cpu().numpy()/normalize\n",
    "        pred_temp[i + 1] = pred_temp[i] + pred*step_size\n",
    "    end = time.time()\n",
    "\n",
    "    print(\"time:\", end - begin)\n",
    "\n",
    "    #MAE\n",
    "    mae = np.sum(np.abs(pred_temp- true_temp).mean(axis=None))\n",
    "    print('MAE:', mae)\n",
    "\n",
    "    #MSE\n",
    "    mse = ((true_temp - pred_temp)**2).mean(axis=None)\n",
    "    print('MSE:', mse)\n",
    "\n",
    "    #Relative error\n",
    "    rel_error = np.linalg.norm(pred_temp - true_temp) / np.linalg.norm(true_temp)*100\n",
    "    print('Relative error (%):', rel_error)\n",
    "\n",
    "#     #Plot\n",
    "#     plt.figure(figsize = (12, 8))\n",
    "#     plt.plot(tt, pred_temp, '-', label='Prediction')\n",
    "#     plt.plot(t, true_temp, '--', label='Ground-truth')\n",
    "#     plt.title('Prediction vs ground-truth for drive-ID = {} (temporally equidistant step size)'.format(idd))\n",
    "#     plt.xlabel('t (seconds)')\n",
    "#     plt.ylabel('Temperature (°C)')\n",
    "#     plt.grid()\n",
    "#     plt.legend(loc='lower right')\n",
    "#     plt.show()\n",
    "\n",
    "    #4)Forward Euler method with updated environmental conditions from the dataset at each iteration\n",
    "    #But with updated temperature from the prediction of the model at previous iteration\n",
    "    #with true step sizes\n",
    "    print('Forwad Euler method with updated env conditions from the dataset at each iteration with true step sizes')\n",
    "\n",
    "    # time\n",
    "    t=ds_test.t\n",
    "    t=t.numpy()\n",
    "\n",
    "    # max time\n",
    "    max_t = t.max()\n",
    "\n",
    "    # Ground-truth temperature\n",
    "    true_temp = ds_test.x[:,4].cpu().numpy()\n",
    "\n",
    "    # Predicted temperature using model prediction and forward euler method\n",
    "    pred_temp = np.zeros((ds_test.x.shape[0]))\n",
    "    pred_temp[0] = true_temp[0].copy()\n",
    "\n",
    "    begin = time.time()\n",
    "    for i in range(0, ds_test.x.shape[0] - 1):\n",
    "        input = ds_test.x[i].detach().clone()\n",
    "        input[4] = torch.tensor(pred_temp[i]).detach().clone()\n",
    "        pred = model(input.to(device))\n",
    "        pred = pred.detach().cpu().numpy()/normalize\n",
    "        pred_temp[i + 1] = pred_temp[i] + pred*(t[i+1]-t[i])\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "    print(\"time:\", end - begin)\n",
    "    #MAE \n",
    "    mae_upd = np.sum(np.abs(pred_temp- true_temp).mean(axis=None))\n",
    "    print('MAE:', mae_upd)\n",
    "\n",
    "    #MSE\n",
    "    mse_upd = ((true_temp - pred_temp)**2).mean(axis=None)\n",
    "    print('MSE:', mse_upd)\n",
    "\n",
    "    # Relative error\n",
    "    rel_error_upd = np.linalg.norm(pred_temp - true_temp) / np.linalg.norm(true_temp)*100\n",
    "    print('Relative error (%):', rel_error_upd)\n",
    "\n",
    "#     #Plot\n",
    "#     plt.figure(figsize = (12, 8))\n",
    "#     plt.plot(t, pred_temp, '-', label='Prediction')\n",
    "#     plt.plot(t, true_temp, '--', label='Ground-truth')\n",
    "#     #plt.title('Approximate (with updated env conditions) and True Solution (true step size)'.format(idd))\n",
    "#     plt.title('Prediction (with updated env. conditions) vs ground-truth for drive-ID = {} (true step size)'.format(idd))\n",
    "#     plt.xlabel('t (seconds)')\n",
    "#     plt.ylabel('Temperature (°C)')\n",
    "#     plt.grid()\n",
    "#     plt.legend(loc='lower right')\n",
    "#     plt.show()\n",
    "\n",
    "#     # variables\n",
    "#     #rel_t = ds_test.rel_t\n",
    "#     power = ds_test.x[:,0]\n",
    "#     speed = ds_test.x[:,1]\n",
    "#     bat_level = ds_test.x[:,2]\n",
    "#     out_temp = ds_test.x[:,3]\n",
    "#     temp = ds_test.x[:,4]\n",
    "#     rel_t = ds_test.x[:,5]\n",
    "\n",
    "#     # Log the loss and accuracy values at the end of each epoch\n",
    "#     wandb.log({\n",
    "#         \"power test {}\".format(idd): power,\n",
    "#         \"speed test {}\".format(idd): speed,\n",
    "#         \"battery level test {}\".format(idd): bat_level,\n",
    "#         \"outside temperature test {}\".format(idd): out_temp,\n",
    "#         \"speed test {}\".format(idd): temp,\n",
    "#         \"rel time test {}\".format(idd): rel_t,\n",
    "#          })\n",
    "\n",
    "    # #5 time\n",
    "    # plt.figure(figsize = (12, 8))\n",
    "    # plt.plot(tt, '-', label='equidistant step size')\n",
    "    # plt.plot(t, '--', label='true stepsize')\n",
    "    # plt.title('Plot of equidistant test time vs true time')\n",
    "    # plt.legend(loc='lower right')\n",
    "\n",
    "    # #6\n",
    "    # plt.figure(figsize = (12, 8))\n",
    "    # plt.plot(np.diff(tt.reshape(-1)),'-', label='equidistant step size')\n",
    "    # plt.plot(np.diff(t.reshape(-1)),  '--', label='true step-size')\n",
    "    # plt.title('Plot of equidistant test time step size vs true time step size')\n",
    "    # plt.legend(loc='upper left')\n",
    "\n",
    "    mae_arr = np.array([mae_der, mae, mae_upd])\n",
    "    mse_arr = np.array([mse_der, mse, mse_upd])\n",
    "    rel_arr = np.array([rel_error_der, rel_error, rel_error_upd])\n",
    "    \n",
    "    return mae_arr, mse_arr, rel_arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjP8nSCE1gs7"
   },
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9p01JZkSBvB4"
   },
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config): \n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "        \n",
    "        \n",
    "        # Create instance of the dataset\n",
    "        ds = TeslaDatasetNo(diff = \"fwd_diff\", device = device, data ='train', normalize = config.normalize, rel_time = True)\n",
    "        ds_test = TeslaDatasetNo(device = device, ID = -1, data = \"test\",normalize = config.normalize, rel_time = True)\n",
    "         \n",
    "        # bounds\n",
    "        config.lb = ds.lb\n",
    "        config.ub = ds.ub\n",
    "        \n",
    "        # trainloader\n",
    "        train_loader = DataLoader(ds, batch_size=config.batch_size,shuffle=True)\n",
    "        validloader = DataLoader(ds_test, batch_size=config.batch_size,shuffle=True)\n",
    "        \n",
    "        #model\n",
    "        model = pf.models.MLP(input_size=config.input_size,\n",
    "                      output_size=config.output_size, \n",
    "                      hidden_size=config.hidden_size, \n",
    "                      num_hidden=config.num_hidden, \n",
    "                      lb=config.lb, \n",
    "                      ub=config.ub,\n",
    "                      #activation = torch.relu\n",
    "                      )\n",
    "        model.to(device)\n",
    "        \n",
    "        #Log the network weight histograms (optional)\n",
    "        wandb.watch(model)\n",
    "        \n",
    "        # optimizer\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=wandb.config.lr)\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        \n",
    "        \n",
    "        min_mlp_loss = np.inf\n",
    "        min_valid_loss = np.inf\n",
    "\n",
    "        x_data_plot=[]\n",
    "        y_data_all_plot=[]\n",
    "        y_data_1_plot=[]\n",
    "        y_data_2_plot=[]\n",
    "\n",
    "        # Set fixed random number seed\n",
    "        torch.manual_seed(1234)\n",
    "        \n",
    "\n",
    "        begin = time.time()\n",
    "        for epoch in range(config.epochs):\n",
    "        # with trange(epochs, unit=\"epochs\") as pbar:\n",
    "        #     for epoch in pbar:\n",
    "                # Set current and total loss value\n",
    "                current_loss = 0.0\n",
    "                total_loss = 0.0\n",
    "                total_loss1 = 0.0\n",
    "                total_loss2 = 0.0\n",
    "\n",
    "                model.train()   # Optional when not using Model Specific layer\n",
    "                for i, data in enumerate(train_loader,0):\n",
    "                    #print(i)\n",
    "                    x_batch, y_batch = data\n",
    "                    #print('data', x_batch.shape)\n",
    "                    if wandb.config.batch_size == 1:\n",
    "                        x_batch=torch.squeeze(x_batch)\n",
    "                        y_batch=torch.squeeze(y_batch)\n",
    "                      #delta_t=torch.squeeze(delta_t)\n",
    "\n",
    "                    # Ground-truth temperature\n",
    "                    true_temp = x_batch[:,4].detach().clone()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    x_batch.requires_grad=True #new\n",
    "                    pred = model(x_batch.to(device))\n",
    "\n",
    "                    u_deriv = derivative(x_batch,pred) #new\n",
    "                    loss1 = criterion(pred,y_batch.to(device))\n",
    "\n",
    "                    #target2 = torch.zeros_like(u_t) #new\n",
    "                    #loss2 = criterion(target2.to(device),u_t.to(device))*wandb.config.alpha #new\n",
    "                    loss2 = torch.mean(u_deriv**2) * config.alpha #new new\n",
    "                    loss = loss1 + loss2 #new\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Print statistics\n",
    "                    current_loss += loss.item()\n",
    "                    total_loss += loss.item()\n",
    "                    total_loss1 += loss1.item()\n",
    "                    total_loss2 += loss2.item()\n",
    "\n",
    "                    if i % 50 == 49:\n",
    "        #                 print('Loss after mini-batch %5d: %.8f' %\n",
    "        #                      (i + 1, current_loss / 50))\n",
    "                        current_loss = 0.0\n",
    "\n",
    "                train_loss = total_loss/(i+1)\n",
    "                loss1 = total_loss1/(i+1) #new\n",
    "                loss2 = total_loss2/(i+1) #new\n",
    "                #print(\"Epoch \", epoch, \"Total Loss \", Loss)\n",
    "                #pbar.set_postfix(loss=Loss)\n",
    "                x_data_plot.append(epoch)\n",
    "                y_data_all_plot.append(train_loss)      \n",
    "                y_data_1_plot.append(loss1)\n",
    "                y_data_2_plot.append(loss2)\n",
    "\n",
    "                    ######## new\n",
    "                valid_loss = 0.0\n",
    "                model.eval()     # Optional when not using Model Specific layer\n",
    "                for j, data in enumerate(validloader,0):\n",
    "                    x_batch, y_batch = data\n",
    "                    # Forward Pass\n",
    "                    target = model(x_batch.to(device))\n",
    "                    # Find the Loss\n",
    "                    loss = criterion(target,y_batch.to(device))\n",
    "                    # Calculate Loss\n",
    "                    valid_loss += loss.item()\n",
    "\n",
    "                print(f'Epoch {epoch} \\t Training Loss: {\\\n",
    "                train_loss:.5f} \\t Loss 1: {loss1:.5f} \\t Loss 2: {loss2:.5f} \\t Validation Loss: {valid_loss / (j+1):.5f}')    \n",
    "\n",
    "\n",
    "\n",
    "            #     print(f'Epoch {epoch} \\t Training Loss: {\\\n",
    "            #     train_loss} \\t Loss 1: {\\loss1} \\t Loss 2: {\\loss2} \\t Validation Loss: {\\\n",
    "            #     valid_loss / (j+1)}')\n",
    "\n",
    "\n",
    "                # uncomment for saving the best model and checkpoints during training\n",
    "                # save best model\n",
    "                if min_valid_loss > valid_loss:\n",
    "                    print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "                    min_valid_loss = valid_loss\n",
    "\n",
    "                    # Saving State Dict\n",
    "            #         model_name_path = Path('/content/drive/MyDrive/NeuralSolvers-heat-eqn/examples/Research project/nomodel/best_model_{}_{}.pt'.format(wandb.run.id, wandb.run.name))\n",
    "            #         torch.save(model.state_dict(), model_name_path)       \n",
    "                    model_name_path = Path('nomodel/best_model_{}_{}.pt'.format(wandb.run.id, wandb.run.name))\n",
    "                    torch.save(model.state_dict(), model_name_path)\n",
    "\n",
    "                # writing checkpoint\n",
    "                if (epoch + 1) % 200 == 0:\n",
    "            #         checkpoint_path = Path('/content/drive/MyDrive/NeuralSolvers-heat-eqn/examples/Research project/nomodel/checkpoint_{}_{}_{}.pt'.format(wandb.run.id, wandb.run.name, epoch))\n",
    "            #         write_checkpoint(checkpoint_path, epoch, min_mlp_loss, optimizer)\n",
    "                    checkpoint_path = Path('nomodel/checkpoint_{}_{}_{}.pt'.format(wandb.run.id, wandb.run.name, epoch))\n",
    "                    write_checkpoint(checkpoint_path, epoch, min_valid_loss, optimizer)\n",
    "\n",
    "                # Log the loss and accuracy values at the end of each epoch\n",
    "                wandb.log({\n",
    "                    \"Epoch\": epoch,\n",
    "                    \"Total Loss\": train_loss,\n",
    "                    \"Loss1 (temperature)\": loss1,\n",
    "                    \"Loss2 (regulariser)\": loss2,\n",
    "                    \"Validation Loss\": valid_loss\n",
    "                    })\n",
    "        end = time.time()\n",
    "        print(\"time:\", end - begin)\n",
    "\n",
    "\n",
    "        # Load the best model\n",
    "        PATH = 'nomodel/best_model_{}_{}.pt'.format(wandb.run.id, wandb.run.name)\n",
    "\n",
    "        model.load_state_dict(torch.load(PATH))\n",
    "        model.eval()\n",
    "\n",
    "\n",
    "        # Test values = [16,39,47,52,72,81,88]\n",
    "        rl = True\n",
    "        diff = \"fwd_diff\"\n",
    "\n",
    "        # Test value = 16\n",
    "        mae16,mse16,rel_err16 = evaluate(model,idd=16,rel_time=rl,diff=diff,normalize = config.normalize)\n",
    "        print(mae16,mse16,rel_err16)\n",
    "\n",
    "        # Test value = 39\n",
    "        mae39,mse39,rel_err39 = evaluate(model,idd=39,rel_time=rl,diff=diff,normalize = config.normalize)\n",
    "\n",
    "        # Test value = 47\n",
    "        mae47,mse47,rel_err47 = evaluate(model,idd=47,rel_time=rl,diff=diff,normalize = config.normalize)\n",
    "\n",
    "        # Test value = 52\n",
    "        mae52,mse52,rel_err52 = evaluate(model,idd=52,rel_time=rl,diff=diff,normalize = config.normalize)\n",
    "\n",
    "        # Test value = 72\n",
    "        mae72,mse72,rel_err72 = evaluate(model,idd=72,rel_time=rl,diff=diff,normalize = config.normalize)\n",
    "\n",
    "        # Test value = 81\n",
    "        mae81,mse81,rel_err81 = evaluate(model,idd=81,rel_time=rl,diff=diff,normalize = config.normalize)\n",
    "\n",
    "        # Test value = 88\n",
    "        mae88,mse88,rel_err88 = evaluate(model,idd=88,rel_time=rl,diff=diff,normalize = config.normalize)\n",
    "\n",
    "        # Test values = [16,39,47,52,72,81,88]\n",
    "        mae_avg = (mae16+mae39+mae47+mae52+mae72+mae81+mae88)/7\n",
    "        mse_avg = (mse16+mse39+mse47+mse52+mse72+mse81+mse88)/7\n",
    "        rel_err_avg = (rel_err16+rel_err39+rel_err47+rel_err52+rel_err72+rel_err81+rel_err88)/7\n",
    "\n",
    "\n",
    "        # Log the loss and accuracy values at the end of each epoch\n",
    "        wandb.log({\n",
    "            \"mae derivative\": mae_avg[0],\n",
    "            \"mae fixed IC\": mae_avg[1],\n",
    "            \"mae updated env\": mae_avg[2],\n",
    "            \"mse derivative\": mse_avg[0],\n",
    "            \"mse fixed IC\": mse_avg[1],\n",
    "            \"mse updated env\": mse_avg[2],\n",
    "            \"rel err derivative\": rel_err_avg[0],\n",
    "            \"rel err fixed IC\": rel_err_avg[1],\n",
    "            \"rel err updated env\": rel_err_avg[2],\n",
    "             })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the loss and accuracy values at the end of each epoch\n",
    "wandb.log({\n",
    "    \"mae derivative\": mae_avg[0],\n",
    "    \"mae fixed IC\": mae_avg[1],\n",
    "    \"mae updated env\": mae_avg[2],\n",
    "    \"mse derivative\": mse_avg[0],\n",
    "    \"mse fixed IC\": mse_avg[1],\n",
    "    \"mse updated env\": mse_avg[2],\n",
    "    \"rel err derivative\": rel_err_avg[0],\n",
    "    \"rel err fixed IC\": rel_err_avg[1],\n",
    "    \"rel err updated env\": rel_err_avg[2],\n",
    "     })"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NeuralOperator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
